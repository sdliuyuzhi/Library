{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Congress knowledge base. Right now these docs cover eight open source projects: cybernode - smart node manager and transaction crawler cyber-search - transaction parser for cybernode cyber-markets - toolchain for parsing of orders and trades cyb-js - cyber-search javascript library cyb - web3 browser chaingear - create your own Registry of general purpose entries on Ethereum blockchain cyberd - research on the cyber protocol cybercongress - community of of scientists, developers, engineers and craftsmen","title":"cyber\u2022Congress"},{"location":"contribute/","text":"Current wiki is built on top of mkdocs.org engine with Material for MkDocs extensions pack. Required Installations \u00b6 https://hub.docker.com/r/squidfunk/mkdocs-material/ Commands Cheat Sheet \u00b6 docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material docker run --rm -it -v ${PWD}:/docs squidfunk/mkdocs-material build docker run --rm -it -v ~/.ssh:/root/.ssh -v ${PWD}:/docs squidfunk/mkdocs-material gh-deploy Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Contributing"},{"location":"contribute/#required-installations","text":"https://hub.docker.com/r/squidfunk/mkdocs-material/","title":"Required Installations"},{"location":"contribute/#commands-cheat-sheet","text":"docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material docker run --rm -it -v ${PWD}:/docs squidfunk/mkdocs-material build docker run --rm -it -v ~/.ssh:/root/.ssh -v ${PWD}:/docs squidfunk/mkdocs-material gh-deploy","title":"Commands Cheat Sheet"},{"location":"contribute/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"cyb/app-guidelines/","text":"DApp guidelines \u00b6 @asadovka What is App? \u00b6 App in Cyb is a one or multi page application. It can be a simple html file as a frontend and may include smart contract logic as a backend. Apps should be available through IPFS hash. App structure \u00b6 Cyb CLI can automatically generate structure for your App. All you need to do is just type some commands, come up with a name and develop! App requirements \u00b6 We set limitations of computation resources usage for each app. less then 5% of CPU usage less then 10% of RAM usage App development \u00b6 Here we describe how to generate initial app structure. Install the latest version of CYB package npm install -g cyb Select the path where you want to store your app. For example /Desktop/My_apps cd username/Desktop/My_apps Enter the name and generate the structure of the app. A folder with all necessary components will be created cyb init appname Go to the app folder and link your app with your Cyb cyb link Open the Appstore in CYB. You will see your app on \u00abYour app\u00bb page You can also use our Help App (cyb://.help/create) for more info. App deployment \u00b6 When you are ready with development of your app you can easely publish it to the DApp Store so every user of Cyb will see it. Open your app, click on \"deploy\" button and enter IPFS hash of your app. Sign the transaction - your app will be registred in Chaingear. And that is it! App design \u00b6 App design process follows Web3 design principles. State rules \u00b6 We use simple colored states for states of transactions or operations: mempool/failed - red confirmed/reverted - yellow finalized/succesful - green Data visualization \u00b6 Data should be visualised in a simple and attractive way. No overloaded plots an diagrams, we use animation instead. Blockchain objects presenting \u00b6 Blockchain data is too complicated and sometimes not obvious for people. Thus we use adaptive tricks to make work process more convenient: Logical grouping for objects. Every app page has common groups of data (general, blockchain specific) for inheritance of views and better navigation or data observing. Classical accounting terms used for balance and cashflow operations. Blockahains use econimic principles for interaction between subjects thus we can describe such processes in established terms. Robohash logo for contracts entities. Contracts can act by themself, have and algorithms, so it's more natural to perceive them like robots instead of pieces of code.","title":"App guidelines"},{"location":"cyb/app-guidelines/#dapp-guidelines","text":"@asadovka","title":"DApp guidelines"},{"location":"cyb/app-guidelines/#what-is-app","text":"App in Cyb is a one or multi page application. It can be a simple html file as a frontend and may include smart contract logic as a backend. Apps should be available through IPFS hash.","title":"What is App?"},{"location":"cyb/app-guidelines/#app-structure","text":"Cyb CLI can automatically generate structure for your App. All you need to do is just type some commands, come up with a name and develop!","title":"App structure"},{"location":"cyb/app-guidelines/#app-requirements","text":"We set limitations of computation resources usage for each app. less then 5% of CPU usage less then 10% of RAM usage","title":"App requirements"},{"location":"cyb/app-guidelines/#app-development","text":"Here we describe how to generate initial app structure. Install the latest version of CYB package npm install -g cyb Select the path where you want to store your app. For example /Desktop/My_apps cd username/Desktop/My_apps Enter the name and generate the structure of the app. A folder with all necessary components will be created cyb init appname Go to the app folder and link your app with your Cyb cyb link Open the Appstore in CYB. You will see your app on \u00abYour app\u00bb page You can also use our Help App (cyb://.help/create) for more info.","title":"App development"},{"location":"cyb/app-guidelines/#app-deployment","text":"When you are ready with development of your app you can easely publish it to the DApp Store so every user of Cyb will see it. Open your app, click on \"deploy\" button and enter IPFS hash of your app. Sign the transaction - your app will be registred in Chaingear. And that is it!","title":"App deployment"},{"location":"cyb/app-guidelines/#app-design","text":"App design process follows Web3 design principles.","title":"App design"},{"location":"cyb/app-guidelines/#state-rules","text":"We use simple colored states for states of transactions or operations: mempool/failed - red confirmed/reverted - yellow finalized/succesful - green","title":"State rules"},{"location":"cyb/app-guidelines/#data-visualization","text":"Data should be visualised in a simple and attractive way. No overloaded plots an diagrams, we use animation instead.","title":"Data visualization"},{"location":"cyb/app-guidelines/#blockchain-objects-presenting","text":"Blockchain data is too complicated and sometimes not obvious for people. Thus we use adaptive tricks to make work process more convenient: Logical grouping for objects. Every app page has common groups of data (general, blockchain specific) for inheritance of views and better navigation or data observing. Classical accounting terms used for balance and cashflow operations. Blockahains use econimic principles for interaction between subjects thus we can describe such processes in established terms. Robohash logo for contracts entities. Contracts can act by themself, have and algorithms, so it's more natural to perceive them like robots instead of pieces of code.","title":"Blockchain objects presenting"},{"location":"cyb/cyb/","text":"Cyb: web3 browser \u00b6 @xhipster @asadovka Concept. Definitions are work in progress. Current implementation is not in comply with spec yet Abstract \u00b6 Cyb is a friendly software robot who helps you explore the universes. Put it simply it just a web3 browser. At the very beginning cyb is focusing on developers and advanced blockchain users who are able to work with private keys and transactions. But we see how Cyb becomes friendly for everyone who wants to interact with consensus computers in a web of the third generation. This new web is designed to free developers from outdated things such as html and v8. So developers can use any markup, execution and rendering engine they wish. That is why we don't focus on implementation of mentioned things. Instead this paper discuss implementation agnostic concepts of browser that are simple enough to be adopted by web3 developers. Initially we design Cyb for conventional desktop browsing. But suggested concepts can be easily used for mobile, voice, vr and robotics implementations. Introduction \u00b6 Current state of web3 experience is non satisfactory. Still we did not meet any piece of software that is able to deliver deep, emotional web3 experience. So we decide to bring to the table one contender that strictly follows web3 principles defined by ourselves ;-) In a rush for this passion we define the following web3 apps which we believe together implement the full web3 vision in the context of a browsing for web3 agents and app developers: .main : main page for every joe .path : navigation bar and its backend .connect : connection manager and state widget .keys : keystore interface and id widget .cyber : cyberd node manager and app for link chains .pins : favourites backend and application bar .sign : phishing resistant signer for messages and transactions with scheduler .crr : cyb root registry .cyb : origianl web3 appstore .access : permission manager that respects agents' resources .feed : notification backend and feed app .ipfs : ipfs node manager and agent experience .eth : ethereum node manager + ens resolver .wallet : universal wallet ux .help : cyb educational library and feedback mechanism .dev : web3 development tool with support of contracts .cg : all the things chaingearable .settings : cyb settings All this apps are considered as core apps and are included in every Cyb distribution. Let us describe in details every app as a pure concept. .main \u00b6 Purpose of the main app is to make agent happier in a moment it returns for surfing and between experiences. Main page of the browser consists of three main elements: search bar: provides all search functions relevance bar: the most relevant cyberlinks for a particular agent footer: cyberlinks to ecosystem resources which are important for education and contribution .path \u00b6 Navigation bar in Cyb is based on the following elements: back button - returns user to the previous state of web3 agent search bar - provides direct access to certain state star button - allow users to pin cyberlinks forward button - brings user to the future state based on Cyb prediction Search bar is used to browse web3. With the help of DURA with knowledge of application involved ( . ) it can get content across different content addressing protocols such as IPFS, DAT, SWARM, and inside blockchains, tangles and DAGs thus forming heterogeneous environment of web3. In web3 vision doc we describe in details a concept of web3 browsing based on DURA specs. That is, in web3 appending \"dot\" works very different in comparison with web2. Dot is literally a search query to a particular app that also has a content address in heterogeneous network. All symbols after \"dot\" make a map with content address of an app in root registry, and all data before \"dot\" is a query parameter to an app. <illustration> For example: .help query will open Cyb help app. chaingear.help will open chaingear info page in the help app. Query without \"dot\" will be automatically redirected to search in cyberd (Note: queries without dot is synonym to <your-query>.cyber ). Empty query always leads to the main page. . query returns a root registry that is being used by default in Cyb. <api-definition> All cyberlinks that was requested by agent can be accessed using path app that is integral part of Cyb experience. .connect \u00b6 In web3 all data has the state, so it become easier to navigate through it and make agent experience better. To be sure that you are working with actual state Cyb needs to manage connection to web3 providers. Our purpose is to build web3 browser that is agnostic from addressing, identity and consensus protocols. But currently we use ipfs, parity-light and cyberd nodes to show off possible experience at early stage of web3 development without necessity to connect to web3 provider at all (be your own web3 provider) for basic needs such as popular static content surfing and simple transfers of tokens. <illustration> Cyb is hiding all complexities of web3 connections under one colorful indicator that range from green to red. Ideally it works like indicator of internet connection we all used to see in smartphones. Connection indicator cyberlinked to a connect app that is integral part of id bar. It gives an ability for an agent to understand status of connections and chose web3 providers. <api-definition> Ultimate purpose of connect is to remove necessity of agents to manually switch between networks. Agent do not need to think about switching across networks. It is a goal of app developers and browser vendors to define an approach that allow seamless interaction during web3 experience with all network magic happens underneath. Cyb is developing in a way that allow async interactions with several peer-to-peer networks in an app context. .keys \u00b6 Purpose of id bar is to enable the concept of identity. Using identity an agent is able to authenticate messages and sign transactions in web3. Cyb assumes that an agent interacting with web3 is using active identity, but offers ability to change id of a signed transaction during signing. Agent understand which id is active using identicon. Cyb computes unique and deterministic identicons for every id, but offer agent to set any identicon for local pleasure. Clicking on id bar allow agent to choose active identity from a keys app. <illustration> Keys app is inherent component of id bar and embedded in Cyb. This app allows to store cryptographic secrets. Think of it as lastpass you don't need to trust that is able to compute different addresses, one time passwords and signatures in the context of an app. The following convention is used for keys : id: String, chainId: Number keystore: Promise <String> mnemonic: String derivationPath?: Promise <String> otherAddresses: Array <String> privateKey: String publicKey: Promise <String> type: String subtype: String The following API is being used to programmatically interact with id bar: setDefaultId(addressIndex: Number): Promise <Boolean> sign(transactionObject: Object): Promise <String> signMessage(messageObject: Object): Promise <String> verifyMessage(verificationObject: Object): Promise <Boolean> .cyber \u00b6 It happens then agent knows some content address but have no idea in which network it can be retrieved as well as what app can deal with it. That is why Cyb has default integration with cyber [CYBER] protocol. Cyb append .cyber app for all request without a dot. .cyber is an app that has simple interface to cyberd, which returns prediction of related cyberlinks thus agent can get required resource directly through peer-to-peer network. Cyb has a setting of default search engine, thus an agent can plug a search she wants. <api-definition> .pins \u00b6 App bar is a place where user can quickly get access to most used web3 objects. User can pin such objects by clicking on button \"favourite\" on navigation bar and then it will appear in app bar. Cyberlink manager is an attached app that allow agents to group and tag pins. .sign \u00b6 .sign allows users to sign messages and transactions in a way that brings web3 experience to the whole new level. Browser use embedded app for signing transactions so user can be always sure that transaction details are valid. In a web2 there is no inherent mechanism to be sure that overlay of an app is produced by a browser and not an app itself. Cyb solves this problem deterministically generating background and sound of overlay window in a way that an underlying app cannot know the seed for generating desired sound and visual pattern. The user need to remember its unique pattern once to safely interacting with different apps including not so trusted. Another problem we are approach to solve with .sign is deferred transactions. Cyb has its own address for which an agent can delegate some rights. Using this API app developer can create a logic that allow create and execute complex sequences of transactions client side. Since inception of Ethereum we sign thousands of transactions and miss even more. That is why we believe this feature is critical for awesome web3 experience. .crr \u00b6 According to 3 rules of root registry every developer can deliver best possible experience for their agents. That is why we want to mix the best from every word in our worlds in our implementation of root registry. To bring better user experience about 3k of records will be cybersquatted to align interests of existing app developers and agents who look for a beautiful, simple and trustful experience. .crr is a potpourri of the most well known concepts consolidated under one namespace! Let me introduce what is included in the shake: programming languages : up to 200 names common programs : up to 100 names tokens : Up to 1k names top-level domains : up to 300 names top english words : up to 1k names utf symbols : up to 300 names Initially cyber\u2022Congress will own all this cybersquatted records. In order to improve probability of adoption of .crr cyber\u2022Congres will distribute this names to original app developers based on proof of dns mechanism. That is, names for programming languages, common programs, tokens and top-level domains will be distributed based on a proof of dns according to verified registry produced by cyber\u2022Congress. Top english words and utf symbols will be distributed using competitions, grants and awards produced by cyber\u2022Congress. Initially unregistered names in .crr will be distributed under flat fee for cyb root registry owner in Chaingear. We are going to start from 1 ETH for every name and will see will it be enough to protect from abusive squatting or not. It is possible that we will switch to auction form of distribution in a future. .cyb \u00b6 Extension over .crr . Added fields: logo, tagline, manifest, meta, code, crr. As result it become suitable to be an app store for browser. Appstore treats pinned apps as installed if at leas one permission is granted. .access \u00b6 Permission management is of paramount importance in the process of safe application distribution. We want to improve upon 3 critical aspects of permission management in web: app authentication resource management dynamic permissions App authentication is hard in web2. You need somehow know the origin and this is practically hard in a face of government level adversaries, than you must compute hash of received file and compare it with a file hash received from origin. Due to practical complexity nobody do that. In web3 if you know that address is correct authentication is done automagically. That is why browser can easily verify that permission is granted for expected app and not malicious. Resource management was not in place. In web2 all permission systems was primarily build around a concept of granting access to a particular data which browser has access to. While this approach find itself useful it just not enough to run any application from untrusted developers. Computing resources has fundamental value now, thus must be carefully managed and metered. In web3 its weird that any untrusted app can eat all resources of a machine in no time. Moreover, if an application is executed in a sandbox all we need to feel ourselves safe (in addition to authenticated permissions) is ensure that app do not eat more resources than expected. Libraries that help app developers to mine some proof-of-work algorithms using visitor machine become ubiquitous. Practically that means that in addition to shity ads web2 users will experience even more worse web experience: greedy, slow and battery consuming apps are coming. The answer to this upcoming problem in a browser permission system which is able to produce bounds on apps consumption of fundamental resources such as cpu, gpu, ram, storage and broadband. We believe that resource management must be in the core of web3 application engine. We are currently doing research on how that can be implemented: containerisation seems to be low hanging fruit that can be embedded right into web experience. Permission affordances . Current permission systems are static in a sense that browser provide limited set of predefined apis. Cookies, location, camera, microphone, sound and notifications: that is very limited set of things browsers can afford. Permissions of a third party developers are not native for a browsers either. Browser just don't care about what data with which apps agent want to share. We ask ourselves what if a browser can ask apps what kind of permissions they can provide thus exposing this permission system to any other apps? We believe this approach will allow web3 developers provide experience inaccessible for previous architectures. .feed \u00b6 Notification panel displaying all pending transactions and web3 events corresponding to certain account. Settings button leads to settings page where user can manage connection to IPFS, Ethereum and Cyber nodes (local or remote ways). .ipfs \u00b6 This app is a third party app developed by IPFS Shipyard. This is very basic app for interacting with ipfs. .eth \u00b6 Simple app which ger DURI requests and route requests to ethereum node (contracts, transactions and blocks). Else resolve ENS. .wallet \u00b6 We believe that transfer of tokens is very basic experience in a web3 thus want to provide embedded in browser wallet app as soon as possible. Thus we consider either to develop our own bicycle or partner with some 3d party wallet developer. .dev \u00b6 Developers experience is critical for the whole web3 adoption. This app helps to develop and publish web3 apps. .cg \u00b6 Chaingear is an app that help developers create ethereum based CRUD databases. We believe it will help developers to adopt web3 easier the same they MySQL helped to site developers in the very beginning of web. .help \u00b6 Help is a two way help application. Using this app cyb helps agents to use itself. Using .help agents help Cyb evolve. As an open source project we are welcome for contributions. Gitcoin is an excellent instrument that we use for delegating tasks for community a processing payments for completed ones. We have our vision of how to develop browser and what kind of features develop first. But we give an opportunity for community to decide and vote with tokens what kind of browser we need to see in near future. Our product Chaingear is also made for this. We use user's feedback to make products better. So we provide options for bug reporting and feedback leaving on every page. .settings \u00b6 Cyb settings Saga on privacy and anonymity \u00b6 ... On censorship resistance \u00b6 ...","title":"Whitepaper"},{"location":"cyb/cyb/#cyb-web3-browser","text":"@xhipster @asadovka Concept. Definitions are work in progress. Current implementation is not in comply with spec yet","title":"Cyb: web3 browser"},{"location":"cyb/cyb/#abstract","text":"Cyb is a friendly software robot who helps you explore the universes. Put it simply it just a web3 browser. At the very beginning cyb is focusing on developers and advanced blockchain users who are able to work with private keys and transactions. But we see how Cyb becomes friendly for everyone who wants to interact with consensus computers in a web of the third generation. This new web is designed to free developers from outdated things such as html and v8. So developers can use any markup, execution and rendering engine they wish. That is why we don't focus on implementation of mentioned things. Instead this paper discuss implementation agnostic concepts of browser that are simple enough to be adopted by web3 developers. Initially we design Cyb for conventional desktop browsing. But suggested concepts can be easily used for mobile, voice, vr and robotics implementations.","title":"Abstract"},{"location":"cyb/cyb/#introduction","text":"Current state of web3 experience is non satisfactory. Still we did not meet any piece of software that is able to deliver deep, emotional web3 experience. So we decide to bring to the table one contender that strictly follows web3 principles defined by ourselves ;-) In a rush for this passion we define the following web3 apps which we believe together implement the full web3 vision in the context of a browsing for web3 agents and app developers: .main : main page for every joe .path : navigation bar and its backend .connect : connection manager and state widget .keys : keystore interface and id widget .cyber : cyberd node manager and app for link chains .pins : favourites backend and application bar .sign : phishing resistant signer for messages and transactions with scheduler .crr : cyb root registry .cyb : origianl web3 appstore .access : permission manager that respects agents' resources .feed : notification backend and feed app .ipfs : ipfs node manager and agent experience .eth : ethereum node manager + ens resolver .wallet : universal wallet ux .help : cyb educational library and feedback mechanism .dev : web3 development tool with support of contracts .cg : all the things chaingearable .settings : cyb settings All this apps are considered as core apps and are included in every Cyb distribution. Let us describe in details every app as a pure concept.","title":"Introduction"},{"location":"cyb/cyb/#main","text":"Purpose of the main app is to make agent happier in a moment it returns for surfing and between experiences. Main page of the browser consists of three main elements: search bar: provides all search functions relevance bar: the most relevant cyberlinks for a particular agent footer: cyberlinks to ecosystem resources which are important for education and contribution","title":".main"},{"location":"cyb/cyb/#path","text":"Navigation bar in Cyb is based on the following elements: back button - returns user to the previous state of web3 agent search bar - provides direct access to certain state star button - allow users to pin cyberlinks forward button - brings user to the future state based on Cyb prediction Search bar is used to browse web3. With the help of DURA with knowledge of application involved ( . ) it can get content across different content addressing protocols such as IPFS, DAT, SWARM, and inside blockchains, tangles and DAGs thus forming heterogeneous environment of web3. In web3 vision doc we describe in details a concept of web3 browsing based on DURA specs. That is, in web3 appending \"dot\" works very different in comparison with web2. Dot is literally a search query to a particular app that also has a content address in heterogeneous network. All symbols after \"dot\" make a map with content address of an app in root registry, and all data before \"dot\" is a query parameter to an app. <illustration> For example: .help query will open Cyb help app. chaingear.help will open chaingear info page in the help app. Query without \"dot\" will be automatically redirected to search in cyberd (Note: queries without dot is synonym to <your-query>.cyber ). Empty query always leads to the main page. . query returns a root registry that is being used by default in Cyb. <api-definition> All cyberlinks that was requested by agent can be accessed using path app that is integral part of Cyb experience.","title":".path"},{"location":"cyb/cyb/#connect","text":"In web3 all data has the state, so it become easier to navigate through it and make agent experience better. To be sure that you are working with actual state Cyb needs to manage connection to web3 providers. Our purpose is to build web3 browser that is agnostic from addressing, identity and consensus protocols. But currently we use ipfs, parity-light and cyberd nodes to show off possible experience at early stage of web3 development without necessity to connect to web3 provider at all (be your own web3 provider) for basic needs such as popular static content surfing and simple transfers of tokens. <illustration> Cyb is hiding all complexities of web3 connections under one colorful indicator that range from green to red. Ideally it works like indicator of internet connection we all used to see in smartphones. Connection indicator cyberlinked to a connect app that is integral part of id bar. It gives an ability for an agent to understand status of connections and chose web3 providers. <api-definition> Ultimate purpose of connect is to remove necessity of agents to manually switch between networks. Agent do not need to think about switching across networks. It is a goal of app developers and browser vendors to define an approach that allow seamless interaction during web3 experience with all network magic happens underneath. Cyb is developing in a way that allow async interactions with several peer-to-peer networks in an app context.","title":".connect"},{"location":"cyb/cyb/#keys","text":"Purpose of id bar is to enable the concept of identity. Using identity an agent is able to authenticate messages and sign transactions in web3. Cyb assumes that an agent interacting with web3 is using active identity, but offers ability to change id of a signed transaction during signing. Agent understand which id is active using identicon. Cyb computes unique and deterministic identicons for every id, but offer agent to set any identicon for local pleasure. Clicking on id bar allow agent to choose active identity from a keys app. <illustration> Keys app is inherent component of id bar and embedded in Cyb. This app allows to store cryptographic secrets. Think of it as lastpass you don't need to trust that is able to compute different addresses, one time passwords and signatures in the context of an app. The following convention is used for keys : id: String, chainId: Number keystore: Promise <String> mnemonic: String derivationPath?: Promise <String> otherAddresses: Array <String> privateKey: String publicKey: Promise <String> type: String subtype: String The following API is being used to programmatically interact with id bar: setDefaultId(addressIndex: Number): Promise <Boolean> sign(transactionObject: Object): Promise <String> signMessage(messageObject: Object): Promise <String> verifyMessage(verificationObject: Object): Promise <Boolean>","title":".keys"},{"location":"cyb/cyb/#cyber","text":"It happens then agent knows some content address but have no idea in which network it can be retrieved as well as what app can deal with it. That is why Cyb has default integration with cyber [CYBER] protocol. Cyb append .cyber app for all request without a dot. .cyber is an app that has simple interface to cyberd, which returns prediction of related cyberlinks thus agent can get required resource directly through peer-to-peer network. Cyb has a setting of default search engine, thus an agent can plug a search she wants. <api-definition>","title":".cyber"},{"location":"cyb/cyb/#pins","text":"App bar is a place where user can quickly get access to most used web3 objects. User can pin such objects by clicking on button \"favourite\" on navigation bar and then it will appear in app bar. Cyberlink manager is an attached app that allow agents to group and tag pins.","title":".pins"},{"location":"cyb/cyb/#sign","text":".sign allows users to sign messages and transactions in a way that brings web3 experience to the whole new level. Browser use embedded app for signing transactions so user can be always sure that transaction details are valid. In a web2 there is no inherent mechanism to be sure that overlay of an app is produced by a browser and not an app itself. Cyb solves this problem deterministically generating background and sound of overlay window in a way that an underlying app cannot know the seed for generating desired sound and visual pattern. The user need to remember its unique pattern once to safely interacting with different apps including not so trusted. Another problem we are approach to solve with .sign is deferred transactions. Cyb has its own address for which an agent can delegate some rights. Using this API app developer can create a logic that allow create and execute complex sequences of transactions client side. Since inception of Ethereum we sign thousands of transactions and miss even more. That is why we believe this feature is critical for awesome web3 experience.","title":".sign"},{"location":"cyb/cyb/#crr","text":"According to 3 rules of root registry every developer can deliver best possible experience for their agents. That is why we want to mix the best from every word in our worlds in our implementation of root registry. To bring better user experience about 3k of records will be cybersquatted to align interests of existing app developers and agents who look for a beautiful, simple and trustful experience. .crr is a potpourri of the most well known concepts consolidated under one namespace! Let me introduce what is included in the shake: programming languages : up to 200 names common programs : up to 100 names tokens : Up to 1k names top-level domains : up to 300 names top english words : up to 1k names utf symbols : up to 300 names Initially cyber\u2022Congress will own all this cybersquatted records. In order to improve probability of adoption of .crr cyber\u2022Congres will distribute this names to original app developers based on proof of dns mechanism. That is, names for programming languages, common programs, tokens and top-level domains will be distributed based on a proof of dns according to verified registry produced by cyber\u2022Congress. Top english words and utf symbols will be distributed using competitions, grants and awards produced by cyber\u2022Congress. Initially unregistered names in .crr will be distributed under flat fee for cyb root registry owner in Chaingear. We are going to start from 1 ETH for every name and will see will it be enough to protect from abusive squatting or not. It is possible that we will switch to auction form of distribution in a future.","title":".crr"},{"location":"cyb/cyb/#cyb","text":"Extension over .crr . Added fields: logo, tagline, manifest, meta, code, crr. As result it become suitable to be an app store for browser. Appstore treats pinned apps as installed if at leas one permission is granted.","title":".cyb"},{"location":"cyb/cyb/#access","text":"Permission management is of paramount importance in the process of safe application distribution. We want to improve upon 3 critical aspects of permission management in web: app authentication resource management dynamic permissions App authentication is hard in web2. You need somehow know the origin and this is practically hard in a face of government level adversaries, than you must compute hash of received file and compare it with a file hash received from origin. Due to practical complexity nobody do that. In web3 if you know that address is correct authentication is done automagically. That is why browser can easily verify that permission is granted for expected app and not malicious. Resource management was not in place. In web2 all permission systems was primarily build around a concept of granting access to a particular data which browser has access to. While this approach find itself useful it just not enough to run any application from untrusted developers. Computing resources has fundamental value now, thus must be carefully managed and metered. In web3 its weird that any untrusted app can eat all resources of a machine in no time. Moreover, if an application is executed in a sandbox all we need to feel ourselves safe (in addition to authenticated permissions) is ensure that app do not eat more resources than expected. Libraries that help app developers to mine some proof-of-work algorithms using visitor machine become ubiquitous. Practically that means that in addition to shity ads web2 users will experience even more worse web experience: greedy, slow and battery consuming apps are coming. The answer to this upcoming problem in a browser permission system which is able to produce bounds on apps consumption of fundamental resources such as cpu, gpu, ram, storage and broadband. We believe that resource management must be in the core of web3 application engine. We are currently doing research on how that can be implemented: containerisation seems to be low hanging fruit that can be embedded right into web experience. Permission affordances . Current permission systems are static in a sense that browser provide limited set of predefined apis. Cookies, location, camera, microphone, sound and notifications: that is very limited set of things browsers can afford. Permissions of a third party developers are not native for a browsers either. Browser just don't care about what data with which apps agent want to share. We ask ourselves what if a browser can ask apps what kind of permissions they can provide thus exposing this permission system to any other apps? We believe this approach will allow web3 developers provide experience inaccessible for previous architectures.","title":".access"},{"location":"cyb/cyb/#feed","text":"Notification panel displaying all pending transactions and web3 events corresponding to certain account. Settings button leads to settings page where user can manage connection to IPFS, Ethereum and Cyber nodes (local or remote ways).","title":".feed"},{"location":"cyb/cyb/#ipfs","text":"This app is a third party app developed by IPFS Shipyard. This is very basic app for interacting with ipfs.","title":".ipfs"},{"location":"cyb/cyb/#eth","text":"Simple app which ger DURI requests and route requests to ethereum node (contracts, transactions and blocks). Else resolve ENS.","title":".eth"},{"location":"cyb/cyb/#wallet","text":"We believe that transfer of tokens is very basic experience in a web3 thus want to provide embedded in browser wallet app as soon as possible. Thus we consider either to develop our own bicycle or partner with some 3d party wallet developer.","title":".wallet"},{"location":"cyb/cyb/#dev","text":"Developers experience is critical for the whole web3 adoption. This app helps to develop and publish web3 apps.","title":".dev"},{"location":"cyb/cyb/#cg","text":"Chaingear is an app that help developers create ethereum based CRUD databases. We believe it will help developers to adopt web3 easier the same they MySQL helped to site developers in the very beginning of web.","title":".cg"},{"location":"cyb/cyb/#help","text":"Help is a two way help application. Using this app cyb helps agents to use itself. Using .help agents help Cyb evolve. As an open source project we are welcome for contributions. Gitcoin is an excellent instrument that we use for delegating tasks for community a processing payments for completed ones. We have our vision of how to develop browser and what kind of features develop first. But we give an opportunity for community to decide and vote with tokens what kind of browser we need to see in near future. Our product Chaingear is also made for this. We use user's feedback to make products better. So we provide options for bug reporting and feedback leaving on every page.","title":".help"},{"location":"cyb/cyb/#settings","text":"Cyb settings","title":".settings"},{"location":"cyb/cyb/#saga-on-privacy-and-anonymity","text":"...","title":"Saga on privacy and anonymity"},{"location":"cyb/cyb/#on-censorship-resistance","text":"...","title":"On censorship resistance"},{"location":"cyb/web3-vision/","text":"DURA: a missing piece for web3 \u00b6 @xhipster cyberCongress Early draft for web3 summit. Looking for a feedback . Abstract \u00b6 Originally an idea of web3 was inspired by Gavin Wood in 2014. A vision of Gavin was around 4 implementable concepts: content addressing, cryptographic identities, consensus computing and browsers. In parallel an idea of Interplanetary File System has been developed by Juan Benet. IPFS creates a foundation for web3: a system of content addressing and cryptographic identities. Since 2014 consensus computing has suffered insanely rapid development so one more missing piece is also in place. Still missing piece is a web3 browsing. Some projects such as Metamask has demonstrated a taste of web3. But one critical component in terms of browsing is just not there. URL scheme is outdated in terms of desired web3 properties and needs a drop in replacement. In this paper firstly we discuss necessary properties that we expect from web3. Based on this analysis we propose DURA scheme aka Distributed Unified Resource Address as drop in replacement for URLs that is being implemented in web3 browser Cyb . We believe DURA is a dump enough scheme (your captain) which can bring up basic consensus across web3 browser vendors due to simplicity, openness and protocol agnostic approach. Introduction \u00b6 Conventional protocols of the Internet such as TCP/IP, DNS, HTTPS and URL brought a web into the point there it is now. Along with all benefits they has created this protocols brought more problem into the table. Globality being a key property of the the web since inception is under real threat. Speed of connections degrade with network growth and from ubiquitous government interventions into privacy and security of web users. One property, not obvious in the beginning, become really important with everyday usage of the Internet: its ability to exchange permanent hyperlinks thus they would not break after time have pass. Reliance on one at a time internet service provider architecture allow governments censor packets. This fact is the last straw in conventional web stack for every engineer who is concerned about the future of our children. Other properties while being not so critical are very desirable: offline and real-time. Average internet user being offline must have ability to work with the state it has and after acquiring connection being able to sync with global state and continue verify state's validity in realtime while having connection. Now this properties offered on app level while such properties must be integrated into lower level protocols: into very core of web3. Speed \u00b6 Usability researches state that interactions that do not make sense in 100 milliseconds are considered as slow by an agent. Achieving such instant speeds is nearly impossible in the current stack of protocols. The following generation of the web must enable instant responses user requests. Necessity for lookup a location of resource using remote machine is an obvious bottleneck for reaching desired properties. Globality \u00b6 Current internet is starting to split into regions mutually inaccessible for each other. China is de facto such a region. Some countries are very close to joining the club. It is of paramount importance that web3 would remain global even in the face of government level adversaries. Security \u00b6 Current web applications are still in its infancy in term of permission abilities. Security of web apps is a very complicated topic. But one thing is obvious: it is hard to setup a secure system with third party apps without built in authentication system of the code being run on client machine. Current system of mutable resource location based on certificate authorities can not be safe by design. Permanent \u00b6 We are all experience broken links. IPFS has immunity to this issue. As long as you keep a file anybody can access it using globally defined immutable in time address of this file computed from the file itself. Mesh ready \u00b6 Current internet paradigm is based on 1 internet provider paradigm. That is basically a bad shit, because in general even if you have 2 or more internet connection like wifi and lte your device and/or operation system don't allow you to get the full possibilities of connectivity enforcing you to use only one connection at a time. Another major bad shit in current internet architecture is that your device is treated as leech by default. Every device keeps data necessary to being useful for surround devices. Huge portion of our network traffic goes not from origin server but from ISP cache. That means that changing a paradigm we can get to very different topology where our neighbors are our web3 providers. Verifiability \u00b6 ... Privacy and Anonymity \u00b6 ... Offline \u00b6 ... Realtime \u00b6 ... Roles \u00b6 In a web of the third generation roles are not like in a web2. There is no clear split on users, internet providers and sites. Key difference is such that interactions can happen truly peer to peer the one can be sovereign enough to be their own internet. Apps. Any content hash can be an app if it is known how to parse it. Agents. An app can become an agent if she can prove that she exist by digital signature. Providers. Any agent that is able to serve content can be web3 provider. Content addresses \u00b6 To understand why they so important we need to understand a difference in foundational concept of web2 and web3. Web2: Where => What-How : You must know resource location on a particular server to retrieve it. Web3: What => How : Instead of location based paradigm web3 is based on the content addressing paradigm. Key point is that we do not need to have knowledge of resource location in order to link to an object. In a web3 the answer to the How is either local or blockchain based registry with simple map between input address and address of an app. Cryptographic identities \u00b6 ... Consensus computers \u00b6 There are some very exhaustive articles around the topic of web3 which are really about consensus computing part of it. Worth to note that saying that any particular blockchain or even all blockchains altogether is web3 is like saying that databases is world wide web. Yes databases technology contributed to a development of www, but without several protocols this databases would not become interconnected through billions of web sites. Remember that in order to implement a vision of full web3 potential we need to find drop in replacement for every piece of current protocol stack: IP, TCP, HTTPS, DNS and URL. None of currently deployed blockchain technologies don't have necessary properties to directly replace dinosaurs. I would say that distributed ledger technology is a better database stack for the upcoming web. DURA scheme \u00b6 Distributed Unified Resource Address or put simply DURA is a more simpler and trustful scheme that has been used in a conventional web. It doesn't requires central authorities such as ICANN or others: [local-handler]://[content-address].[root-registry-app]/[app-navigation] We believe that a term cyberlink can be used for DURA links in order to differentiate with hyperlinks of previous internet architecture. Local handler \u00b6 dura:// It is a local handler that every os can handle. Being fully optional it can be very important in the very beginning of web3. Content address \u00b6 dura://QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa If a browser is able to understand in which network she can resolve this particular content hash it can resolve it without much ado. Though we expect that IPFS will be not the only system and it become practically hard to say with 100% certainty whether given hash is ipfs hash or swarm or torrent or some other address type. That is there the concept of a root registry came into the game. In some sense it serves as an alternative to a self describing scheme used in CIDs. In some sense its not as it offer visually more sound links for agents. We believe that the root registry and self description concepts are complementary to each other. Root registry \u00b6 Current state of DNS root management is outdated. The most bad thing is that we still must to trust the most important things in our lives to strange organizations such as ICANN, IANA. We can ask ourselves why after 40 years of ubiquitous computer movement we still don't have simple common knowledge about what file extensions must be used with what software? Our proposal is a concept of a root register. Structure of root register is a simple map between short name and ipfs hash of a program that is being triggered: com:QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa io:QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa exe:QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa pdf:QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa eth:QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa A name must be letter, number or hypen. Purpose of a root register is to reach some very basic os and network agnostic agreement about what extensions with what programs must be used. Of course the problem with such registry is that it must be somehow and somewhere maintained. Three rules of a root registry \u00b6 Software vendors must compete for a better root registry. Software vendors must add setting with a change of a root registry. Agents of browsers and operation systems must have ability to overwrite maps for local pleasure. One of the implementation is a cyb root registry Root registry app \u00b6 cyb://QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa.ipfs This is an example of full DURA link that is able get hash of ipfs app in a root registry, than throw predicate into this app and get a desired resource from a peer to peer network. Note: need to add real case of ipfs DURA app being used in Cyb. Note: need to add real case on how web2 link can be resolved using DURA approach. App navigation \u00b6 Traditionally its up to developers how to structure navigation within an app. But we believe that some scheme will be invented to differentiate statefull and stateless links as it is very important for DURA extensions \u00b6 A lot of cool stuff can be implemented using extensions. Extension is any symbol that adds predictable an logical behavior for parsing and rendering of requests. Examples: - local paths - connection links - URL parameters Key principle is that semantics is programmable and can be delivered from a distributed network using symbols that has been registered in a root registry. Where extension \u00b6 Keyword: - / before content address is a local path Examples: /users/xhipster/cybernode.ai /ethereum/cybersearch.eth Linkchain extension \u00b6 Keyword: - @ between DURA statements Examples: xhipster.eth@cybercongress.ai Need to find cyberlinks between two cyberlinks. Reference \u00b6 ...","title":"Web3 vision"},{"location":"cyb/web3-vision/#dura-a-missing-piece-for-web3","text":"@xhipster cyberCongress Early draft for web3 summit. Looking for a feedback .","title":"DURA: a missing piece for web3"},{"location":"cyb/web3-vision/#abstract","text":"Originally an idea of web3 was inspired by Gavin Wood in 2014. A vision of Gavin was around 4 implementable concepts: content addressing, cryptographic identities, consensus computing and browsers. In parallel an idea of Interplanetary File System has been developed by Juan Benet. IPFS creates a foundation for web3: a system of content addressing and cryptographic identities. Since 2014 consensus computing has suffered insanely rapid development so one more missing piece is also in place. Still missing piece is a web3 browsing. Some projects such as Metamask has demonstrated a taste of web3. But one critical component in terms of browsing is just not there. URL scheme is outdated in terms of desired web3 properties and needs a drop in replacement. In this paper firstly we discuss necessary properties that we expect from web3. Based on this analysis we propose DURA scheme aka Distributed Unified Resource Address as drop in replacement for URLs that is being implemented in web3 browser Cyb . We believe DURA is a dump enough scheme (your captain) which can bring up basic consensus across web3 browser vendors due to simplicity, openness and protocol agnostic approach.","title":"Abstract"},{"location":"cyb/web3-vision/#introduction","text":"Conventional protocols of the Internet such as TCP/IP, DNS, HTTPS and URL brought a web into the point there it is now. Along with all benefits they has created this protocols brought more problem into the table. Globality being a key property of the the web since inception is under real threat. Speed of connections degrade with network growth and from ubiquitous government interventions into privacy and security of web users. One property, not obvious in the beginning, become really important with everyday usage of the Internet: its ability to exchange permanent hyperlinks thus they would not break after time have pass. Reliance on one at a time internet service provider architecture allow governments censor packets. This fact is the last straw in conventional web stack for every engineer who is concerned about the future of our children. Other properties while being not so critical are very desirable: offline and real-time. Average internet user being offline must have ability to work with the state it has and after acquiring connection being able to sync with global state and continue verify state's validity in realtime while having connection. Now this properties offered on app level while such properties must be integrated into lower level protocols: into very core of web3.","title":"Introduction"},{"location":"cyb/web3-vision/#speed","text":"Usability researches state that interactions that do not make sense in 100 milliseconds are considered as slow by an agent. Achieving such instant speeds is nearly impossible in the current stack of protocols. The following generation of the web must enable instant responses user requests. Necessity for lookup a location of resource using remote machine is an obvious bottleneck for reaching desired properties.","title":"Speed"},{"location":"cyb/web3-vision/#globality","text":"Current internet is starting to split into regions mutually inaccessible for each other. China is de facto such a region. Some countries are very close to joining the club. It is of paramount importance that web3 would remain global even in the face of government level adversaries.","title":"Globality"},{"location":"cyb/web3-vision/#security","text":"Current web applications are still in its infancy in term of permission abilities. Security of web apps is a very complicated topic. But one thing is obvious: it is hard to setup a secure system with third party apps without built in authentication system of the code being run on client machine. Current system of mutable resource location based on certificate authorities can not be safe by design.","title":"Security"},{"location":"cyb/web3-vision/#permanent","text":"We are all experience broken links. IPFS has immunity to this issue. As long as you keep a file anybody can access it using globally defined immutable in time address of this file computed from the file itself.","title":"Permanent"},{"location":"cyb/web3-vision/#mesh-ready","text":"Current internet paradigm is based on 1 internet provider paradigm. That is basically a bad shit, because in general even if you have 2 or more internet connection like wifi and lte your device and/or operation system don't allow you to get the full possibilities of connectivity enforcing you to use only one connection at a time. Another major bad shit in current internet architecture is that your device is treated as leech by default. Every device keeps data necessary to being useful for surround devices. Huge portion of our network traffic goes not from origin server but from ISP cache. That means that changing a paradigm we can get to very different topology where our neighbors are our web3 providers.","title":"Mesh ready"},{"location":"cyb/web3-vision/#verifiability","text":"...","title":"Verifiability"},{"location":"cyb/web3-vision/#privacy-and-anonymity","text":"...","title":"Privacy and Anonymity"},{"location":"cyb/web3-vision/#offline","text":"...","title":"Offline"},{"location":"cyb/web3-vision/#realtime","text":"...","title":"Realtime"},{"location":"cyb/web3-vision/#roles","text":"In a web of the third generation roles are not like in a web2. There is no clear split on users, internet providers and sites. Key difference is such that interactions can happen truly peer to peer the one can be sovereign enough to be their own internet. Apps. Any content hash can be an app if it is known how to parse it. Agents. An app can become an agent if she can prove that she exist by digital signature. Providers. Any agent that is able to serve content can be web3 provider.","title":"Roles"},{"location":"cyb/web3-vision/#content-addresses","text":"To understand why they so important we need to understand a difference in foundational concept of web2 and web3. Web2: Where => What-How : You must know resource location on a particular server to retrieve it. Web3: What => How : Instead of location based paradigm web3 is based on the content addressing paradigm. Key point is that we do not need to have knowledge of resource location in order to link to an object. In a web3 the answer to the How is either local or blockchain based registry with simple map between input address and address of an app.","title":"Content addresses"},{"location":"cyb/web3-vision/#cryptographic-identities","text":"...","title":"Cryptographic identities"},{"location":"cyb/web3-vision/#consensus-computers","text":"There are some very exhaustive articles around the topic of web3 which are really about consensus computing part of it. Worth to note that saying that any particular blockchain or even all blockchains altogether is web3 is like saying that databases is world wide web. Yes databases technology contributed to a development of www, but without several protocols this databases would not become interconnected through billions of web sites. Remember that in order to implement a vision of full web3 potential we need to find drop in replacement for every piece of current protocol stack: IP, TCP, HTTPS, DNS and URL. None of currently deployed blockchain technologies don't have necessary properties to directly replace dinosaurs. I would say that distributed ledger technology is a better database stack for the upcoming web.","title":"Consensus computers"},{"location":"cyb/web3-vision/#dura-scheme","text":"Distributed Unified Resource Address or put simply DURA is a more simpler and trustful scheme that has been used in a conventional web. It doesn't requires central authorities such as ICANN or others: [local-handler]://[content-address].[root-registry-app]/[app-navigation] We believe that a term cyberlink can be used for DURA links in order to differentiate with hyperlinks of previous internet architecture.","title":"DURA scheme"},{"location":"cyb/web3-vision/#local-handler","text":"dura:// It is a local handler that every os can handle. Being fully optional it can be very important in the very beginning of web3.","title":"Local handler"},{"location":"cyb/web3-vision/#content-address","text":"dura://QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa If a browser is able to understand in which network she can resolve this particular content hash it can resolve it without much ado. Though we expect that IPFS will be not the only system and it become practically hard to say with 100% certainty whether given hash is ipfs hash or swarm or torrent or some other address type. That is there the concept of a root registry came into the game. In some sense it serves as an alternative to a self describing scheme used in CIDs. In some sense its not as it offer visually more sound links for agents. We believe that the root registry and self description concepts are complementary to each other.","title":"Content address"},{"location":"cyb/web3-vision/#root-registry","text":"Current state of DNS root management is outdated. The most bad thing is that we still must to trust the most important things in our lives to strange organizations such as ICANN, IANA. We can ask ourselves why after 40 years of ubiquitous computer movement we still don't have simple common knowledge about what file extensions must be used with what software? Our proposal is a concept of a root register. Structure of root register is a simple map between short name and ipfs hash of a program that is being triggered: com:QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa io:QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa exe:QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa pdf:QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa eth:QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa A name must be letter, number or hypen. Purpose of a root register is to reach some very basic os and network agnostic agreement about what extensions with what programs must be used. Of course the problem with such registry is that it must be somehow and somewhere maintained.","title":"Root registry"},{"location":"cyb/web3-vision/#three-rules-of-a-root-registry","text":"Software vendors must compete for a better root registry. Software vendors must add setting with a change of a root registry. Agents of browsers and operation systems must have ability to overwrite maps for local pleasure. One of the implementation is a cyb root registry","title":"Three rules of a root registry"},{"location":"cyb/web3-vision/#root-registry-app","text":"cyb://QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa.ipfs This is an example of full DURA link that is able get hash of ipfs app in a root registry, than throw predicate into this app and get a desired resource from a peer to peer network. Note: need to add real case of ipfs DURA app being used in Cyb. Note: need to add real case on how web2 link can be resolved using DURA approach.","title":"Root registry app"},{"location":"cyb/web3-vision/#app-navigation","text":"Traditionally its up to developers how to structure navigation within an app. But we believe that some scheme will be invented to differentiate statefull and stateless links as it is very important for","title":"App navigation"},{"location":"cyb/web3-vision/#dura-extensions","text":"A lot of cool stuff can be implemented using extensions. Extension is any symbol that adds predictable an logical behavior for parsing and rendering of requests. Examples: - local paths - connection links - URL parameters Key principle is that semantics is programmable and can be delivered from a distributed network using symbols that has been registered in a root registry.","title":"DURA extensions"},{"location":"cyb/web3-vision/#where-extension","text":"Keyword: - / before content address is a local path Examples: /users/xhipster/cybernode.ai /ethereum/cybersearch.eth","title":"Where extension"},{"location":"cyb/web3-vision/#linkchain-extension","text":"Keyword: - @ between DURA statements Examples: xhipster.eth@cybercongress.ai Need to find cyberlinks between two cyberlinks.","title":"Linkchain extension"},{"location":"cyb/web3-vision/#reference","text":"...","title":"Reference"},{"location":"cyb-js/cyb-js/","text":"Cyb-js will come soon \u00b6","title":"Overview"},{"location":"cyb-js/cyb-js/#cyb-js-will-come-soon","text":"","title":"Cyb-js will come soon"},{"location":"cyber-markets/cyber-markets/","text":"About Markets \u00b6","title":"Overview"},{"location":"cyber-markets/cyber-markets/#about-markets","text":"","title":"About Markets"},{"location":"cyber-markets/api/Readme/","text":"Build and Run docs locally \u00b6 docker build -t build/raw-api -f ./docs/api/Dockerfile ./ && docker run -p 8080 :8080 build/raw-api","title":"Readme"},{"location":"cyber-markets/api/Readme/#build-and-run-docs-locally","text":"docker build -t build/raw-api -f ./docs/api/Dockerfile ./ && docker run -p 8080 :8080 build/raw-api","title":"Build and Run docs locally"},{"location":"cyber-markets/components/overview/","text":"Markets Components \u00b6 Component Scale Status Description Cluster Address Metrics External Kafka 1 Data Entry Kafka Manager 1 Kafka Explorer Elassandra 1 - N Data Back Exchanges Connector 1 - N Connect to CEXs/DEXs for raw data 8080:/actuator/metrics Tickers real-time 1 - N Calculate Tickers From Raw real-time data Tickers historical 1 Calculate Tickers From Raw historical data Storer 1 Write raw data to database Markets REST API 1 - N Rest Api To Markets Entities y Markets Stream API 1 - N not implemented Web Socket Api To Markets Entities y Markets Api Docs 1 - N Markets Api Docs Based On Swagger y Kafka \u00b6 Kafka is message queue used as main data entry. All raw data firstly goes into kafka, than various services consume it to provide new functionality. Elassandra (Elastic + Cassandra) \u00b6 Elassandra is used as main backend storage with linear growth fulltext-search index on source cassandra data. Exchanges Connector \u00b6 Collect raw data from centralized and decentralized exchanges such as trades, order books and put it to kafka. Storer \u00b6 Writes data from kafka topics or directly from exchanges-connector to cassandra cluster Tickers real-time \u00b6 Calculate tickers from trades which collecting exchanges connectors module. By default, the recalculation occurs every 3 seconds. Trades that do not hit the gap in 3 seconds are skipped. Tickers historical \u00b6 Calculate tickers, prices, etc and aggregate raw data which collecting exchanges connectors module. Tickers are calculated from data which stored in the Elassandra.","title":"Overview"},{"location":"cyber-markets/components/overview/#markets-components","text":"Component Scale Status Description Cluster Address Metrics External Kafka 1 Data Entry Kafka Manager 1 Kafka Explorer Elassandra 1 - N Data Back Exchanges Connector 1 - N Connect to CEXs/DEXs for raw data 8080:/actuator/metrics Tickers real-time 1 - N Calculate Tickers From Raw real-time data Tickers historical 1 Calculate Tickers From Raw historical data Storer 1 Write raw data to database Markets REST API 1 - N Rest Api To Markets Entities y Markets Stream API 1 - N not implemented Web Socket Api To Markets Entities y Markets Api Docs 1 - N Markets Api Docs Based On Swagger y","title":"Markets Components"},{"location":"cyber-markets/components/overview/#kafka","text":"Kafka is message queue used as main data entry. All raw data firstly goes into kafka, than various services consume it to provide new functionality.","title":"Kafka"},{"location":"cyber-markets/components/overview/#elassandra-elastic-cassandra","text":"Elassandra is used as main backend storage with linear growth fulltext-search index on source cassandra data.","title":"Elassandra (Elastic + Cassandra)"},{"location":"cyber-markets/components/overview/#exchanges-connector","text":"Collect raw data from centralized and decentralized exchanges such as trades, order books and put it to kafka.","title":"Exchanges Connector"},{"location":"cyber-markets/components/overview/#storer","text":"Writes data from kafka topics or directly from exchanges-connector to cassandra cluster","title":"Storer"},{"location":"cyber-markets/components/overview/#tickers-real-time","text":"Calculate tickers from trades which collecting exchanges connectors module. By default, the recalculation occurs every 3 seconds. Trades that do not hit the gap in 3 seconds are skipped.","title":"Tickers real-time"},{"location":"cyber-markets/components/overview/#tickers-historical","text":"Calculate tickers, prices, etc and aggregate raw data which collecting exchanges connectors module. Tickers are calculated from data which stored in the Elassandra.","title":"Tickers historical"},{"location":"cyber-markets/contributing/cheat-sheet/","text":"Kafka \u00b6 Stop kafka and delete kafka data(cheat sheet) \u00b6 docker stop fast-data-dev-markets docker rm fast-data-dev-markets Elassandra \u00b6 Stop elassandra and delete elassandra data(cheat sheet) \u00b6 docker stop elassandra-markets docker rm elassandra-markets Chains \u00b6 Run parity node(cheat sheet) \u00b6 sudo docker run -d -p 8545 :8545 --name parity_eth \\ -v ${ REPLACE_IT_BY_HOST_FOLDER } :/cyberdata parity/parity:stable \\ --db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 4","title":"Cheat-sheet"},{"location":"cyber-markets/contributing/cheat-sheet/#kafka","text":"","title":"Kafka"},{"location":"cyber-markets/contributing/cheat-sheet/#stop-kafka-and-delete-kafka-datacheat-sheet","text":"docker stop fast-data-dev-markets docker rm fast-data-dev-markets","title":"Stop kafka and delete kafka data(cheat sheet)"},{"location":"cyber-markets/contributing/cheat-sheet/#elassandra","text":"","title":"Elassandra"},{"location":"cyber-markets/contributing/cheat-sheet/#stop-elassandra-and-delete-elassandra-datacheat-sheet","text":"docker stop elassandra-markets docker rm elassandra-markets","title":"Stop elassandra and delete elassandra data(cheat sheet)"},{"location":"cyber-markets/contributing/cheat-sheet/#chains","text":"","title":"Chains"},{"location":"cyber-markets/contributing/cheat-sheet/#run-parity-nodecheat-sheet","text":"sudo docker run -d -p 8545 :8545 --name parity_eth \\ -v ${ REPLACE_IT_BY_HOST_FOLDER } :/cyberdata parity/parity:stable \\ --db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 4","title":"Run parity node(cheat sheet)"},{"location":"cyber-markets/contributing/contributing/","text":"Contributing to Cyber Markets \u00b6 Thank you for considering a contribution to Cyber Markets! This guide explains how to: Get started Development workflow * Get help if you encounter trouble Get in touch \u00b6 Before starting to work on a feature or a fix, please open an issue to discuss the use case or bug with us. This can save both you and us a lot of time. For any non-trivial change, we'll ask you to create a short design document explaining: Why is this change done? What's the use case? What will the API look like? (For new features) What test cases should it have? What could go wrong? How will it roughly be implemented? (We'll happily provide code pointers to save you time) Development Workflow \u00b6 Development Setup \u00b6 Please, use development environment setup guide . Make Changes \u00b6 Use this Architecture Overview as a start point for making changes. Local Check \u00b6 Several checks should passed to succeed build. Detekt code analyze tool should not report any issues JUnit tests should pass Before committing you changes, please, run local project check by: ./gradlew build //linux, mac gradlew.bat build //windows Creating Commits And Writing Commit Messages \u00b6 The commit messages that accompany your code changes are an important piece of documentation, please follow these guidelines when writing commit messages: Keep commits discrete: avoid including multiple unrelated changes in a single commit Keep commits self-contained: avoid spreading a single change across multiple commits. A single commit should make sense in isolation Add GitHub issue to CHANGELOG.md Include GitHub issue in the commit message on a first line at the beginning. Example: #123 Refactor CONTRIBUTING.md --Add Creating Commits And Writing Commit Messages Section Submitting Your Change \u00b6 After you submit your pull request, a core developer will review it. It is normal that this takes several iterations, so don't get discouraged by change requests. They ensure the high quality that we all enjoy. Getting Help \u00b6 If you run into any trouble, please reach out to us on the issue you are working on. Our Thanks \u00b6 We deeply appreciate your effort toward improving Search. For any contribution, large or small, you will be immortalized in the release notes for the version you've contributed to.","title":"Contributing"},{"location":"cyber-markets/contributing/contributing/#contributing-to-cyber-markets","text":"Thank you for considering a contribution to Cyber Markets! This guide explains how to: Get started Development workflow * Get help if you encounter trouble","title":"Contributing to Cyber Markets"},{"location":"cyber-markets/contributing/contributing/#get-in-touch","text":"Before starting to work on a feature or a fix, please open an issue to discuss the use case or bug with us. This can save both you and us a lot of time. For any non-trivial change, we'll ask you to create a short design document explaining: Why is this change done? What's the use case? What will the API look like? (For new features) What test cases should it have? What could go wrong? How will it roughly be implemented? (We'll happily provide code pointers to save you time)","title":"Get in touch"},{"location":"cyber-markets/contributing/contributing/#development-workflow","text":"","title":"Development Workflow"},{"location":"cyber-markets/contributing/contributing/#development-setup","text":"Please, use development environment setup guide .","title":"Development Setup"},{"location":"cyber-markets/contributing/contributing/#make-changes","text":"Use this Architecture Overview as a start point for making changes.","title":"Make Changes"},{"location":"cyber-markets/contributing/contributing/#local-check","text":"Several checks should passed to succeed build. Detekt code analyze tool should not report any issues JUnit tests should pass Before committing you changes, please, run local project check by: ./gradlew build //linux, mac gradlew.bat build //windows","title":"Local Check"},{"location":"cyber-markets/contributing/contributing/#creating-commits-and-writing-commit-messages","text":"The commit messages that accompany your code changes are an important piece of documentation, please follow these guidelines when writing commit messages: Keep commits discrete: avoid including multiple unrelated changes in a single commit Keep commits self-contained: avoid spreading a single change across multiple commits. A single commit should make sense in isolation Add GitHub issue to CHANGELOG.md Include GitHub issue in the commit message on a first line at the beginning. Example: #123 Refactor CONTRIBUTING.md --Add Creating Commits And Writing Commit Messages Section","title":"Creating Commits And Writing Commit Messages"},{"location":"cyber-markets/contributing/contributing/#submitting-your-change","text":"After you submit your pull request, a core developer will review it. It is normal that this takes several iterations, so don't get discouraged by change requests. They ensure the high quality that we all enjoy.","title":"Submitting Your Change"},{"location":"cyber-markets/contributing/contributing/#getting-help","text":"If you run into any trouble, please reach out to us on the issue you are working on.","title":"Getting Help"},{"location":"cyber-markets/contributing/contributing/#our-thanks","text":"We deeply appreciate your effort toward improving Search. For any contribution, large or small, you will be immortalized in the release notes for the version you've contributed to.","title":"Our Thanks"},{"location":"cyber-markets/contributing/dev-environment/","text":"Development environment \u00b6 Useful Links \u00b6 cheat sheet Prestart \u00b6 Instal Java 8 JDK Install Docker and Docker Compose Install Intellij Idea Run Kafka, Elassandra, Prometheus and Grafana \u00b6 Start containers(required) \u00b6 For mac: docker-compose -f dev-environment/env-mac.yml up -d For linux family: docker-compose -f dev-environment/env.yml up -d Bootstrap Elassandra with keyspaces(required) \u00b6 docker cp dev-environment/elassandra-bootstrap.cql elassandra-markets:/elassandra-bootstrap.cql docker exec -it elassandra-markets bash cqlsh -f elassandra-bootstrap.cql Import project to Intellij Idea \u00b6 Open Project in idea by selecting: Import Project -> selecting build.gradle file from the repository root Wait for dependency downloading and indexation Run Exchanges Connector, Tickers, or API from intellij Idea \u00b6 Go to ExchangesConnectorApplication.kt and press green triangle on left to the code (on example line 16): If, you use parity endpoint different rather that localhost:8545, than Etherdelta connector will fail due to lack of environment property PARITY_URL. Let's define it: Select \"Edit Configurations\" Add next properties: Now, run exchanges connector one more time then etherdelta connector should start. You can add environment variables in the same way for Tickers, APIs and etc.","title":"Dev-environment"},{"location":"cyber-markets/contributing/dev-environment/#development-environment","text":"","title":"Development environment"},{"location":"cyber-markets/contributing/dev-environment/#useful-links","text":"cheat sheet","title":"Useful Links"},{"location":"cyber-markets/contributing/dev-environment/#prestart","text":"Instal Java 8 JDK Install Docker and Docker Compose Install Intellij Idea","title":"Prestart"},{"location":"cyber-markets/contributing/dev-environment/#run-kafka-elassandra-prometheus-and-grafana","text":"","title":"Run Kafka, Elassandra, Prometheus and Grafana"},{"location":"cyber-markets/contributing/dev-environment/#start-containersrequired","text":"For mac: docker-compose -f dev-environment/env-mac.yml up -d For linux family: docker-compose -f dev-environment/env.yml up -d","title":"Start containers(required)"},{"location":"cyber-markets/contributing/dev-environment/#bootstrap-elassandra-with-keyspacesrequired","text":"docker cp dev-environment/elassandra-bootstrap.cql elassandra-markets:/elassandra-bootstrap.cql docker exec -it elassandra-markets bash cqlsh -f elassandra-bootstrap.cql","title":"Bootstrap Elassandra with keyspaces(required)"},{"location":"cyber-markets/contributing/dev-environment/#import-project-to-intellij-idea","text":"Open Project in idea by selecting: Import Project -> selecting build.gradle file from the repository root Wait for dependency downloading and indexation","title":"Import project to Intellij Idea"},{"location":"cyber-markets/contributing/dev-environment/#run-exchanges-connector-tickers-or-api-from-intellij-idea","text":"Go to ExchangesConnectorApplication.kt and press green triangle on left to the code (on example line 16): If, you use parity endpoint different rather that localhost:8545, than Etherdelta connector will fail due to lack of environment property PARITY_URL. Let's define it: Select \"Edit Configurations\" Add next properties: Now, run exchanges connector one more time then etherdelta connector should start. You can add environment variables in the same way for Tickers, APIs and etc.","title":"Run Exchanges Connector, Tickers, or API from intellij Idea"},{"location":"cyber-search/cyber-search/","text":"About Search \u00b6 \ud83d\ude80 Toolchain for transactions parsing and processing!!!!","title":"About Search"},{"location":"cyber-search/cyber-search/#about-search","text":"\ud83d\ude80 Toolchain for transactions parsing and processing!!!!","title":"About Search"},{"location":"cyber-search/api/","text":"Build and Run locally \u00b6 docker build -t build/raw-api -f ./docs/api/Dockerfile ./ && docker run -p 8080 :8080 build/raw-api","title":"Readme"},{"location":"cyber-search/api/#build-and-run-locally","text":"docker build -t build/raw-api -f ./docs/api/Dockerfile ./ && docker run -p 8080 :8080 build/raw-api","title":"Build and Run locally"},{"location":"cyber-search/components/bitcoin-components/","text":"Bitcoin Components \u00b6 Component Scale Description Cluster Address Metrics External Bitcoin Pump 1 Bitcoin Chain Data Kafka Pump 8080:/actuator/metrics Bitcoin Cassandra Dump 1 Dump Kafka Topics Into Cassandra 8080:/actuator/metrics Bitcoin Contract Summary 1 - N Calculates Contract Summaries (balances and etc) 8080:/actuator/metrics Bitcoin Pump \u00b6 Pumps Bitcoin raw data(block,tx,uncles) into Kafka. Bitcoin Cassandra Dump \u00b6 Consumes Bitcoin raw data from Kafka topics, translate it into cassandra entities and store them into Elassandra. Bitcoin Contract Summary \u00b6 Collects summary for each contract stored in Bitcoin chain. Consumes Kafka topics and update data in Elassandra.","title":"Bitcoin"},{"location":"cyber-search/components/bitcoin-components/#bitcoin-components","text":"Component Scale Description Cluster Address Metrics External Bitcoin Pump 1 Bitcoin Chain Data Kafka Pump 8080:/actuator/metrics Bitcoin Cassandra Dump 1 Dump Kafka Topics Into Cassandra 8080:/actuator/metrics Bitcoin Contract Summary 1 - N Calculates Contract Summaries (balances and etc) 8080:/actuator/metrics","title":"Bitcoin Components"},{"location":"cyber-search/components/bitcoin-components/#bitcoin-pump","text":"Pumps Bitcoin raw data(block,tx,uncles) into Kafka.","title":"Bitcoin Pump"},{"location":"cyber-search/components/bitcoin-components/#bitcoin-cassandra-dump","text":"Consumes Bitcoin raw data from Kafka topics, translate it into cassandra entities and store them into Elassandra.","title":"Bitcoin Cassandra Dump"},{"location":"cyber-search/components/bitcoin-components/#bitcoin-contract-summary","text":"Collects summary for each contract stored in Bitcoin chain. Consumes Kafka topics and update data in Elassandra.","title":"Bitcoin Contract Summary"},{"location":"cyber-search/components/cassandra-service/","text":"Cassandra Service \u00b6 Usage of cassandra-service Module \u00b6 \u0421assandra service module could be used in two ways: With specifying CHAIN_FAMILY environment variable Without specifying CHAIN_FAMILY environment variable Specifying CHAIN_FAMILY \u00b6 When you're specifying CHAIN_FAMILY environment variable cassandra-service module will work in the context of one keyspace based on this family. For example if CHAIN_FAMILY=BITCOIN it will create bitcoin keyspace if it doesn't exists and do all migrations for this keyspace if they're not applied. In spring context only repositories beans related to this keyspace will be available All of the cassandra work handled by CassandraRepositoriesConfiguration and it specific inheritor. This kind of interaction with cassandra-service used in dumps and contract summaries modules. Without CHAIN_FAMILY \u00b6 Without specifying CHAIN_FAMILY environment variable cassandra-service module will act like this: It will run no migrations and creations of keyspaces It will look on available keyspaces in current cassandra database based on SearchRepositoriesConfiguration inheritors and if they're exists it will initialize all repositories beans for this keyspaces and put them into spring context. This is useful when you want to interact with all of available keyspaces and indicies. For example in search-api module we need all of repositories beans to build requests.","title":"Cassandra"},{"location":"cyber-search/components/cassandra-service/#cassandra-service","text":"","title":"Cassandra Service"},{"location":"cyber-search/components/cassandra-service/#usage-of-cassandra-service-module","text":"\u0421assandra service module could be used in two ways: With specifying CHAIN_FAMILY environment variable Without specifying CHAIN_FAMILY environment variable","title":"Usage of cassandra-service Module"},{"location":"cyber-search/components/cassandra-service/#specifying-chain_family","text":"When you're specifying CHAIN_FAMILY environment variable cassandra-service module will work in the context of one keyspace based on this family. For example if CHAIN_FAMILY=BITCOIN it will create bitcoin keyspace if it doesn't exists and do all migrations for this keyspace if they're not applied. In spring context only repositories beans related to this keyspace will be available All of the cassandra work handled by CassandraRepositoriesConfiguration and it specific inheritor. This kind of interaction with cassandra-service used in dumps and contract summaries modules.","title":"Specifying CHAIN_FAMILY"},{"location":"cyber-search/components/cassandra-service/#without-chain_family","text":"Without specifying CHAIN_FAMILY environment variable cassandra-service module will act like this: It will run no migrations and creations of keyspaces It will look on available keyspaces in current cassandra database based on SearchRepositoriesConfiguration inheritors and if they're exists it will initialize all repositories beans for this keyspaces and put them into spring context. This is useful when you want to interact with all of available keyspaces and indicies. For example in search-api module we need all of repositories beans to build requests.","title":"Without CHAIN_FAMILY"},{"location":"cyber-search/components/custom-chain-name/","text":"Custom Chain Name \u00b6 You have an ability to run our services with custom chain name based on supported chain families. It may be needed if you have your own custom chain and you want API endpoints, Kafka topic names and Cassandra keyspaces to use your chain name. Currently we're supporting BITCOIN and ETHEREUM chain families. To make it work you need to define CHAIN_NAME environment variable along with CHAIN_FAMILY . Also you may need to define your chain node url in PUMP by setting CHAIN_NODE_URL environment variable. For example if you want to index ETHEREUM based chain called MY_PRECIOUS . You have to use following config in all services ( CHAIN_NODE_URL is necessary only for PUMP service): CHAIN_FAMILY = ETHEREUM CHAIN_NAME = MY_PRECIOUS CHAIN_NODE_URL = https: //my_precious_node_url:port Then all Kafka topics will be named as MY_PRECIOUS_TX_PUMP , MY_PRECIOUS_BLOCK_PUMP , MY_PRECIOUS_UNCLE_PUMP . Cassandra keyspace will be named my_precious . And all API url will be build like this http://localhost:8080/my_precious/block/42 and etc.","title":"Custom chain name"},{"location":"cyber-search/components/custom-chain-name/#custom-chain-name","text":"You have an ability to run our services with custom chain name based on supported chain families. It may be needed if you have your own custom chain and you want API endpoints, Kafka topic names and Cassandra keyspaces to use your chain name. Currently we're supporting BITCOIN and ETHEREUM chain families. To make it work you need to define CHAIN_NAME environment variable along with CHAIN_FAMILY . Also you may need to define your chain node url in PUMP by setting CHAIN_NODE_URL environment variable. For example if you want to index ETHEREUM based chain called MY_PRECIOUS . You have to use following config in all services ( CHAIN_NODE_URL is necessary only for PUMP service): CHAIN_FAMILY = ETHEREUM CHAIN_NAME = MY_PRECIOUS CHAIN_NODE_URL = https: //my_precious_node_url:port Then all Kafka topics will be named as MY_PRECIOUS_TX_PUMP , MY_PRECIOUS_BLOCK_PUMP , MY_PRECIOUS_UNCLE_PUMP . Cassandra keyspace will be named my_precious . And all API url will be build like this http://localhost:8080/my_precious/block/42 and etc.","title":"Custom Chain Name"},{"location":"cyber-search/components/ethereum-components/","text":"Ethereum Components \u00b6 Component Scale Description Cluster Address Metrics External Ethereum Pump 1 Ethereum Chain Data Kafka Pump 8080:/actuator/metrics Ethereum Cassandra Dump 1 Dump Kafka Topics Into Cassandra 8080:/actuator/metrics Ethereum Contract Summary 1 - N Calculates Contract Summaries (balances and etc) 8080:/actuator/metrics Ethereum Pump \u00b6 Pumps Ethereum raw data(block,tx,uncles) into Kafka. Ethereum Cassandra Dump \u00b6 Consumes Ethereum raw data from Kafka topics, translate it into cassandra entities and store them into Elassandra. Ethereum Contract Summary \u00b6 Collects summary for each contract stored in Ethereum chain. Consumes Kafka topics and update data in Elassandra.","title":"Ethereum"},{"location":"cyber-search/components/ethereum-components/#ethereum-components","text":"Component Scale Description Cluster Address Metrics External Ethereum Pump 1 Ethereum Chain Data Kafka Pump 8080:/actuator/metrics Ethereum Cassandra Dump 1 Dump Kafka Topics Into Cassandra 8080:/actuator/metrics Ethereum Contract Summary 1 - N Calculates Contract Summaries (balances and etc) 8080:/actuator/metrics","title":"Ethereum Components"},{"location":"cyber-search/components/ethereum-components/#ethereum-pump","text":"Pumps Ethereum raw data(block,tx,uncles) into Kafka.","title":"Ethereum Pump"},{"location":"cyber-search/components/ethereum-components/#ethereum-cassandra-dump","text":"Consumes Ethereum raw data from Kafka topics, translate it into cassandra entities and store them into Elassandra.","title":"Ethereum Cassandra Dump"},{"location":"cyber-search/components/ethereum-components/#ethereum-contract-summary","text":"Collects summary for each contract stored in Ethereum chain. Consumes Kafka topics and update data in Elassandra.","title":"Ethereum Contract Summary"},{"location":"cyber-search/components/search-common-components/","text":"Search Common Components \u00b6 Component Scale Description Cluster Address Metrics External Kafka 1 Data Entry Kafka Manager 1 Kafka Explorer Elassandra 1 - N Data Back And Search elassandra.search.svc:9042 Search Api 1 - N Api To Search Chain Entities search-api.search.svc:80 8080:/actuator/metrics y Search Api Docs 1 - N Search Api Docs Based On Swagger search-api-docs.search.svc:80 y Chains Components \u00b6 Ethereum Bitcoin Kafka \u00b6 Kafka is message queue used as main data entry. All raw data firstly goes into kafka, than various services consume it to provide new functionality. Elassandra (Elastic + Cassandra) \u00b6 Elassandra is used as main backend storage with linear growth fulltext-search index on source cassandra data. Search Api \u00b6 Main search access endpoint. See api calls documentations .","title":"Search-common"},{"location":"cyber-search/components/search-common-components/#search-common-components","text":"Component Scale Description Cluster Address Metrics External Kafka 1 Data Entry Kafka Manager 1 Kafka Explorer Elassandra 1 - N Data Back And Search elassandra.search.svc:9042 Search Api 1 - N Api To Search Chain Entities search-api.search.svc:80 8080:/actuator/metrics y Search Api Docs 1 - N Search Api Docs Based On Swagger search-api-docs.search.svc:80 y","title":"Search Common Components"},{"location":"cyber-search/components/search-common-components/#chains-components","text":"Ethereum Bitcoin","title":"Chains Components"},{"location":"cyber-search/components/search-common-components/#kafka","text":"Kafka is message queue used as main data entry. All raw data firstly goes into kafka, than various services consume it to provide new functionality.","title":"Kafka"},{"location":"cyber-search/components/search-common-components/#elassandra-elastic-cassandra","text":"Elassandra is used as main backend storage with linear growth fulltext-search index on source cassandra data.","title":"Elassandra (Elastic + Cassandra)"},{"location":"cyber-search/components/search-common-components/#search-api","text":"Main search access endpoint. See api calls documentations .","title":"Search Api"},{"location":"cyber-search/contributing/cheat-sheet/","text":"Kafka \u00b6 Stop kafka and delete kafka data(cheat sheet) \u00b6 docker stop fast-data-dev-search docker rm fast-data-dev-search Elassandra \u00b6 Stop elassandra and delete elassandra data(cheat sheet) \u00b6 docker stop elassandra-search docker rm elassandra-search Get indices info \u00b6 curl -XGET 'localhost:9200/_cat/indices?v&pretty' Chains \u00b6 Run parity node \u00b6 sudo docker run -d -p 8545 :8545 --name parity_eth \\ -v ${ REPLACE_IT_BY_HOST_FOLDER } :/cyberdata parity/parity:stable \\ --db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 4 Run bitcoind node \u00b6 docker run -d -p 8332:8332 --name bitcoind --restart always \\ -v ${REPLACE_IT_BY_HOST_FOLDER}:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\ -server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0","title":"Cheat-sheet"},{"location":"cyber-search/contributing/cheat-sheet/#kafka","text":"","title":"Kafka"},{"location":"cyber-search/contributing/cheat-sheet/#stop-kafka-and-delete-kafka-datacheat-sheet","text":"docker stop fast-data-dev-search docker rm fast-data-dev-search","title":"Stop kafka and delete kafka data(cheat sheet)"},{"location":"cyber-search/contributing/cheat-sheet/#elassandra","text":"","title":"Elassandra"},{"location":"cyber-search/contributing/cheat-sheet/#stop-elassandra-and-delete-elassandra-datacheat-sheet","text":"docker stop elassandra-search docker rm elassandra-search","title":"Stop elassandra and delete elassandra data(cheat sheet)"},{"location":"cyber-search/contributing/cheat-sheet/#get-indices-info","text":"curl -XGET 'localhost:9200/_cat/indices?v&pretty'","title":"Get indices info"},{"location":"cyber-search/contributing/cheat-sheet/#chains","text":"","title":"Chains"},{"location":"cyber-search/contributing/cheat-sheet/#run-parity-node","text":"sudo docker run -d -p 8545 :8545 --name parity_eth \\ -v ${ REPLACE_IT_BY_HOST_FOLDER } :/cyberdata parity/parity:stable \\ --db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 4","title":"Run parity node"},{"location":"cyber-search/contributing/cheat-sheet/#run-bitcoind-node","text":"docker run -d -p 8332:8332 --name bitcoind --restart always \\ -v ${REPLACE_IT_BY_HOST_FOLDER}:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\ -server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0","title":"Run bitcoind node"},{"location":"cyber-search/contributing/contributing/","text":"Contributing to Cyber Search \u00b6 Thank you for considering a contribution to Cyber Search! This guide explains how to: Get started Development workflow * Get help if you encounter trouble Get in touch \u00b6 Before starting to work on a feature or a fix, please open an issue to discuss the use case or bug with us. This can save both you and us a lot of time. For any non-trivial change, we'll ask you to create a short design document explaining: Why is this change done? What's the use case? What will the API look like? (For new features) What test cases should it have? What could go wrong? How will it roughly be implemented? (We'll happily provide code pointers to save you time) Development Workflow \u00b6 Development Setup \u00b6 Please, use development environment setup guide . Make Changes \u00b6 Use this Architecture Overview as a start point for making changes. Local Check \u00b6 Several checks should passed to succeed build. Detekt code analyze tool should not report any issues JUnit tests should pass Before committing you changes, please, run local project check by: ./gradlew build //linux, mac gradlew.bat build //windows Creating Commits And Writing Commit Messages \u00b6 The commit messages that accompany your code changes are an important piece of documentation, please follow these guidelines when writing commit messages: solidity/CONTRIBUTING.md Keep commits discrete: avoid including multiple unrelated changes in a single commit Keep commits self-contained: avoid spreading a single change across multiple commits. A single commit should make sense in isolation Add GitHub issue to CHANGELOG.md Include GitHub issue in the commit message on a first line at the beginning. Example: #123 Refactor CONTRIBUTING.md --Add Creating Commits And Writing Commit Messages Section Submitting Your Change \u00b6 After you submit your pull request, a core developer will review it. It is normal that this takes several iterations, so don't get discouraged by change requests. They ensure the high quality that we all enjoy. Getting Help \u00b6 If you run into any trouble, please reach out to us on the issue you are working on. Our Thanks \u00b6 We deeply appreciate your effort toward improving Search. For any contribution, large or small, you will be immortalized in the release notes for the version you've contributed to.","title":"Contributing"},{"location":"cyber-search/contributing/contributing/#contributing-to-cyber-search","text":"Thank you for considering a contribution to Cyber Search! This guide explains how to: Get started Development workflow * Get help if you encounter trouble","title":"Contributing to Cyber Search"},{"location":"cyber-search/contributing/contributing/#get-in-touch","text":"Before starting to work on a feature or a fix, please open an issue to discuss the use case or bug with us. This can save both you and us a lot of time. For any non-trivial change, we'll ask you to create a short design document explaining: Why is this change done? What's the use case? What will the API look like? (For new features) What test cases should it have? What could go wrong? How will it roughly be implemented? (We'll happily provide code pointers to save you time)","title":"Get in touch"},{"location":"cyber-search/contributing/contributing/#development-workflow","text":"","title":"Development Workflow"},{"location":"cyber-search/contributing/contributing/#development-setup","text":"Please, use development environment setup guide .","title":"Development Setup"},{"location":"cyber-search/contributing/contributing/#make-changes","text":"Use this Architecture Overview as a start point for making changes.","title":"Make Changes"},{"location":"cyber-search/contributing/contributing/#local-check","text":"Several checks should passed to succeed build. Detekt code analyze tool should not report any issues JUnit tests should pass Before committing you changes, please, run local project check by: ./gradlew build //linux, mac gradlew.bat build //windows","title":"Local Check"},{"location":"cyber-search/contributing/contributing/#creating-commits-and-writing-commit-messages","text":"The commit messages that accompany your code changes are an important piece of documentation, please follow these guidelines when writing commit messages: solidity/CONTRIBUTING.md Keep commits discrete: avoid including multiple unrelated changes in a single commit Keep commits self-contained: avoid spreading a single change across multiple commits. A single commit should make sense in isolation Add GitHub issue to CHANGELOG.md Include GitHub issue in the commit message on a first line at the beginning. Example: #123 Refactor CONTRIBUTING.md --Add Creating Commits And Writing Commit Messages Section","title":"Creating Commits And Writing Commit Messages"},{"location":"cyber-search/contributing/contributing/#submitting-your-change","text":"After you submit your pull request, a core developer will review it. It is normal that this takes several iterations, so don't get discouraged by change requests. They ensure the high quality that we all enjoy.","title":"Submitting Your Change"},{"location":"cyber-search/contributing/contributing/#getting-help","text":"If you run into any trouble, please reach out to us on the issue you are working on.","title":"Getting Help"},{"location":"cyber-search/contributing/contributing/#our-thanks","text":"We deeply appreciate your effort toward improving Search. For any contribution, large or small, you will be immortalized in the release notes for the version you've contributed to.","title":"Our Thanks"},{"location":"cyber-search/contributing/dev-environment/","text":"Development environment \u00b6 Useful Links \u00b6 cheat sheet Prestart \u00b6 Install Java 8 JDK Install Docker and Docker Compose Install Intellij Idea Run Kafka, Elassandra, Prometheus and Grafana \u00b6 Start containers(required) \u00b6 For mac: cd dev-environment docker-compose -f env-mac.yml up -d For linux family: cd dev-environment docker-compose -f env.yml up -d Run chain node (only for pumps) \u00b6 In order to fetch data from chains pumps need chain node to interact with. To run chain node locally using docker use following commands: Parity for Ethereum sudo docker run -d -p 8545:8545 --name parity_eth \\ -v ${REPLACE_IT_BY_HOST_FOLDER}:/cyberdata parity/parity:stable \\ --db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 4 Bitcoind for Bitcoin docker run -d -p 8332:8332 --name bitcoind --restart always \\ -v ${REPLACE_IT_BY_HOST_FOLDER}:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\ -server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0 Or you could use any public available node (with appropriate settings) by passing CHAIN_NODE_URL environment variable to pump. For example CHAIN_NODE_URL=http://127.0.0.1:8545 . Import project to Intellij Idea \u00b6 Open Project in idea by selecting: Import Project -> selecting build.gradle file from the repository root Wait for dependency downloading and indexation Run Ethereum Pump from intellij Idea \u00b6 Go to EthereumPumpApplication.kt and press green triangle on left to the code (on example line 14): Pump will fail due to lack of CHAIN_FAMILY environment property, let's define it: Select \"Edit Configuration\" Add properties: Now, run pump one more time, it should start.","title":"Dev-environment"},{"location":"cyber-search/contributing/dev-environment/#development-environment","text":"","title":"Development environment"},{"location":"cyber-search/contributing/dev-environment/#useful-links","text":"cheat sheet","title":"Useful Links"},{"location":"cyber-search/contributing/dev-environment/#prestart","text":"Install Java 8 JDK Install Docker and Docker Compose Install Intellij Idea","title":"Prestart"},{"location":"cyber-search/contributing/dev-environment/#run-kafka-elassandra-prometheus-and-grafana","text":"","title":"Run Kafka, Elassandra, Prometheus and Grafana"},{"location":"cyber-search/contributing/dev-environment/#start-containersrequired","text":"For mac: cd dev-environment docker-compose -f env-mac.yml up -d For linux family: cd dev-environment docker-compose -f env.yml up -d","title":"Start containers(required)"},{"location":"cyber-search/contributing/dev-environment/#run-chain-node-only-for-pumps","text":"In order to fetch data from chains pumps need chain node to interact with. To run chain node locally using docker use following commands: Parity for Ethereum sudo docker run -d -p 8545:8545 --name parity_eth \\ -v ${REPLACE_IT_BY_HOST_FOLDER}:/cyberdata parity/parity:stable \\ --db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 4 Bitcoind for Bitcoin docker run -d -p 8332:8332 --name bitcoind --restart always \\ -v ${REPLACE_IT_BY_HOST_FOLDER}:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\ -server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0 Or you could use any public available node (with appropriate settings) by passing CHAIN_NODE_URL environment variable to pump. For example CHAIN_NODE_URL=http://127.0.0.1:8545 .","title":"Run chain node (only for pumps)"},{"location":"cyber-search/contributing/dev-environment/#import-project-to-intellij-idea","text":"Open Project in idea by selecting: Import Project -> selecting build.gradle file from the repository root Wait for dependency downloading and indexation","title":"Import project to Intellij Idea"},{"location":"cyber-search/contributing/dev-environment/#run-ethereum-pump-from-intellij-idea","text":"Go to EthereumPumpApplication.kt and press green triangle on left to the code (on example line 14): Pump will fail due to lack of CHAIN_FAMILY environment property, let's define it: Select \"Edit Configuration\" Add properties: Now, run pump one more time, it should start.","title":"Run Ethereum Pump from intellij Idea"},{"location":"cyber-search/contributing/pump-development/","text":"Pump Development Guide \u00b6 To develop your own chain pump you have two deal with two modules: common pumps Some explanations \u00b6 To describe single chain in our system we're using following scheme. Every chain has ChainFamily . For example BITCOIN, ETHEREUM families. ChainFamily has it's own set of entities like TX, BLOCK, UNCLE and default url of chain node. Next, chain has a name. Chain name used everywhere in modules. Kafka topics, Cassandra keyspaces, API endpoints will be based on chain name. By default chain name is equal to ChainFamily . For example if you have ChainFamily == BITCOIN and you not specifying a chain name it will be set to BITCOIN . Main goal of chain name is to separate forks of one chain that have the same structure. For example BITCOIN and BITCOIN_CASH or ETHEREUM and ETHEREUM_CLASSIC . So as far as you can understand single chain pump is chain pump for specific ChainFamily Adding models to common module \u00b6 First of all you have to put models representing your blockchain entities in common module. They should be placed in fund.cyber.search.model.{your_chain_family_name} package. This models will be used to transfer your blockchain data among our microservices (using Kafka). After doing so put description of your chain family to ChainFamily enum. You could find it in fund.cyber.search.model.chains.ChainInfo.kt file. Description includes following info: defaultNodeUrl - default URL of blockchain node to connect for data. entityTypes - map of blockchain entities types (from fund.cyber.search.model.chains.ChainEntity enum) to their class representation. If you can't find needed entity type in fund.cyber.search.model.chains.ChainEntity enum you have to add it by your own. Developing pump \u00b6 Now you're ready to write pump for your chain family. Note that this pump is suitable only for block based chains. We're using Spring Boot (https://projects.spring.io/spring-boot/). So you also should be familiar with spring beans, spring dependency injection and spring configuration. Creating gradle module \u00b6 To start with, create new Gradle submodule with name of your chain in pumps module. Include your module in pumps/build.gradle and in settings.gradle . Example of pumps/build.gradle file with your module: project ( \":pumps:common\" ) { apply plugin: \"io.spring.dependency-management\" jar . archiveName = \"common-pumps\" dependencies { ... } } ... project ( \":pumps:{your_module_name}\" ) { apply plugin: \"org.springframework.boot\" dependencies { compile project ( \":pumps:common\" ) //your module dependencies } } ... Example of settings.gradle file: include \"common\" include \"common-kafka\" include \"cassandra-service\" ... include \"pumps:common\" include \"pumps:bitcoin\" include \"pumps:ethereum\" include \"pumps:{your_chain_name}\" ... All classes should be placed under fund.cyber.pump.{your_chain_name} package (except main class). Creating spring boot main class \u00b6 The next step is creating spring boot main class. It should be placed in your module under fund.cyber package. Example of main class: @SpringBootApplication ( exclude = [ KafkaAutoConfiguration :: class ]) class BitcoinPumpApplication { companion object { @JvmStatic fun main ( args : Array < String >) { val application = SpringApplication ( BitcoinPumpApplication :: class . java ) application . runPump ( args ) } } } Default spring beans in context \u00b6 After spring context start you'll have few already configured beans in context that you could inject in your module classes: org.springframework.retry.support.RetryTemplate - Spring Retry template for retrying failed operations. io.micrometer.core.instrument.MeterRegistry - bean for monitoring. fund.cyber.search.model.chains.ChainInfo - bean with all needed properties of running chain. class ChainInfo ( val family : ChainFamily , val name : String = \"\" , val nodeUrl : String = family . defaultNodeUrl ) { val fullName get () = family . name + if ( name . isEmpty ()) \"\" else \"_$name\" val entityTypes get () = family . entityTypes . keys fun entityClassByType ( type : ChainEntityType ) = family . entityTypes [ type ] } ChainInfo bean constructed from environment properties at start: CHAIN_FAMILY - chain family name (matches ChainFamily enum). For example CHAIN_FAMILY=BITCOIN . Required option . CHAIN_NAME - name of your specific chain. Not necessary . By default equals to CHAIN_FAMILY . All kafka topic will be named accordingly. For example CHAIN_NAME=BITCOIN_CASH then a topic name will be BITCOIN_CASH_TX_PUMP . * CHAIN_NODE_URL - URL of node running your chain. Not necessary . By default will be equal to one that described in your ChainFamily . Implementing pump \u00b6 To integrate your pump with our system you simply have to implement two interfaces: * fund.cyber.pump.common.node.BlockBundle /** * Blockchain block with all dependent entities. Should collect all entities in scope of one block. * For example: transactions, uncles, etc.. */ interface BlockBundle { /** * Hash of the block */ val hash : String /** * Hash of the parent block */ val parentHash : String /** * Number of the block in blockchain */ val number : Long /** * Size of the block in bytes */ val blockSize : Int /** * Get dependent entity values list by entity type. * * @param chainEntityType type of entity (for example: [ChainEntityType.TX]) * @return list of entity values (for example: transactions) */ fun entitiesByType ( chainEntityType : ChainEntityType ): List < ChainEntity > } fund.cyber.pump.common.node.BlockchainInterface /** * Interface representing blockchain * * @param T block bundle of this blockchain */ interface BlockchainInterface < out T : BlockBundle > { /** * Get last number of the block in blockchain network. * * @return block number */ fun lastNetworkBlock (): Long /** * Get [BlockBundle] by block number. * * @param number block number * @return block bundle */ fun blockBundleByNumber ( number : Long ): T } Also you should define spring bean of fund.cyber.pump.common.node.BlockchainInterface implementation either by annotate it with @Component or defining @Bean in spring configuration. For example: @Component class BitcoinBlockchainInterface ( ... ) : BlockchainInterface < BitcoinBlockBundle > { private val downloadSpeedMonitor = monitoring . timer ( \"pump_bundle_download\" ) override fun lastNetworkBlock (): Long = bitcoinJsonRpcClient . getLastBlockNumber () override fun blockBundleByNumber ( number : Long ): BitcoinBlockBundle { return downloadSpeedMonitor . recordCallable { val block = bitcoinJsonRpcClient . getBlockByNumber ( number ) !! return @recordCallable rpcToBundleEntitiesConverter . convertToBundle ( block ) } } } So, as you can see, you're not care of what happening with data next and how to store it. All you need is just to tell BlockBundle how to map it's fields on entities and we'll take care of everything else. Note that you should use toSearchHashFormat() extension function placed in fund.cyber.api.common.Func.kt on all fields in hex format when building your chain entities. Memory Pool Pump \u00b6 You also could add memory pool pumping logic by simply implement PoolInterface interface interface PoolInterface < T : PoolItem > { fun subscribePool (): Flowable < T > } It contains only one method that should return io.reactivex.Flowable of pool items. Create Dockerfile \u00b6 Put Docker file in root folder of your module. Here is template of Dockerfile: # Build Stage # Container with application FROM openjdk:8-jre-slim COPY /build/libs /cyberapp/bin ENTRYPOINT exec java $JAVA_OPTS -jar /cyberapp/bin/${your_chain_name}.jar Update CI \u00b6 Finally, you should update .circleci/config.yml file with steps for building and pushing docker image. * Add deploy job to jobs section. Template: deploy_chain_pumps_${your_chain_name}_image: <<: *defaults steps: - attach_workspace: at: ~/build - setup_remote_docker: version: 17.11.0-ce - run: name: Build ${your_chain_name} Pump Image command: | docker build -t build/pump-${your_chain_name} -f ./pumps/${your_chain_name}/Dockerfile ./pumps/${your_chain_name} docker login -u $DOCKER_USER -p $DOCKER_PASS docker tag build/pump-${your_chain_name} cybernode/chain-pump-${your_chain_name}:$CIRCLE_TAG docker push cybernode/chain-pump-${your_chain_name}:$CIRCLE_TAG docker tag build/pump-${your_chain_name} cybernode/chain-pump-${your_chain_name}:latest docker push cybernode/chain-pump-${your_chain_name}:latest Add deploy job to workflows.search_build.jobs section. Template: - deploy_chain_pumps_${your_chain_name}_image: <<: *release_filter requires: - build_project","title":"Pump-development"},{"location":"cyber-search/contributing/pump-development/#pump-development-guide","text":"To develop your own chain pump you have two deal with two modules: common pumps","title":"Pump Development Guide"},{"location":"cyber-search/contributing/pump-development/#some-explanations","text":"To describe single chain in our system we're using following scheme. Every chain has ChainFamily . For example BITCOIN, ETHEREUM families. ChainFamily has it's own set of entities like TX, BLOCK, UNCLE and default url of chain node. Next, chain has a name. Chain name used everywhere in modules. Kafka topics, Cassandra keyspaces, API endpoints will be based on chain name. By default chain name is equal to ChainFamily . For example if you have ChainFamily == BITCOIN and you not specifying a chain name it will be set to BITCOIN . Main goal of chain name is to separate forks of one chain that have the same structure. For example BITCOIN and BITCOIN_CASH or ETHEREUM and ETHEREUM_CLASSIC . So as far as you can understand single chain pump is chain pump for specific ChainFamily","title":"Some explanations"},{"location":"cyber-search/contributing/pump-development/#adding-models-to-common-module","text":"First of all you have to put models representing your blockchain entities in common module. They should be placed in fund.cyber.search.model.{your_chain_family_name} package. This models will be used to transfer your blockchain data among our microservices (using Kafka). After doing so put description of your chain family to ChainFamily enum. You could find it in fund.cyber.search.model.chains.ChainInfo.kt file. Description includes following info: defaultNodeUrl - default URL of blockchain node to connect for data. entityTypes - map of blockchain entities types (from fund.cyber.search.model.chains.ChainEntity enum) to their class representation. If you can't find needed entity type in fund.cyber.search.model.chains.ChainEntity enum you have to add it by your own.","title":"Adding models to common module"},{"location":"cyber-search/contributing/pump-development/#developing-pump","text":"Now you're ready to write pump for your chain family. Note that this pump is suitable only for block based chains. We're using Spring Boot (https://projects.spring.io/spring-boot/). So you also should be familiar with spring beans, spring dependency injection and spring configuration.","title":"Developing pump"},{"location":"cyber-search/contributing/pump-development/#creating-gradle-module","text":"To start with, create new Gradle submodule with name of your chain in pumps module. Include your module in pumps/build.gradle and in settings.gradle . Example of pumps/build.gradle file with your module: project ( \":pumps:common\" ) { apply plugin: \"io.spring.dependency-management\" jar . archiveName = \"common-pumps\" dependencies { ... } } ... project ( \":pumps:{your_module_name}\" ) { apply plugin: \"org.springframework.boot\" dependencies { compile project ( \":pumps:common\" ) //your module dependencies } } ... Example of settings.gradle file: include \"common\" include \"common-kafka\" include \"cassandra-service\" ... include \"pumps:common\" include \"pumps:bitcoin\" include \"pumps:ethereum\" include \"pumps:{your_chain_name}\" ... All classes should be placed under fund.cyber.pump.{your_chain_name} package (except main class).","title":"Creating gradle module"},{"location":"cyber-search/contributing/pump-development/#creating-spring-boot-main-class","text":"The next step is creating spring boot main class. It should be placed in your module under fund.cyber package. Example of main class: @SpringBootApplication ( exclude = [ KafkaAutoConfiguration :: class ]) class BitcoinPumpApplication { companion object { @JvmStatic fun main ( args : Array < String >) { val application = SpringApplication ( BitcoinPumpApplication :: class . java ) application . runPump ( args ) } } }","title":"Creating spring boot main class"},{"location":"cyber-search/contributing/pump-development/#default-spring-beans-in-context","text":"After spring context start you'll have few already configured beans in context that you could inject in your module classes: org.springframework.retry.support.RetryTemplate - Spring Retry template for retrying failed operations. io.micrometer.core.instrument.MeterRegistry - bean for monitoring. fund.cyber.search.model.chains.ChainInfo - bean with all needed properties of running chain. class ChainInfo ( val family : ChainFamily , val name : String = \"\" , val nodeUrl : String = family . defaultNodeUrl ) { val fullName get () = family . name + if ( name . isEmpty ()) \"\" else \"_$name\" val entityTypes get () = family . entityTypes . keys fun entityClassByType ( type : ChainEntityType ) = family . entityTypes [ type ] } ChainInfo bean constructed from environment properties at start: CHAIN_FAMILY - chain family name (matches ChainFamily enum). For example CHAIN_FAMILY=BITCOIN . Required option . CHAIN_NAME - name of your specific chain. Not necessary . By default equals to CHAIN_FAMILY . All kafka topic will be named accordingly. For example CHAIN_NAME=BITCOIN_CASH then a topic name will be BITCOIN_CASH_TX_PUMP . * CHAIN_NODE_URL - URL of node running your chain. Not necessary . By default will be equal to one that described in your ChainFamily .","title":"Default spring beans in context"},{"location":"cyber-search/contributing/pump-development/#implementing-pump","text":"To integrate your pump with our system you simply have to implement two interfaces: * fund.cyber.pump.common.node.BlockBundle /** * Blockchain block with all dependent entities. Should collect all entities in scope of one block. * For example: transactions, uncles, etc.. */ interface BlockBundle { /** * Hash of the block */ val hash : String /** * Hash of the parent block */ val parentHash : String /** * Number of the block in blockchain */ val number : Long /** * Size of the block in bytes */ val blockSize : Int /** * Get dependent entity values list by entity type. * * @param chainEntityType type of entity (for example: [ChainEntityType.TX]) * @return list of entity values (for example: transactions) */ fun entitiesByType ( chainEntityType : ChainEntityType ): List < ChainEntity > } fund.cyber.pump.common.node.BlockchainInterface /** * Interface representing blockchain * * @param T block bundle of this blockchain */ interface BlockchainInterface < out T : BlockBundle > { /** * Get last number of the block in blockchain network. * * @return block number */ fun lastNetworkBlock (): Long /** * Get [BlockBundle] by block number. * * @param number block number * @return block bundle */ fun blockBundleByNumber ( number : Long ): T } Also you should define spring bean of fund.cyber.pump.common.node.BlockchainInterface implementation either by annotate it with @Component or defining @Bean in spring configuration. For example: @Component class BitcoinBlockchainInterface ( ... ) : BlockchainInterface < BitcoinBlockBundle > { private val downloadSpeedMonitor = monitoring . timer ( \"pump_bundle_download\" ) override fun lastNetworkBlock (): Long = bitcoinJsonRpcClient . getLastBlockNumber () override fun blockBundleByNumber ( number : Long ): BitcoinBlockBundle { return downloadSpeedMonitor . recordCallable { val block = bitcoinJsonRpcClient . getBlockByNumber ( number ) !! return @recordCallable rpcToBundleEntitiesConverter . convertToBundle ( block ) } } } So, as you can see, you're not care of what happening with data next and how to store it. All you need is just to tell BlockBundle how to map it's fields on entities and we'll take care of everything else. Note that you should use toSearchHashFormat() extension function placed in fund.cyber.api.common.Func.kt on all fields in hex format when building your chain entities.","title":"Implementing pump"},{"location":"cyber-search/contributing/pump-development/#memory-pool-pump","text":"You also could add memory pool pumping logic by simply implement PoolInterface interface interface PoolInterface < T : PoolItem > { fun subscribePool (): Flowable < T > } It contains only one method that should return io.reactivex.Flowable of pool items.","title":"Memory Pool Pump"},{"location":"cyber-search/contributing/pump-development/#create-dockerfile","text":"Put Docker file in root folder of your module. Here is template of Dockerfile: # Build Stage # Container with application FROM openjdk:8-jre-slim COPY /build/libs /cyberapp/bin ENTRYPOINT exec java $JAVA_OPTS -jar /cyberapp/bin/${your_chain_name}.jar","title":"Create Dockerfile"},{"location":"cyber-search/contributing/pump-development/#update-ci","text":"Finally, you should update .circleci/config.yml file with steps for building and pushing docker image. * Add deploy job to jobs section. Template: deploy_chain_pumps_${your_chain_name}_image: <<: *defaults steps: - attach_workspace: at: ~/build - setup_remote_docker: version: 17.11.0-ce - run: name: Build ${your_chain_name} Pump Image command: | docker build -t build/pump-${your_chain_name} -f ./pumps/${your_chain_name}/Dockerfile ./pumps/${your_chain_name} docker login -u $DOCKER_USER -p $DOCKER_PASS docker tag build/pump-${your_chain_name} cybernode/chain-pump-${your_chain_name}:$CIRCLE_TAG docker push cybernode/chain-pump-${your_chain_name}:$CIRCLE_TAG docker tag build/pump-${your_chain_name} cybernode/chain-pump-${your_chain_name}:latest docker push cybernode/chain-pump-${your_chain_name}:latest Add deploy job to workflows.search_build.jobs section. Template: - deploy_chain_pumps_${your_chain_name}_image: <<: *release_filter requires: - build_project","title":"Update CI"},{"location":"cyberd/Changelog/","text":"Change Log \u00b6 Unreleased \u00b6 Full Changelog Closed issues: Make docker container based on nvidia-gpu image. #104 0.0.8 (2018-12-11) \u00b6 Full Changelog Implemented enhancements: Cid validation #93 Update to cosmos-sdk version 0.26.1 #79 Build node releases with cleveldb #59 Remove 'cosmosaccaddr' prefix from cyberd address #39 Closed issues: Make up cyberd landing #87 Add possibility to join for new validators. #75 Calculate rank using GPU #74 Create basic wiki cyberd indexer #71 Create cyberd PoC based on Cosmos SDK #37 Draw logo for cyberd #16 Build basic economic model #1 Merged pull requests: #104 Make docker container based on nvidia-gpu image #110 ( hleb-albau ) [DON'T MERGE] Add bandwidth by stake. Part 2. #108 ( arturalbov ) Add bandwidth by stake Part 1 #107 ( arturalbov ) #93 Cid validation #106 ( hleb-albau ) #78 Add guide How to join network as validator #105 ( hleb-albau ) #1 Build basic economic model #102 ( hleb-albau ) Update cosmos to 0.27.0 #100 ( hleb-albau ) Small fixes for validators joining #98 ( arturalbov ) Remove poc folder #97 ( hleb-albau ) Add possibility to join for new validators #96 ( arturalbov ) Calculate eth network significance #85 ( hleb-albau ) 74 gpu rank calculation #83 ( hleb-albau ) Remove 'cosmosaccaddr' prefix from cyberd address #82 ( arturalbov ) UPD docs_upd job #81 ( SaveTheAles ) Update cosmos-sdk version to 0.26.1 #80 ( arturalbov ) 0.0.7 (2018-10-25) \u00b6 Full Changelog Implemented enhancements: Proxy service. Add search pagination #69 Add send tokens endpoint to proxy #62 Fixed bugs: Proxy service. Search request with \"spaces\" fails #67 Non-deterministic rank calculation #66 Closed issues: Claim service: increment tx sequence manually #64 Merged pull requests: #71 Create basic wiki cyberd indexer #72 ( hleb-albau ) Proxy service. Add search pagination #70 ( arturalbov ) Proxy service. Search request with spaces fails #68 ( arturalbov ) Claim service: increment tx sequence manually #65 ( arturalbov ) 0.0.6 (2018-10-24) \u00b6 Full Changelog 0.0.5 (2018-10-23) \u00b6 Full Changelog Implemented enhancements: [RPC] Fix small finding #57 0.0.4 (2018-10-23) \u00b6 Full Changelog Closed issues: Service to claim cyberd zeronet tokens #61 Update cosmos-sdk to latest dev branch version #56 Perfomance Degradation: Heavy Disk Usage #50 Merged pull requests: Add send tokens endpoint to proxy #63 ( arturalbov ) Claim service #60 ( arturalbov ) #57 [RPC] Fix small finding #58 ( hleb-albau ) Update cosmos-sdk to latest dev branch version #53 ( hleb-albau ) 0.0.3 (2018-10-19) \u00b6 Full Changelog Implemented enhancements: CLI: Add \"wait_for_confirmation\" Flag #47 Fixed bugs: RPC Client: /search on non existing cid return first added cid #48 Closed issues: Write validation logic of IPFS hash for Losion Zeronet #18 Genesis Zeronet #17 Performance testing of Zeronet #4 Merged pull requests: Add cyberdproxy process to docker container. Add status endpoint #55 ( arturalbov ) Proxy rpc #54 ( arturalbov ) 0.0.2 (2018-10-05) \u00b6 Full Changelog Implemented enhancements: Cosmos PoC: Integrate Rank Calculation #43 Cosmos PoC: Extenend Standart Tendermint RPC API #42 Closed issues: Implement persistent storage for links. #40 run extra node for cyberd #20 Merged pull requests: Fix search on non existing cid #49 ( arturalbov ) Add Circle CI build job #46 ( arturalbov ) Extenend Standart Tendermint RPC API #45 ( arturalbov ) #43 Simplest Rank #44 ( hleb-albau ) #40 introduce in-memory store #41 ( hleb-albau ) #37 redesign db, app refactor #38 ( hleb-albau ) Cosmos POC: Clean up CLI #35 ( arturalbov ) Update cyberd/cosmos README #34 ( arturalbov ) 0.0.1 (2018-09-25) \u00b6 Full Changelog Closed issues: Make cyberd docs to be included into common wiki. #32 Write LT/NLT logic to Losion Zeronet #19 Research basic technologies #14 Research basic papers #13 Perfomance testing of Ethermint #12 Perfomance testing of Plasma #11 Perfomance testing of PoA networks #7 Cleanup paper #6 Perfomance testing of EOS #3 Merged pull requests: Genesis zeronet: Cosmos SDK #33 ( arturalbov ) [WIP] 21 calculate spring rank for ethereum #23 ( hleb-albau ) Fixing typo #2 ( trummax ) 0.2.0 (2018-03-18) \u00b6 * This Change Log was automatically generated by github_changelog_generator","title":"Changelog"},{"location":"cyberd/Changelog/#change-log","text":"","title":"Change Log"},{"location":"cyberd/Changelog/#unreleased","text":"Full Changelog Closed issues: Make docker container based on nvidia-gpu image. #104","title":"Unreleased"},{"location":"cyberd/Changelog/#008-2018-12-11","text":"Full Changelog Implemented enhancements: Cid validation #93 Update to cosmos-sdk version 0.26.1 #79 Build node releases with cleveldb #59 Remove 'cosmosaccaddr' prefix from cyberd address #39 Closed issues: Make up cyberd landing #87 Add possibility to join for new validators. #75 Calculate rank using GPU #74 Create basic wiki cyberd indexer #71 Create cyberd PoC based on Cosmos SDK #37 Draw logo for cyberd #16 Build basic economic model #1 Merged pull requests: #104 Make docker container based on nvidia-gpu image #110 ( hleb-albau ) [DON'T MERGE] Add bandwidth by stake. Part 2. #108 ( arturalbov ) Add bandwidth by stake Part 1 #107 ( arturalbov ) #93 Cid validation #106 ( hleb-albau ) #78 Add guide How to join network as validator #105 ( hleb-albau ) #1 Build basic economic model #102 ( hleb-albau ) Update cosmos to 0.27.0 #100 ( hleb-albau ) Small fixes for validators joining #98 ( arturalbov ) Remove poc folder #97 ( hleb-albau ) Add possibility to join for new validators #96 ( arturalbov ) Calculate eth network significance #85 ( hleb-albau ) 74 gpu rank calculation #83 ( hleb-albau ) Remove 'cosmosaccaddr' prefix from cyberd address #82 ( arturalbov ) UPD docs_upd job #81 ( SaveTheAles ) Update cosmos-sdk version to 0.26.1 #80 ( arturalbov )","title":"0.0.8 (2018-12-11)"},{"location":"cyberd/Changelog/#007-2018-10-25","text":"Full Changelog Implemented enhancements: Proxy service. Add search pagination #69 Add send tokens endpoint to proxy #62 Fixed bugs: Proxy service. Search request with \"spaces\" fails #67 Non-deterministic rank calculation #66 Closed issues: Claim service: increment tx sequence manually #64 Merged pull requests: #71 Create basic wiki cyberd indexer #72 ( hleb-albau ) Proxy service. Add search pagination #70 ( arturalbov ) Proxy service. Search request with spaces fails #68 ( arturalbov ) Claim service: increment tx sequence manually #65 ( arturalbov )","title":"0.0.7 (2018-10-25)"},{"location":"cyberd/Changelog/#006-2018-10-24","text":"Full Changelog","title":"0.0.6 (2018-10-24)"},{"location":"cyberd/Changelog/#005-2018-10-23","text":"Full Changelog Implemented enhancements: [RPC] Fix small finding #57","title":"0.0.5 (2018-10-23)"},{"location":"cyberd/Changelog/#004-2018-10-23","text":"Full Changelog Closed issues: Service to claim cyberd zeronet tokens #61 Update cosmos-sdk to latest dev branch version #56 Perfomance Degradation: Heavy Disk Usage #50 Merged pull requests: Add send tokens endpoint to proxy #63 ( arturalbov ) Claim service #60 ( arturalbov ) #57 [RPC] Fix small finding #58 ( hleb-albau ) Update cosmos-sdk to latest dev branch version #53 ( hleb-albau )","title":"0.0.4 (2018-10-23)"},{"location":"cyberd/Changelog/#003-2018-10-19","text":"Full Changelog Implemented enhancements: CLI: Add \"wait_for_confirmation\" Flag #47 Fixed bugs: RPC Client: /search on non existing cid return first added cid #48 Closed issues: Write validation logic of IPFS hash for Losion Zeronet #18 Genesis Zeronet #17 Performance testing of Zeronet #4 Merged pull requests: Add cyberdproxy process to docker container. Add status endpoint #55 ( arturalbov ) Proxy rpc #54 ( arturalbov )","title":"0.0.3 (2018-10-19)"},{"location":"cyberd/Changelog/#002-2018-10-05","text":"Full Changelog Implemented enhancements: Cosmos PoC: Integrate Rank Calculation #43 Cosmos PoC: Extenend Standart Tendermint RPC API #42 Closed issues: Implement persistent storage for links. #40 run extra node for cyberd #20 Merged pull requests: Fix search on non existing cid #49 ( arturalbov ) Add Circle CI build job #46 ( arturalbov ) Extenend Standart Tendermint RPC API #45 ( arturalbov ) #43 Simplest Rank #44 ( hleb-albau ) #40 introduce in-memory store #41 ( hleb-albau ) #37 redesign db, app refactor #38 ( hleb-albau ) Cosmos POC: Clean up CLI #35 ( arturalbov ) Update cyberd/cosmos README #34 ( arturalbov )","title":"0.0.2 (2018-10-05)"},{"location":"cyberd/Changelog/#001-2018-09-25","text":"Full Changelog Closed issues: Make cyberd docs to be included into common wiki. #32 Write LT/NLT logic to Losion Zeronet #19 Research basic technologies #14 Research basic papers #13 Perfomance testing of Ethermint #12 Perfomance testing of Plasma #11 Perfomance testing of PoA networks #7 Cleanup paper #6 Perfomance testing of EOS #3 Merged pull requests: Genesis zeronet: Cosmos SDK #33 ( arturalbov ) [WIP] 21 calculate spring rank for ethereum #23 ( hleb-albau ) Fixing typo #2 ( trummax )","title":"0.0.1 (2018-09-25)"},{"location":"cyberd/Changelog/#020-2018-03-18","text":"* This Change Log was automatically generated by github_changelog_generator","title":"0.2.0 (2018-03-18)"},{"location":"cyberd/Contributing/","text":"","title":"Contributing"},{"location":"cyberd/OLD_README/","text":"Cyberd Usage Guide \u00b6 Installing Cyberd Node \u00b6 Use docker \u00b6 In order to start cyberd node locally using docker run following command (replace ${YOUR_DATA_LOCAL_FOLDER} and ${YOUR_CONFIG_LOCAL_FOLDER} with your local folders where you want to store cyberd data and configuration): docker run -d --restart always --name = cyberd -p 26656 :26656 -p 26657 :26657 -p 26660 :26660 -v ${ YOUR_DATA_LOCAL_FOLDER } :/root/.cyberd/data -v ${ YOUR_CONFIG_LOCAL_FOLDER } :/root/.cyberd/config cybernode/cyberd:master It will run NON-VALIDATOR local node and connect it to our seed node called \"earth\". You could check that node is running by executing: docker logs cyberd Use compiled binary \u00b6 You could find latest binaries in our releases . Choose appropriate binary for your system, download them and add to $PATH. To connect to our network you should copy genesis.json and config.toml files from here and put them into $HOME/.cyberd/config/ folder After everything is set run: cyberd init to initialize your node. To start node use following command: cyberd start Node is started! You should see logs with generated blocks. Build binaries manually \u00b6 You need to have GO 1.11+ installed on your computer. Install GO by following the official docs . Remember to set your $GOPATH , $GOBIN , and $PATH environment variables, for example: ```$bash mkdir -p $HOME/go/bin echo \"export GOPATH=$HOME/go\" >> ~/.bash_profile echo \"export GOBIN=$GOPATH/bin\" >> ~/.bash_profile echo \"export PATH=$PATH:$GOBIN\" >> ~/.bash_profile Once you have GO installed clone our repository: ```$bash git clone https://github.com/cybercongress/cyberd.git In project directory go into /cosmos/poc/ and run: ```$bash export GO111MODULE=on go install ./cyberd go install -o cyberdcli ./cli This will create binaries for `cyberd` and `cyberdcli` After you built binaries follow [this](#use-compiled-binary) instructions to run node. ## Using cyberdcli You could find latest cli in our [releases section](https://github.com/cybercongress/cyberd/releases) or build it by yourself using [this](#build-binaries-manually) guide. ### Generating keys After you have cyberdcli binary installed you probably want to generate personal keys to sign and broadcast transactions. To generate keys simply run: cyberdcli keys add ${your-key-name} Enter and confirm a passphrase: Enter a passphrase for your key: Repeat the passphrase: You just created your first locally stored key, under the given name. Once you do this, the `~/.cyberdcli` folder is created, which will hold the this key and any other keys you make. Now that you have the key for alice, you can start broadcasting transactions. ### Link transaction The following command will link cid1 with cid2. cyberdcli link --from=${your_key_name} --cid-from=42 --cid-to=cyberd --sequence=0 --chain-id=test-chain-fbqPMq You could find chain id in `$HOME/.cyberd/config/genesis.json`. For our zeronet chain id is `test-chain-fbqPMq`. Every new transaction should have incremented `--sequence` parameter. If everything went fine you should see similar message: Committed at block 107 (tx hash: BE458B956646F8B3F25F071A958A5FD7E908791F) ### Search transaction by hash To find transaction run: ```bash cyberdcli tx BE458B956646F8B3F25F071A958A5FD7E908791F Help \u00b6 cyberdcli --help Please note that currently not all functions are available. RPC client \u00b6 RPC client is available on localhost:26657 . You could find URLs list here Our public endpoint \u00b6 http://earth.cybernode.ai:34657 Example http://earth.cybernode.ai:34657/block?height=42","title":"Cyberd Usage Guide"},{"location":"cyberd/OLD_README/#cyberd-usage-guide","text":"","title":"Cyberd Usage Guide"},{"location":"cyberd/OLD_README/#installing-cyberd-node","text":"","title":"Installing Cyberd Node"},{"location":"cyberd/OLD_README/#use-docker","text":"In order to start cyberd node locally using docker run following command (replace ${YOUR_DATA_LOCAL_FOLDER} and ${YOUR_CONFIG_LOCAL_FOLDER} with your local folders where you want to store cyberd data and configuration): docker run -d --restart always --name = cyberd -p 26656 :26656 -p 26657 :26657 -p 26660 :26660 -v ${ YOUR_DATA_LOCAL_FOLDER } :/root/.cyberd/data -v ${ YOUR_CONFIG_LOCAL_FOLDER } :/root/.cyberd/config cybernode/cyberd:master It will run NON-VALIDATOR local node and connect it to our seed node called \"earth\". You could check that node is running by executing: docker logs cyberd","title":"Use docker"},{"location":"cyberd/OLD_README/#use-compiled-binary","text":"You could find latest binaries in our releases . Choose appropriate binary for your system, download them and add to $PATH. To connect to our network you should copy genesis.json and config.toml files from here and put them into $HOME/.cyberd/config/ folder After everything is set run: cyberd init to initialize your node. To start node use following command: cyberd start Node is started! You should see logs with generated blocks.","title":"Use compiled binary"},{"location":"cyberd/OLD_README/#build-binaries-manually","text":"You need to have GO 1.11+ installed on your computer. Install GO by following the official docs . Remember to set your $GOPATH , $GOBIN , and $PATH environment variables, for example: ```$bash mkdir -p $HOME/go/bin echo \"export GOPATH=$HOME/go\" >> ~/.bash_profile echo \"export GOBIN=$GOPATH/bin\" >> ~/.bash_profile echo \"export PATH=$PATH:$GOBIN\" >> ~/.bash_profile Once you have GO installed clone our repository: ```$bash git clone https://github.com/cybercongress/cyberd.git In project directory go into /cosmos/poc/ and run: ```$bash export GO111MODULE=on go install ./cyberd go install -o cyberdcli ./cli This will create binaries for `cyberd` and `cyberdcli` After you built binaries follow [this](#use-compiled-binary) instructions to run node. ## Using cyberdcli You could find latest cli in our [releases section](https://github.com/cybercongress/cyberd/releases) or build it by yourself using [this](#build-binaries-manually) guide. ### Generating keys After you have cyberdcli binary installed you probably want to generate personal keys to sign and broadcast transactions. To generate keys simply run: cyberdcli keys add ${your-key-name} Enter and confirm a passphrase: Enter a passphrase for your key: Repeat the passphrase: You just created your first locally stored key, under the given name. Once you do this, the `~/.cyberdcli` folder is created, which will hold the this key and any other keys you make. Now that you have the key for alice, you can start broadcasting transactions. ### Link transaction The following command will link cid1 with cid2. cyberdcli link --from=${your_key_name} --cid-from=42 --cid-to=cyberd --sequence=0 --chain-id=test-chain-fbqPMq You could find chain id in `$HOME/.cyberd/config/genesis.json`. For our zeronet chain id is `test-chain-fbqPMq`. Every new transaction should have incremented `--sequence` parameter. If everything went fine you should see similar message: Committed at block 107 (tx hash: BE458B956646F8B3F25F071A958A5FD7E908791F) ### Search transaction by hash To find transaction run: ```bash cyberdcli tx BE458B956646F8B3F25F071A958A5FD7E908791F","title":"Build binaries manually"},{"location":"cyberd/OLD_README/#help","text":"cyberdcli --help Please note that currently not all functions are available.","title":"Help"},{"location":"cyberd/OLD_README/#rpc-client","text":"RPC client is available on localhost:26657 . You could find URLs list here","title":"RPC client"},{"location":"cyberd/OLD_README/#our-public-endpoint","text":"http://earth.cybernode.ai:34657 Example http://earth.cybernode.ai:34657/block?height=42","title":"Our public endpoint"},{"location":"cyberd/Roadmap/","text":"/// A file contain a list for discussion during research around cyberd. If some useful considerations sparks around some points an issue with tag discussion must be created. Discussion topics \u00b6 User has tokens (power of linking) and reputation which equal sum of link's weights created/connected to their Q/A. reputation vesting ~ inflation <- function * link weight ~ reputation <- function * broadband ~ reputation <- function * link weight applying with quadratic voting Retire CID in favor for DURI user defined domain registries Discussion on anonimity, complexity of a protocol and expensiveness of computation Discuss decay of links with time language incentivization bias_. In the core of cyberd is quadratic voting. The system needs it to effectively incentivize participants for quality ranking. But that natively leads to weak incentives across different language groups. E.g blockchain Golos was deployed as Russian alternative to Steem because Russian posts acquired only 0.2% of rewards though providing 10% of the content. The idea of deploying cyberd is great if it can be truly global from the start. The only way to overcome this bias is a global deployment from day 0, because otherwise we need significantly increase the complexity of the reward system. We offer good incentives for translation of this white paper to 50 languages worldwide as well as call for action to all blockchain communities across the globe. burning of CYBER in order to increase the rank A conventional advertiser is also has a tool to participate. Any link can be promoted thus increasing cyber\u2022rank. Revenue from the promotion is burning thus decreasing the supply of CYBER and bringing value to all shareholders of a system. We do believe that this approach is fair and don't harm user experience but open opportunity to compete with conventional search ad a bit. incentivization of developers. As our primary focus is developers of decentralized and distributed applications we offer a convenient way of revenue generation in addition to possible revenue on dynamic snippets development. Every search and answer transaction can define a smart contract with up to 6 beneficiaries with a distribution of potential reward from this particular action. This creates a solid foundation for bootstrapping application ecosystem across a conventional web, mobile app stores and upcoming VR platforms. idea: split reward pool in accordance to relevance of terms. Priorities for learning the beast: Arxiv, Chains, Wiki, Github, Bittorent, IPFS We cannot make distributions based on the history of ranking object because we decide to reject an idea of storing the historical state due to tremendous costs.","title":"Roadmap"},{"location":"cyberd/Roadmap/#discussion-topics","text":"User has tokens (power of linking) and reputation which equal sum of link's weights created/connected to their Q/A. reputation vesting ~ inflation <- function * link weight ~ reputation <- function * broadband ~ reputation <- function * link weight applying with quadratic voting Retire CID in favor for DURI user defined domain registries Discussion on anonimity, complexity of a protocol and expensiveness of computation Discuss decay of links with time language incentivization bias_. In the core of cyberd is quadratic voting. The system needs it to effectively incentivize participants for quality ranking. But that natively leads to weak incentives across different language groups. E.g blockchain Golos was deployed as Russian alternative to Steem because Russian posts acquired only 0.2% of rewards though providing 10% of the content. The idea of deploying cyberd is great if it can be truly global from the start. The only way to overcome this bias is a global deployment from day 0, because otherwise we need significantly increase the complexity of the reward system. We offer good incentives for translation of this white paper to 50 languages worldwide as well as call for action to all blockchain communities across the globe. burning of CYBER in order to increase the rank A conventional advertiser is also has a tool to participate. Any link can be promoted thus increasing cyber\u2022rank. Revenue from the promotion is burning thus decreasing the supply of CYBER and bringing value to all shareholders of a system. We do believe that this approach is fair and don't harm user experience but open opportunity to compete with conventional search ad a bit. incentivization of developers. As our primary focus is developers of decentralized and distributed applications we offer a convenient way of revenue generation in addition to possible revenue on dynamic snippets development. Every search and answer transaction can define a smart contract with up to 6 beneficiaries with a distribution of potential reward from this particular action. This creates a solid foundation for bootstrapping application ecosystem across a conventional web, mobile app stores and upcoming VR platforms. idea: split reward pool in accordance to relevance of terms. Priorities for learning the beast: Arxiv, Chains, Wiki, Github, Bittorent, IPFS We cannot make distributions based on the history of ranking object because we decide to reject an idea of storing the historical state due to tremendous costs.","title":"Discussion topics"},{"location":"cyberd/cyberd/","text":"cyberd: A search consensus computer for web3 \u00b6 @xhipster, @litvintech v 0.3. Research notes Kenig and Minsk cyb: - nick. a friendly software robot who helps you explore universes cyber: - noun. a superintelligent network computer for answers - verb. to do something intelligent, to be very smart CYB: - ticker. transferable token expressing will to become smarter CYBER: - ticker. non-transferable token expressing intelligence Content \u00b6 cyberd: A search consensus computer for web3 Content Abstract Introduction to web3 The protocol Knowledge graph Agents of knowledge Link chains Notion of consensus computer Relevance machine cyber\u2022Rank Proof of relevance State grow history problem Motivation for read requests Self prediction Universal oracle Smart contracts Selfish predictions Spam protection Distribution Incentive structure Applications Evolvability Decentralization Performance Scalability Conclusion References Abstract \u00b6 An incentivized consensus computer would allow to compute provably relevant answers without opinionated blackbox intermediaries such as Google. Stateless content-addressable peer-to-peer communication networks such as IPFS and stateful consensus computers such as Ethereum provide part of the solution but there are at least three problems associated with implementation. Of course, the first problem is subjective nature of relevance. The second problem is that it is hard to scale consensus computer of huge knowledge graph. The third problem is that the quality of such knowledge graph will suffer from different attack surfaces such as sybil and selfish behavior of interacting agents. In this paper we (1) define a protocol for provable consensus computing of relevance between IPFS objects based on some offline observation and theory behind prediction markets, (2) solve a problem of implementation inside consensus computer based on SpringRank and propose workaround for pruning historical state and (3) design distribution and incentive scheme based on our experience and known attacks. Also we discuss some considerations on minimalistic architecture of the protocol as we believe that is critical for formation of a network of domain specific search consensus computers. As result of our work some applications never existed before emerge. Introduction to web3 \u00b6 Original protocols of the Internet such as TCP/IP, DNS, URL and HTTPS brought a web into the point there it is now. Along with all benefits they has created they brought more problem into the table. Globality being a key property of the the web since inception is under real threat. Speed of connections degrade with network grow and from ubiquitous government interventions into privacy and security of web users. One property, not obvious in the beginning, become really important with everyday usage of the Internet: its ability to exchange permanent hyperlinks thus they would not break after time have pass. Reliance on one at a time internet service provider architecture allow governments censor packets is the last straw in conventional web stack for every engineer who is concerned about the future of our children. Other properties while being not so critical are very desirable: offline and real-time. Average internet user being offline must have ability to work with the state it has and after acquiring connection being able to sync with global state and continue verify state's validity in realtime while having connection. Now this properties offered on app level while such properties must be integrated into lower level protocols. The emergence of a distributed protocol stack [W3S] creates an opportunity for a new kind of Internet. We call it web3. It has a promise to remove problems of conventional protocol stack and add to the web better speed and more accessible connection. But as usually in a story with a new stack, new problems emerge. One of such problem is general purpose search. Existing general purpose search engines are restrictive centralized databases everybody forced to trust. These search engines were designed primarily for client-server architecture based on TCP/IP, DNS, URL and HTPPS protocols. Web3 create a challenge and opportunity for a search engine based on developing technologies and specifically designed for them. Surprisingly the permission-less blockchain architecture itself allows organizing general purpose search engine in a way inaccessible for previous architectures. The protocol \u00b6 def knowledge graph state take link chains check that linkchain signatures are valid check that resource consumption of a signer is not exceed 24 moving average emit prediction of links for every valid link chain every block calculate cyber\u2022rank deltas for the knowledge graph every block distribute 42 CYB based on links CYBERs for links with proven keys distribute payouts based on CYBER for links without proven keys distribute payouts according to CYBER weight of incoming links with proven keys every block apply predictions of consensus computer according to prediction bound every block write data to key/value store according to storage bound based on size and cyber\u2022rank every epoch nodes reach consensus around pruned state history via ipfs hash of state blob Knowledge graph \u00b6 We represent a knowledge graph as weighted graph of directed links between content addresses. Illustration Agents of knowledge \u00b6 Our knowledge graph include everything it can including agents themselves. This subset of content addresses with proven knowledge of private keys and cuckoo cycle pow ids is being used by incentive scheme of a consensus algorithm. Link chains \u00b6 A concept of linkchain is a convention around simple semantics of communication format with a consensus computer. a tx format is <peer id> <up to 7 ipfs hashes of links> <signature> <signature> must be valid from <peer id> Explain the concept of link chains and demonstrate a case of semantic linking based on interpretation of an ERC-20 transfer transaction or something similar Illustration Amount of content addresses ipfs-addresses: up to 7 link-chain-proof: ... cyber-protocol-current: ... Notion of consensus computer \u00b6 Consensus computer is an abstract machine that has capacity in terms of fundamental computing resources such as memory, processing and bandwidth. /// Ideal consensus computer is a computer in which sum of agents contribution as actual . Its like a perforamnce indicator resources simple part of memory, processing and bandwidth of all computers. /// (1) maximization of knowledge graph, aligned with computational, storage and broadband bound and (2) reduction for attack surfaces such as sybil and selfish behavior Illustration Relevance machine \u00b6 Ranking of knowledge graph based on prediction market on links relevance. A useful property of a computer is that it must have inductive reasoning property. She must be able to interfere predictions without any knowledge about objects except when, who and where some prediction was asked. If we assume that a consensus computer must have some information about linked objects the complexity of such model growth unpredictably, hence a requirements for a computer for memory and computations. That is, deduction of a meaning inside consensus computer is expensive thus our design hardly depend on the blindness assumption. Instead we design incentives around meaning extractions Proof of who. Digital signatures and zero knowledge proofs. Privacy is foundational. The problem is to compute rank based on link of an agent based on its ranking without revealing identity. Zero knowledge proofs in general are very expensive. Privacy of search by design or as an option? Proof of when. Proof-of-history + Tendermint Proof of where. Mining triangulations and attaching proof of location for every link chain cyber\u2022Rank \u00b6 As input for every edge value get signer account's: sums of `&lt;CYBER&gt;` plus (sum of square root from `&lt;CYB&gt;`) squared Ranking using consensus computer is hard because consensus computers bring serious resource bounds. e.g. Nebulas fail to deliver something useful onchain. No rank computed inside consensus computer => no possibility to incentivize network participants to form predictions on relevance. A problem here is that computational complexity of conventional ranks grow sublineary with the growth of the network. So we need to find (1) deterministic algorithm that allow to compute a rank for continuously appended network to scale the consensus computer to orders of magnitude that of Google. Perfect algorithm (2) must have linear memory and computation complexity. The most importantly without having (3) good prediction capabilities for existence of relevant links it proves to be useless. One of recent algorithms: SpringRank. Original idea came from physics. Links represented as system of springs with some energy. Illustration H(s) = 1/2 ... Proof of relevance \u00b6 Payouts ... Based on linkchains its possible to know which content addresses proved that private keys are exist, and which do not. We design a system under assumption that in terms of search such thing as bad behavior does not exist as nothing bad can be in the intention of finding answers. Ranks is computed on the only fact that something has been searched, thus linked and as result affected predictive model. Good analogy is observing in quantum mechanics. So no negative voting is implemented. Doing this we remove subjectivity out of the protocol and can define one possible method for proof of relevance. Also this approach significantly reduce attack surface. Implication of this assumption is that we must bind resource supply of relevance machine with demand of queries. State grow history problem \u00b6 Every N blocks cybernodes prune blockchain/history and calculate IPFS hash for them/publish to IPFS, add hash to block and validate them with consensus. This state/blob economicaly finalized and new node start from them. Cybernodes motivated to store/provide this blob cause this cause network grow. Dynamically recalculate N with economy, network size, rank score... Motivation for read requests \u00b6 Explain an economic difference and censorship impact between read search queries and write search queries Idea: All nodes run payment channels to serve request for their users and take tokens for request processing. Because this is heavy computation (only cybernodes will serve this) nodes will serve this only with payments for them. Cybernodes run protocol and earn tokens for read requests. Solution is payment channels based on HLTC and proof verification which unlocks amount earned for already served request (new signatures post via requester/user to cybernode via whisper/ipfs-pub-sub) Self prediction \u00b6 A consensus computer is able to continuously build a knowledge graph by itself predicting existence of links and applying this predictions to a state of itself. Idea: Everything that has been earned by a consensus computer can form a validators budget. Forgetting links: Prune min possible rank / 2 Universal oracle \u00b6 A consensus computer is able to store the most relevant data in key value store. She is doing it by making a decision every block about what record he want to prune and what he want to apply. This key-value store can be ... Smart contracts \u00b6 Ability to programmatically extend state based on proven knowledge graph is of paramount importance. Thus we consider that WASM programs will be available for execution on top of knowledge graph. Selfish predictions \u00b6 Protection from selfish predictions: from linear in the beginning to sum of square roots being mature. Spam protection \u00b6 In the center of spam protection system is an assumption that write operations can be executed only by those who have vested interest in the evolutionary success of a consensus computer. Every 1% of stake in consensus computer gives the ability to use 1% of possible network broadband and computing capabilities. As nobody uses all possessed broadband we can use fractional reserves while limiting broadband like ISPs do, but we cant because it degrades value of rank. So we just compute 24 hours moving avarage and In order to automate governance process we will not add a feature of staking and unstaking, but instead implement s-curve type of automatic staking time depending on which all bandwith and computational limits will be accounted Distribution \u00b6 No ERC-20. Reasons: expensive and non deterministic. Compute SpringRank for Ethereum (Link is tx, weight is amount) Create initial genesis for our network Every snapshot amount to be distributed is announced for the next snapshot. Every new PoC do distribution based on snapshot of previous chain merged with new Ethereum snapshot. For each new PoC amount of distribution which goes to Ethereum snapshot decreases. Incentive structure \u00b6 To make cyber\u2022rank economically resistant to Sybil attack and to incentivize all participant for rational behavior a system uses CYBER token. Since inception, a network prints 42 CYB every block. Reward pool is defined as 100% of emission and split among the contracts according to (?): Validators Linkers Applications \u00b6 Web3 browsers . It easy to imagine the emergence of a full-blown blockchain browser. Currently, there are several efforts for developing browsers around blockchains and distributed tech. Among them are Beaker, Mist and Brave .. All of them suffer from very limited functionality. Our developments can be useful for teams who are developing such tools. Programmable semantic cores . Relevance everywhere means that on any given user input string in any application relevant answer can be computed either globally, in the context of an app or in the context of a user. Actions in search . Proposed design enable native support for blockchain asset related activity. It is trivial to design an applications which are (1) owned by creators, (2) appear right in search results and (3) allow a transact-able call to actions with (4) provable attribution of a conversion to search query. e-Commerce has never been so easy for everybody. Offline search . IPFS make possible easy retrieval of documents from surroundings without global internet connection. cyberd itself can be distributed using IPFS. That create a possibility for ubiquitous offline search. Command tools . Command line tools can rely on relevant and structured answers from a search engine. That practically means that the following CLI tool is possible to implement > cyberd earn using 100 gb hdd Enjoy the following predictions: - apt install go-filecoin /// 0.001 btc per month - apt install siad /// 0.0001 btc per month per GB - apt install storjd /// 0.00008 btc per month per GB According to the best prediction I made a decision try `go get go-filecoin` Git clone ... Building go-filecoin Starting go-filecoin Creating wallet using your standard seed You address is .... Placing bids ... Waiting for incoming storage requests ... Search from CLI tools will inevitably create a highly competitive market of a dedicated semantic core for bots. Autonomous robots . Blockchain technology enables creation of devices which are able to earn, store, spend and invest digital assets by themselves. If a robot can earn, store, spend and invest she can do everything you can do What is needed is simple yet powerful API about the state of reality evaluated in transact-able assets. Our solution offers minimalistic but continuously self-improving API that provides necessary tools for programming economically rational robots. Language convergence . A programmer should not care about what language do the user use. We don't need to have knowledge of what language user is searching in. Entire UTF-8 spectrum is at work. A semantic core is open so competition for answering can become distributed across different domain-specific areas, including semantic cores of different languages. The unified approach creates an opportunity for cyber\u2022Bahasa. Since the Internet, we observe a process of rapid language convergence. We use more truly global words across the entire planet independently of our nationality, language and race, Name the Internet. The dream of truly global language is hard to deploy because it is hard to agree on what mean what. But we have tools to make that dream come true. It is not hard to predict that the shorter a word the more it's cyber\u2022rank will be. Global publicly available list of symbols, words and phrases sorted by cyber\u2022rank with corresponding links provided by cyberd can be the foundation for the emergence of truly global language everybody can accept. Recent scientific advances in machine translation [GNMT] are breathtaking but meaningless for those who wish to apply them without Google scale trained model. Proposed cyber\u2022rank offers exactly this. This is sure not the exhaustive list of possible applications but very exciting, though. Evolvability \u00b6 Following ideas from Tezos we can define the current state of a protocol as immutable content address. Thus she can adopt to a new environment changing content address of current protocol following the rules hidden behind previous protocol. We would love to check the hypothesis that it is possible to have a protocol which follows simple rule: The more closer some content address to literally cyber-protocol-current ipfs address the more probability than it will become winning. The most close protocol cyber-protocol-current is the protocol which is the most relevant to users. Thus nodes must always signal connection with cyber-protocol-current by sending linkchains with semantics like <{cyber-protocol-current}> <cid> . Decentralization \u00b6 Decentralization comes with costs and slowness. We want to find a good balance between those as we believe both are sensitive for widespread web3 adoption. That is the area of research for us now. Performance \u00b6 We need very fast conformation times. Proposed blockchain design is based on Tendermint consensus algorithm and has fast and predictable before seconds block confirmation time and very fast finality time. Average confirmation timeframe is less than second thus conformations can be asynchronous and nearly invisible for users. A good thing is that users don't need confirmations at all before getting search response as there is no risk associated with that. Scalability \u00b6 Let us say that our node implementation based on Cosmos-SDK can process 10k transactions per second. Thus every day at least 8.64 million nodes can submit 100 predictions and get back search results simultaneously. That is enough to verify all assumptions in the wild. As blockchain technology evolve we want to check that every hypothesis work before scale it further. Conclusion \u00b6 We describe a motivated blockchain based search engine for web3. A search engine is based on the content-addressable peer-to-peer paradigm and uses IPFS as a foundation. IPFS provide significant benefits in terms of resources consumption. IPFS addresses as a primary objects are robust in its simplicity. For every IPFS hash cyber\u2022rank is computed by a consensus computer with no single point of failure. Cyber\u2022rank is a spring rank with economic protection from selfish predictions. Sybil resistance is also implemented on two levels: during id generation and during bandwidth limiting. Embedded smart contracts offer fair compensations for those who is able to predict relevance of content addresses. The primary goal is indexing of peer-to-peer systems with self-authenticated data either stateless, such as IPFS, Swarm, DAT, Git, BitTorent, or stateful such as Bitcoin, Ethereum and other blockchains and tangles. Proposed semantics of linking offers robust mechanism for predicting meaningful relations between objects. A source code of a relevance machine is open source. Every bit of data accumulated by a consensus computer is available for everybody if the one has resources to process it. The performance of proposed software implementation is sufficient for seamless user interactions. Scalability of proposed implementation is enough to index all self-authenticated data that exist today. The blockchain is managed by a decentralized autonomous organization which functions under Tendermint consensus algorithm. Thought a system provide necessary utility to offer an alternative for conventional search engines it is not limited to this use case either. The system is extendable for numerous applications and e.g. makes possible to design economically rational self-owned robots that are able to autonomously understand objects around them. References \u00b6 W3S: Web3 stack","title":"Overview"},{"location":"cyberd/cyberd/#cyberd-a-search-consensus-computer-for-web3","text":"@xhipster, @litvintech v 0.3. Research notes Kenig and Minsk cyb: - nick. a friendly software robot who helps you explore universes cyber: - noun. a superintelligent network computer for answers - verb. to do something intelligent, to be very smart CYB: - ticker. transferable token expressing will to become smarter CYBER: - ticker. non-transferable token expressing intelligence","title":"cyberd: A search consensus computer for web3"},{"location":"cyberd/cyberd/#content","text":"cyberd: A search consensus computer for web3 Content Abstract Introduction to web3 The protocol Knowledge graph Agents of knowledge Link chains Notion of consensus computer Relevance machine cyber\u2022Rank Proof of relevance State grow history problem Motivation for read requests Self prediction Universal oracle Smart contracts Selfish predictions Spam protection Distribution Incentive structure Applications Evolvability Decentralization Performance Scalability Conclusion References","title":"Content"},{"location":"cyberd/cyberd/#abstract","text":"An incentivized consensus computer would allow to compute provably relevant answers without opinionated blackbox intermediaries such as Google. Stateless content-addressable peer-to-peer communication networks such as IPFS and stateful consensus computers such as Ethereum provide part of the solution but there are at least three problems associated with implementation. Of course, the first problem is subjective nature of relevance. The second problem is that it is hard to scale consensus computer of huge knowledge graph. The third problem is that the quality of such knowledge graph will suffer from different attack surfaces such as sybil and selfish behavior of interacting agents. In this paper we (1) define a protocol for provable consensus computing of relevance between IPFS objects based on some offline observation and theory behind prediction markets, (2) solve a problem of implementation inside consensus computer based on SpringRank and propose workaround for pruning historical state and (3) design distribution and incentive scheme based on our experience and known attacks. Also we discuss some considerations on minimalistic architecture of the protocol as we believe that is critical for formation of a network of domain specific search consensus computers. As result of our work some applications never existed before emerge.","title":"Abstract"},{"location":"cyberd/cyberd/#introduction-to-web3","text":"Original protocols of the Internet such as TCP/IP, DNS, URL and HTTPS brought a web into the point there it is now. Along with all benefits they has created they brought more problem into the table. Globality being a key property of the the web since inception is under real threat. Speed of connections degrade with network grow and from ubiquitous government interventions into privacy and security of web users. One property, not obvious in the beginning, become really important with everyday usage of the Internet: its ability to exchange permanent hyperlinks thus they would not break after time have pass. Reliance on one at a time internet service provider architecture allow governments censor packets is the last straw in conventional web stack for every engineer who is concerned about the future of our children. Other properties while being not so critical are very desirable: offline and real-time. Average internet user being offline must have ability to work with the state it has and after acquiring connection being able to sync with global state and continue verify state's validity in realtime while having connection. Now this properties offered on app level while such properties must be integrated into lower level protocols. The emergence of a distributed protocol stack [W3S] creates an opportunity for a new kind of Internet. We call it web3. It has a promise to remove problems of conventional protocol stack and add to the web better speed and more accessible connection. But as usually in a story with a new stack, new problems emerge. One of such problem is general purpose search. Existing general purpose search engines are restrictive centralized databases everybody forced to trust. These search engines were designed primarily for client-server architecture based on TCP/IP, DNS, URL and HTPPS protocols. Web3 create a challenge and opportunity for a search engine based on developing technologies and specifically designed for them. Surprisingly the permission-less blockchain architecture itself allows organizing general purpose search engine in a way inaccessible for previous architectures.","title":"Introduction to web3"},{"location":"cyberd/cyberd/#the-protocol","text":"def knowledge graph state take link chains check that linkchain signatures are valid check that resource consumption of a signer is not exceed 24 moving average emit prediction of links for every valid link chain every block calculate cyber\u2022rank deltas for the knowledge graph every block distribute 42 CYB based on links CYBERs for links with proven keys distribute payouts based on CYBER for links without proven keys distribute payouts according to CYBER weight of incoming links with proven keys every block apply predictions of consensus computer according to prediction bound every block write data to key/value store according to storage bound based on size and cyber\u2022rank every epoch nodes reach consensus around pruned state history via ipfs hash of state blob","title":"The protocol"},{"location":"cyberd/cyberd/#knowledge-graph","text":"We represent a knowledge graph as weighted graph of directed links between content addresses. Illustration","title":"Knowledge graph"},{"location":"cyberd/cyberd/#agents-of-knowledge","text":"Our knowledge graph include everything it can including agents themselves. This subset of content addresses with proven knowledge of private keys and cuckoo cycle pow ids is being used by incentive scheme of a consensus algorithm.","title":"Agents of knowledge"},{"location":"cyberd/cyberd/#link-chains","text":"A concept of linkchain is a convention around simple semantics of communication format with a consensus computer. a tx format is <peer id> <up to 7 ipfs hashes of links> <signature> <signature> must be valid from <peer id> Explain the concept of link chains and demonstrate a case of semantic linking based on interpretation of an ERC-20 transfer transaction or something similar Illustration Amount of content addresses ipfs-addresses: up to 7 link-chain-proof: ... cyber-protocol-current: ...","title":"Link chains"},{"location":"cyberd/cyberd/#notion-of-consensus-computer","text":"Consensus computer is an abstract machine that has capacity in terms of fundamental computing resources such as memory, processing and bandwidth. /// Ideal consensus computer is a computer in which sum of agents contribution as actual . Its like a perforamnce indicator resources simple part of memory, processing and bandwidth of all computers. /// (1) maximization of knowledge graph, aligned with computational, storage and broadband bound and (2) reduction for attack surfaces such as sybil and selfish behavior Illustration","title":"Notion of consensus computer"},{"location":"cyberd/cyberd/#relevance-machine","text":"Ranking of knowledge graph based on prediction market on links relevance. A useful property of a computer is that it must have inductive reasoning property. She must be able to interfere predictions without any knowledge about objects except when, who and where some prediction was asked. If we assume that a consensus computer must have some information about linked objects the complexity of such model growth unpredictably, hence a requirements for a computer for memory and computations. That is, deduction of a meaning inside consensus computer is expensive thus our design hardly depend on the blindness assumption. Instead we design incentives around meaning extractions Proof of who. Digital signatures and zero knowledge proofs. Privacy is foundational. The problem is to compute rank based on link of an agent based on its ranking without revealing identity. Zero knowledge proofs in general are very expensive. Privacy of search by design or as an option? Proof of when. Proof-of-history + Tendermint Proof of where. Mining triangulations and attaching proof of location for every link chain","title":"Relevance machine"},{"location":"cyberd/cyberd/#cyberrank","text":"As input for every edge value get signer account's: sums of `&lt;CYBER&gt;` plus (sum of square root from `&lt;CYB&gt;`) squared Ranking using consensus computer is hard because consensus computers bring serious resource bounds. e.g. Nebulas fail to deliver something useful onchain. No rank computed inside consensus computer => no possibility to incentivize network participants to form predictions on relevance. A problem here is that computational complexity of conventional ranks grow sublineary with the growth of the network. So we need to find (1) deterministic algorithm that allow to compute a rank for continuously appended network to scale the consensus computer to orders of magnitude that of Google. Perfect algorithm (2) must have linear memory and computation complexity. The most importantly without having (3) good prediction capabilities for existence of relevant links it proves to be useless. One of recent algorithms: SpringRank. Original idea came from physics. Links represented as system of springs with some energy. Illustration H(s) = 1/2 ...","title":"cyber\u2022Rank"},{"location":"cyberd/cyberd/#proof-of-relevance","text":"Payouts ... Based on linkchains its possible to know which content addresses proved that private keys are exist, and which do not. We design a system under assumption that in terms of search such thing as bad behavior does not exist as nothing bad can be in the intention of finding answers. Ranks is computed on the only fact that something has been searched, thus linked and as result affected predictive model. Good analogy is observing in quantum mechanics. So no negative voting is implemented. Doing this we remove subjectivity out of the protocol and can define one possible method for proof of relevance. Also this approach significantly reduce attack surface. Implication of this assumption is that we must bind resource supply of relevance machine with demand of queries.","title":"Proof of relevance"},{"location":"cyberd/cyberd/#state-grow-history-problem","text":"Every N blocks cybernodes prune blockchain/history and calculate IPFS hash for them/publish to IPFS, add hash to block and validate them with consensus. This state/blob economicaly finalized and new node start from them. Cybernodes motivated to store/provide this blob cause this cause network grow. Dynamically recalculate N with economy, network size, rank score...","title":"State grow history problem"},{"location":"cyberd/cyberd/#motivation-for-read-requests","text":"Explain an economic difference and censorship impact between read search queries and write search queries Idea: All nodes run payment channels to serve request for their users and take tokens for request processing. Because this is heavy computation (only cybernodes will serve this) nodes will serve this only with payments for them. Cybernodes run protocol and earn tokens for read requests. Solution is payment channels based on HLTC and proof verification which unlocks amount earned for already served request (new signatures post via requester/user to cybernode via whisper/ipfs-pub-sub)","title":"Motivation for read requests"},{"location":"cyberd/cyberd/#self-prediction","text":"A consensus computer is able to continuously build a knowledge graph by itself predicting existence of links and applying this predictions to a state of itself. Idea: Everything that has been earned by a consensus computer can form a validators budget. Forgetting links: Prune min possible rank / 2","title":"Self prediction"},{"location":"cyberd/cyberd/#universal-oracle","text":"A consensus computer is able to store the most relevant data in key value store. She is doing it by making a decision every block about what record he want to prune and what he want to apply. This key-value store can be ...","title":"Universal oracle"},{"location":"cyberd/cyberd/#smart-contracts","text":"Ability to programmatically extend state based on proven knowledge graph is of paramount importance. Thus we consider that WASM programs will be available for execution on top of knowledge graph.","title":"Smart contracts"},{"location":"cyberd/cyberd/#selfish-predictions","text":"Protection from selfish predictions: from linear in the beginning to sum of square roots being mature.","title":"Selfish predictions"},{"location":"cyberd/cyberd/#spam-protection","text":"In the center of spam protection system is an assumption that write operations can be executed only by those who have vested interest in the evolutionary success of a consensus computer. Every 1% of stake in consensus computer gives the ability to use 1% of possible network broadband and computing capabilities. As nobody uses all possessed broadband we can use fractional reserves while limiting broadband like ISPs do, but we cant because it degrades value of rank. So we just compute 24 hours moving avarage and In order to automate governance process we will not add a feature of staking and unstaking, but instead implement s-curve type of automatic staking time depending on which all bandwith and computational limits will be accounted","title":"Spam protection"},{"location":"cyberd/cyberd/#distribution","text":"No ERC-20. Reasons: expensive and non deterministic. Compute SpringRank for Ethereum (Link is tx, weight is amount) Create initial genesis for our network Every snapshot amount to be distributed is announced for the next snapshot. Every new PoC do distribution based on snapshot of previous chain merged with new Ethereum snapshot. For each new PoC amount of distribution which goes to Ethereum snapshot decreases.","title":"Distribution"},{"location":"cyberd/cyberd/#incentive-structure","text":"To make cyber\u2022rank economically resistant to Sybil attack and to incentivize all participant for rational behavior a system uses CYBER token. Since inception, a network prints 42 CYB every block. Reward pool is defined as 100% of emission and split among the contracts according to (?): Validators Linkers","title":"Incentive structure"},{"location":"cyberd/cyberd/#applications","text":"Web3 browsers . It easy to imagine the emergence of a full-blown blockchain browser. Currently, there are several efforts for developing browsers around blockchains and distributed tech. Among them are Beaker, Mist and Brave .. All of them suffer from very limited functionality. Our developments can be useful for teams who are developing such tools. Programmable semantic cores . Relevance everywhere means that on any given user input string in any application relevant answer can be computed either globally, in the context of an app or in the context of a user. Actions in search . Proposed design enable native support for blockchain asset related activity. It is trivial to design an applications which are (1) owned by creators, (2) appear right in search results and (3) allow a transact-able call to actions with (4) provable attribution of a conversion to search query. e-Commerce has never been so easy for everybody. Offline search . IPFS make possible easy retrieval of documents from surroundings without global internet connection. cyberd itself can be distributed using IPFS. That create a possibility for ubiquitous offline search. Command tools . Command line tools can rely on relevant and structured answers from a search engine. That practically means that the following CLI tool is possible to implement > cyberd earn using 100 gb hdd Enjoy the following predictions: - apt install go-filecoin /// 0.001 btc per month - apt install siad /// 0.0001 btc per month per GB - apt install storjd /// 0.00008 btc per month per GB According to the best prediction I made a decision try `go get go-filecoin` Git clone ... Building go-filecoin Starting go-filecoin Creating wallet using your standard seed You address is .... Placing bids ... Waiting for incoming storage requests ... Search from CLI tools will inevitably create a highly competitive market of a dedicated semantic core for bots. Autonomous robots . Blockchain technology enables creation of devices which are able to earn, store, spend and invest digital assets by themselves. If a robot can earn, store, spend and invest she can do everything you can do What is needed is simple yet powerful API about the state of reality evaluated in transact-able assets. Our solution offers minimalistic but continuously self-improving API that provides necessary tools for programming economically rational robots. Language convergence . A programmer should not care about what language do the user use. We don't need to have knowledge of what language user is searching in. Entire UTF-8 spectrum is at work. A semantic core is open so competition for answering can become distributed across different domain-specific areas, including semantic cores of different languages. The unified approach creates an opportunity for cyber\u2022Bahasa. Since the Internet, we observe a process of rapid language convergence. We use more truly global words across the entire planet independently of our nationality, language and race, Name the Internet. The dream of truly global language is hard to deploy because it is hard to agree on what mean what. But we have tools to make that dream come true. It is not hard to predict that the shorter a word the more it's cyber\u2022rank will be. Global publicly available list of symbols, words and phrases sorted by cyber\u2022rank with corresponding links provided by cyberd can be the foundation for the emergence of truly global language everybody can accept. Recent scientific advances in machine translation [GNMT] are breathtaking but meaningless for those who wish to apply them without Google scale trained model. Proposed cyber\u2022rank offers exactly this. This is sure not the exhaustive list of possible applications but very exciting, though.","title":"Applications"},{"location":"cyberd/cyberd/#evolvability","text":"Following ideas from Tezos we can define the current state of a protocol as immutable content address. Thus she can adopt to a new environment changing content address of current protocol following the rules hidden behind previous protocol. We would love to check the hypothesis that it is possible to have a protocol which follows simple rule: The more closer some content address to literally cyber-protocol-current ipfs address the more probability than it will become winning. The most close protocol cyber-protocol-current is the protocol which is the most relevant to users. Thus nodes must always signal connection with cyber-protocol-current by sending linkchains with semantics like <{cyber-protocol-current}> <cid> .","title":"Evolvability"},{"location":"cyberd/cyberd/#decentralization","text":"Decentralization comes with costs and slowness. We want to find a good balance between those as we believe both are sensitive for widespread web3 adoption. That is the area of research for us now.","title":"Decentralization"},{"location":"cyberd/cyberd/#performance","text":"We need very fast conformation times. Proposed blockchain design is based on Tendermint consensus algorithm and has fast and predictable before seconds block confirmation time and very fast finality time. Average confirmation timeframe is less than second thus conformations can be asynchronous and nearly invisible for users. A good thing is that users don't need confirmations at all before getting search response as there is no risk associated with that.","title":"Performance"},{"location":"cyberd/cyberd/#scalability","text":"Let us say that our node implementation based on Cosmos-SDK can process 10k transactions per second. Thus every day at least 8.64 million nodes can submit 100 predictions and get back search results simultaneously. That is enough to verify all assumptions in the wild. As blockchain technology evolve we want to check that every hypothesis work before scale it further.","title":"Scalability"},{"location":"cyberd/cyberd/#conclusion","text":"We describe a motivated blockchain based search engine for web3. A search engine is based on the content-addressable peer-to-peer paradigm and uses IPFS as a foundation. IPFS provide significant benefits in terms of resources consumption. IPFS addresses as a primary objects are robust in its simplicity. For every IPFS hash cyber\u2022rank is computed by a consensus computer with no single point of failure. Cyber\u2022rank is a spring rank with economic protection from selfish predictions. Sybil resistance is also implemented on two levels: during id generation and during bandwidth limiting. Embedded smart contracts offer fair compensations for those who is able to predict relevance of content addresses. The primary goal is indexing of peer-to-peer systems with self-authenticated data either stateless, such as IPFS, Swarm, DAT, Git, BitTorent, or stateful such as Bitcoin, Ethereum and other blockchains and tangles. Proposed semantics of linking offers robust mechanism for predicting meaningful relations between objects. A source code of a relevance machine is open source. Every bit of data accumulated by a consensus computer is available for everybody if the one has resources to process it. The performance of proposed software implementation is sufficient for seamless user interactions. Scalability of proposed implementation is enough to index all self-authenticated data that exist today. The blockchain is managed by a decentralized autonomous organization which functions under Tendermint consensus algorithm. Thought a system provide necessary utility to offer an alternative for conventional search engines it is not limited to this use case either. The system is extendable for numerous applications and e.g. makes possible to design economically rational self-owned robots that are able to autonomously understand objects around them.","title":"Conclusion"},{"location":"cyberd/cyberd/#references","text":"W3S: Web3 stack","title":"References"},{"location":"cyberd/earth/","text":"Earth cheat sheets \u00b6 # Copy from earth scp -P 33224 earth@earth.cybernode.ai:/path/file /host/path/file # Copy to earth scp -P 33324 testnet/genesis.json earth@earth.cybernode.ai:/cyberdata/cyberd/config/genesis.json scp -P 33324 testnet/config.toml earth@earth.cybernode.ai:/cyberdata/cyberd/config/config.toml docker run -d --restart always --name=cyberd --runtime=nvidia \\ -p 34656:26656 -p 34657:26657 -p 34660:26660 \\ -v /cyberdata/cyberd:/root/.cyberd \\ cyberd/cyberd:euler-dev0 docker run --rm \\ -v /cyberdata/cyberd:/root/.cyberd \\ cyberd/cyberd:euler-dev0 cyberd unsafe-reset-all","title":"Earth cheat sheets"},{"location":"cyberd/earth/#earth-cheat-sheets","text":"# Copy from earth scp -P 33224 earth@earth.cybernode.ai:/path/file /host/path/file # Copy to earth scp -P 33324 testnet/genesis.json earth@earth.cybernode.ai:/cyberdata/cyberd/config/genesis.json scp -P 33324 testnet/config.toml earth@earth.cybernode.ai:/cyberdata/cyberd/config/config.toml docker run -d --restart always --name=cyberd --runtime=nvidia \\ -p 34656:26656 -p 34657:26657 -p 34660:26660 \\ -v /cyberdata/cyberd:/root/.cyberd \\ cyberd/cyberd:euler-dev0 docker run --rm \\ -v /cyberdata/cyberd:/root/.cyberd \\ cyberd/cyberd:euler-dev0 cyberd unsafe-reset-all","title":"Earth cheat sheets"},{"location":"cyberd/rpc/","text":"API reference \u00b6 Cyberd provides a JSON-RPC API. Http endpoint is served under localhost:20657 . WebSockets are the preferred transport for cyberd RPC and are used by applications such as cyb. Default WebSocket connection endpoint for cyberd is ws://localhost:20657/websocket . There are test endpoints available at http://earth.cybernode.ai:34657 and ws://earth.cybernode.ai:34657/websocket . Standard Methods \u00b6 Query Example \u00b6 Query http endpoint using curl: curl --data '{\"method\":\"cyberd_mostResentBlockNumber\",\"params\":[],\"id\":\"1\",\"jsonrpc\":\"2.0\"}' \\ -H \"Content-Type: application/json\" -X POST earth.cybernode.ai:34657 Query ws endpoint from js: let websocket = new WebSocket ( \"ws://earth.cybernode.ai:34657/websocket\" ); websocket . send ( JSON . stringify ({ \"method\" : \"cyberd_mostResentBlockNumber\" , \"params\" : [], \"id\" : \"1\" , \"jsonrpc\" : \"2.0\" })); Method Overview \u00b6 The following is an overview of the RPC methods and their current status. Click the method name for further details such as parameter and return information. # Method Description 1 mostResentBlockNumber Returns the number of most recent block. Method Details \u00b6 Method cyberd_mostResentBlockNumber Parameters None Description Returns the number of most recent block. Returns int Example Return 42 Return to Overview Notifications (WebSocket-specific) \u00b6 Cyberd uses standard JSON-RPC notifications to notify clients of changes, rather than requiring clients to poll cyberd for updates. JSON-RPC notifications are a subset of requests, but do not contain an ID. The notification type is categorized by the query params field. Subscribe Example \u00b6 Subscribe for new blocks header from js: js let websocket = new WebSocket(\"ws://earth.cybernode.ai:34657/websocket\"); websocket.send(JSON.stringify({ \"method\": \"subscribe\", \"params\": [\"tm.event='NewBlockHeader'\"], \"id\": \"1\", \"jsonrpc\": \"2.0\" })); Events Overview \u00b6 # Event Description 1 NewBlockHeader Sends block header notification when a new block is committed. Events Details \u00b6 NewBlockHeader \u00b6 Event NewBlockHeader Description Sends block header notification when a new block is committed. Query 'tm.event=\\'NewBlockHeader\\'' Return to Overview { \"jsonrpc\" : \"2.0\" , \"id\" : \"1#event\" , \"result\" : { \"query\" : \"tm.event='NewBlockHeader'\" , \"data\" : { \"type\" : \"tendermint/event/NewBlockHeader\" , \"value\" : { \"header\" : { \"chain_id\" : \"test-chain-gRXWCL\" , \"height\" : \"40561\" , \"time\" : \"2018-11-08T14:40:11.820674115Z\" , \"num_txs\" : \"0\" , \"total_txs\" : \"510\" , \"last_block_id\" : { \"hash\" : \"CC1693981B0353C907EBC2EBEB1578ABD21C955D\" , \"parts\" : { \"total\" : \"1\" , \"hash\" : \"E854DA23981283464484FEA41D8AB5FF7697728C\" } }, \"last_commit_hash\" : \"51E1DEBA90591C532E8C58BF99F29AE4B2264644\" , \"data_hash\" : \"\" , \"validators_hash\" : \"985037CEBE01051C949F01278621449712C85715\" , \"next_validators_hash\" : \"985037CEBE01051C949F01278621449712C85715\" , \"consensus_hash\" : \"0E520AF30D47BE28F293E040E418D0361BFB5370\" , \"app_hash\" : \"2DBED732547DD6A3223027EF8C80FD3C3A1AD11420CA899533DCFCA260F8D170\" , \"last_results_hash\" : \"\" , \"evidence_hash\" : \"\" , \"proposer_address\" : \"A41C2ED742E47A9BDC4E71BE4B879F949045EC97\" } } } } }","title":"API reference"},{"location":"cyberd/rpc/#api-reference","text":"Cyberd provides a JSON-RPC API. Http endpoint is served under localhost:20657 . WebSockets are the preferred transport for cyberd RPC and are used by applications such as cyb. Default WebSocket connection endpoint for cyberd is ws://localhost:20657/websocket . There are test endpoints available at http://earth.cybernode.ai:34657 and ws://earth.cybernode.ai:34657/websocket .","title":"API reference"},{"location":"cyberd/rpc/#standard-methods","text":"","title":"Standard Methods"},{"location":"cyberd/rpc/#query-example","text":"Query http endpoint using curl: curl --data '{\"method\":\"cyberd_mostResentBlockNumber\",\"params\":[],\"id\":\"1\",\"jsonrpc\":\"2.0\"}' \\ -H \"Content-Type: application/json\" -X POST earth.cybernode.ai:34657 Query ws endpoint from js: let websocket = new WebSocket ( \"ws://earth.cybernode.ai:34657/websocket\" ); websocket . send ( JSON . stringify ({ \"method\" : \"cyberd_mostResentBlockNumber\" , \"params\" : [], \"id\" : \"1\" , \"jsonrpc\" : \"2.0\" }));","title":"Query Example"},{"location":"cyberd/rpc/#method-overview","text":"The following is an overview of the RPC methods and their current status. Click the method name for further details such as parameter and return information. # Method Description 1 mostResentBlockNumber Returns the number of most recent block.","title":"Method Overview"},{"location":"cyberd/rpc/#method-details","text":"Method cyberd_mostResentBlockNumber Parameters None Description Returns the number of most recent block. Returns int Example Return 42 Return to Overview","title":"Method Details"},{"location":"cyberd/rpc/#notifications-websocket-specific","text":"Cyberd uses standard JSON-RPC notifications to notify clients of changes, rather than requiring clients to poll cyberd for updates. JSON-RPC notifications are a subset of requests, but do not contain an ID. The notification type is categorized by the query params field.","title":"Notifications (WebSocket-specific)"},{"location":"cyberd/rpc/#subscribe-example","text":"Subscribe for new blocks header from js: js let websocket = new WebSocket(\"ws://earth.cybernode.ai:34657/websocket\"); websocket.send(JSON.stringify({ \"method\": \"subscribe\", \"params\": [\"tm.event='NewBlockHeader'\"], \"id\": \"1\", \"jsonrpc\": \"2.0\" }));","title":"Subscribe Example"},{"location":"cyberd/rpc/#events-overview","text":"# Event Description 1 NewBlockHeader Sends block header notification when a new block is committed.","title":"Events Overview"},{"location":"cyberd/rpc/#events-details","text":"","title":"Events Details"},{"location":"cyberd/rpc/#newblockheader","text":"Event NewBlockHeader Description Sends block header notification when a new block is committed. Query 'tm.event=\\'NewBlockHeader\\'' Return to Overview { \"jsonrpc\" : \"2.0\" , \"id\" : \"1#event\" , \"result\" : { \"query\" : \"tm.event='NewBlockHeader'\" , \"data\" : { \"type\" : \"tendermint/event/NewBlockHeader\" , \"value\" : { \"header\" : { \"chain_id\" : \"test-chain-gRXWCL\" , \"height\" : \"40561\" , \"time\" : \"2018-11-08T14:40:11.820674115Z\" , \"num_txs\" : \"0\" , \"total_txs\" : \"510\" , \"last_block_id\" : { \"hash\" : \"CC1693981B0353C907EBC2EBEB1578ABD21C955D\" , \"parts\" : { \"total\" : \"1\" , \"hash\" : \"E854DA23981283464484FEA41D8AB5FF7697728C\" } }, \"last_commit_hash\" : \"51E1DEBA90591C532E8C58BF99F29AE4B2264644\" , \"data_hash\" : \"\" , \"validators_hash\" : \"985037CEBE01051C949F01278621449712C85715\" , \"next_validators_hash\" : \"985037CEBE01051C949F01278621449712C85715\" , \"consensus_hash\" : \"0E520AF30D47BE28F293E040E418D0361BFB5370\" , \"app_hash\" : \"2DBED732547DD6A3223027EF8C80FD3C3A1AD11420CA899533DCFCA260F8D170\" , \"last_results_hash\" : \"\" , \"evidence_hash\" : \"\" , \"proposer_address\" : \"A41C2ED742E47A9BDC4E71BE4B879F949045EC97\" } } } } }","title":"NewBlockHeader"},{"location":"cyberd/run_testnet/","text":"Running two validators testnet \u00b6 First machine - First validator \u00b6 Build cyberd daemon and cli with default instructions. Let's suppose binaries named daemon and cli . Create new account and copy seed phrase. ./cli keys add testnet_v2 Create network files for one validator: ./daemon testnet --v 1 Add generated previously account to genesis. ./daemon add-genesis-account cbd1pnuzd4tvqvrawjtk6q8wwvg9cmw4mnnmy9skdy 500CBD --home = ./mytestnet/node0/cyberd cp testnet/config.toml ./mytestnet/node0/cyberd/config/config.toml Run daemon ./daemon start --home = ./mytestnet/node0/cyberd Second machine - Second validator \u00b6 Build cyberd daemon and cli with default instructions. Let's suppose binaries named daemon and cli . Init daemon. ./daemon init --home=./mytestnet/ Copy genesis and config scp -P 33324 earth@earth.cybernode.ai:/home/earth/cyberd/mytestnet/node0/cyberd/config/genesis.json ./mytestnet/cyberd/config scp -P 33324 earth@earth.cybernode.ai:/home/earth/cyberd/mytestnet/node0/cyberd/config/config.toml ./mytestnet/cyberd/config ./daemon tendermint show-validator --home = ./mytestnet/ ./cli tx stake create-validator \\ --amount = 100CBD \\ --pubkey = cbdvalconspub1zcjduepq2pr9cqd9apves6av0xafzgzvvl0ky8904wlkecwu4vghxteutrsq8pcfa7 \\ --moniker = \"hlb\" \\ --chain-id = chain-hFpwd3 \\ --from = testnet_hlb \\ --commission-rate = \"0.10\" \\ --commission-max-rate = \"0.20\" \\ --commission-max-change-rate = \"0.01\"","title":"Running two validators testnet"},{"location":"cyberd/run_testnet/#running-two-validators-testnet","text":"","title":"Running two validators testnet"},{"location":"cyberd/run_testnet/#first-machine-first-validator","text":"Build cyberd daemon and cli with default instructions. Let's suppose binaries named daemon and cli . Create new account and copy seed phrase. ./cli keys add testnet_v2 Create network files for one validator: ./daemon testnet --v 1 Add generated previously account to genesis. ./daemon add-genesis-account cbd1pnuzd4tvqvrawjtk6q8wwvg9cmw4mnnmy9skdy 500CBD --home = ./mytestnet/node0/cyberd cp testnet/config.toml ./mytestnet/node0/cyberd/config/config.toml Run daemon ./daemon start --home = ./mytestnet/node0/cyberd","title":"First machine - First validator"},{"location":"cyberd/run_testnet/#second-machine-second-validator","text":"Build cyberd daemon and cli with default instructions. Let's suppose binaries named daemon and cli . Init daemon. ./daemon init --home=./mytestnet/ Copy genesis and config scp -P 33324 earth@earth.cybernode.ai:/home/earth/cyberd/mytestnet/node0/cyberd/config/genesis.json ./mytestnet/cyberd/config scp -P 33324 earth@earth.cybernode.ai:/home/earth/cyberd/mytestnet/node0/cyberd/config/config.toml ./mytestnet/cyberd/config ./daemon tendermint show-validator --home = ./mytestnet/ ./cli tx stake create-validator \\ --amount = 100CBD \\ --pubkey = cbdvalconspub1zcjduepq2pr9cqd9apves6av0xafzgzvvl0ky8904wlkecwu4vghxteutrsq8pcfa7 \\ --moniker = \"hlb\" \\ --chain-id = chain-hFpwd3 \\ --from = testnet_hlb \\ --commission-rate = \"0.10\" \\ --commission-max-rate = \"0.20\" \\ --commission-max-change-rate = \"0.01\"","title":"Second machine - Second validator"},{"location":"cyberd/run_validator/","text":"Join Cyberd Network As Validator \u00b6 Prepare your server \u00b6 First, you have to setup a server. You are supposed to run your validator node all time, so you will need a reliable server to keep it running. Also, you may consider to use any cloud services like AWS or DigitalOcean. Cyberd is based on Cosmos SDK written in Go. It should work on any platform which can compile and run programs in Go. However, I strongly recommend running the validator node on a Linux server. Here is the current required server specification to run validator node: No. of CPUs: 6 RAM: 32GB Card with Nvidia CUDA support(ex 1080ti) and at least 8gb VRAM. Disk: 256GB SSD Install Dependencies \u00b6 Our main distribution unit is docker container. All images are located in default Dockerhub registry . Rank calculated on GPU using CUDA Toolkit . In order to access GPU from container, nvidia drivers version 410+ and nvidia docker runtime should be installed on host system. Check both driver and docker runtime installed correctly: docker run --runtime = nvidia --rm nvidia/cuda:10.0-base nvidia-smi # Should be displayed something like this. Tue Dec 11 18 :02:15 2018 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 410 .78 Driver Version: 410 .78 CUDA Version: 10 .0 | | -------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | =============================== + ====================== + ====================== | | 0 GeForce GTX 1050 Off | 00000000 :01:00.0 Off | N/A | | N/A 52C P0 N/A / N/A | 302MiB / 4042MiB | 2 % Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | | ============================================================================= | +-----------------------------------------------------------------------------+ Start Fullnode \u00b6 First, create folder, where daemon and cli will store data. Some commands may require admin privileges. mkdir /cyberd Run daemon with mounted volumes on created during previous step folder. docker run -d --name = cyberd --runtime = nvidia \\ -p 26656 :26656 -p 26657 :26657 -p 26660 :26660 \\ -v /cyberd/daemon:/root/.cyberd \\ -v /cyberd/cli:/root/.cyberdcli \\ cyberd/cyberd:euler-dev0 To check if your node is connected to the testnet, you can run this: docker exec cyberd cyberdcli status You should be seeing a returned JSON with your node status including node_info, sync_info and validator_info. Prepare stake address \u00b6 Alright, you are now connected to the testnet. To be a validator, you will need some CBD (cyberd coin) to be bounded as your stake. Top 146 validators by bounded stake will be active validators taking part in consensus. If you already have address with CBD , just restore it with your seed phrase into your local keystore. docker exec -ti cyberd cyberdcli keys add <your_key_name> --recover docker exec cyberd cyberdcli show <your_key_name> If no, create new one using command below. Also, you should send coins to that address to bound them later during validator submitting. docker exec -ti cyberd cyberdcli add <your_key_name> docker exec cyberd cyberdcli show <your_key_name> is any name you pick to represent this key pair. You have to refer to this later when you use the keys to sign transactions. It will ask you to enter your password twice to encrypt the key. You also need to enter your password when you use your key to sign any transaction. The command returns the address, public key and a seed phrase which you can use it to recover your account if you forget your password later. Keep the seed phrase in a safe place in case you have to use them. The address showing here is your account address. Let\u2019s call this . It stores your assets. Send create validator transaction \u00b6 Validators are actors on the network committing new blocks by submitting their votes. It refers to the node itself, not a single person or a single account. Therefore, the public key here is referring to the node public key, not the public key of the address you have just created. To get the node public key, run the following command. docker exec cyberd cyberd tendermint show_validator It will return a bech32 public key. Let\u2019s call it . The next step you have to declare a validator candidate. The validator candidate is the account which stake the coins. So the validator candidate is an account this time. To declare a validator candidate, run the following command adjusting stake amount and other fields. docker exec cyberd cyberdcli tx stake create-validator \\ --amount = 100CBD \\ --pubkey = <your_node_pubkey> \\ --moniker = <your_node_nickname> \\ --trust-node \\ --from = <your_key_name> \\ --commission-rate = \"0.10\" \\ --commission-max-rate = \"0.20\" \\ --commission-max-change-rate = \"0.01\"","title":"Join Cyberd Network As Validator"},{"location":"cyberd/run_validator/#join-cyberd-network-as-validator","text":"","title":"Join Cyberd Network As Validator"},{"location":"cyberd/run_validator/#prepare-your-server","text":"First, you have to setup a server. You are supposed to run your validator node all time, so you will need a reliable server to keep it running. Also, you may consider to use any cloud services like AWS or DigitalOcean. Cyberd is based on Cosmos SDK written in Go. It should work on any platform which can compile and run programs in Go. However, I strongly recommend running the validator node on a Linux server. Here is the current required server specification to run validator node: No. of CPUs: 6 RAM: 32GB Card with Nvidia CUDA support(ex 1080ti) and at least 8gb VRAM. Disk: 256GB SSD","title":"Prepare your server"},{"location":"cyberd/run_validator/#install-dependencies","text":"Our main distribution unit is docker container. All images are located in default Dockerhub registry . Rank calculated on GPU using CUDA Toolkit . In order to access GPU from container, nvidia drivers version 410+ and nvidia docker runtime should be installed on host system. Check both driver and docker runtime installed correctly: docker run --runtime = nvidia --rm nvidia/cuda:10.0-base nvidia-smi # Should be displayed something like this. Tue Dec 11 18 :02:15 2018 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 410 .78 Driver Version: 410 .78 CUDA Version: 10 .0 | | -------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | =============================== + ====================== + ====================== | | 0 GeForce GTX 1050 Off | 00000000 :01:00.0 Off | N/A | | N/A 52C P0 N/A / N/A | 302MiB / 4042MiB | 2 % Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | | ============================================================================= | +-----------------------------------------------------------------------------+","title":"Install Dependencies"},{"location":"cyberd/run_validator/#start-fullnode","text":"First, create folder, where daemon and cli will store data. Some commands may require admin privileges. mkdir /cyberd Run daemon with mounted volumes on created during previous step folder. docker run -d --name = cyberd --runtime = nvidia \\ -p 26656 :26656 -p 26657 :26657 -p 26660 :26660 \\ -v /cyberd/daemon:/root/.cyberd \\ -v /cyberd/cli:/root/.cyberdcli \\ cyberd/cyberd:euler-dev0 To check if your node is connected to the testnet, you can run this: docker exec cyberd cyberdcli status You should be seeing a returned JSON with your node status including node_info, sync_info and validator_info.","title":"Start Fullnode"},{"location":"cyberd/run_validator/#prepare-stake-address","text":"Alright, you are now connected to the testnet. To be a validator, you will need some CBD (cyberd coin) to be bounded as your stake. Top 146 validators by bounded stake will be active validators taking part in consensus. If you already have address with CBD , just restore it with your seed phrase into your local keystore. docker exec -ti cyberd cyberdcli keys add <your_key_name> --recover docker exec cyberd cyberdcli show <your_key_name> If no, create new one using command below. Also, you should send coins to that address to bound them later during validator submitting. docker exec -ti cyberd cyberdcli add <your_key_name> docker exec cyberd cyberdcli show <your_key_name> is any name you pick to represent this key pair. You have to refer to this later when you use the keys to sign transactions. It will ask you to enter your password twice to encrypt the key. You also need to enter your password when you use your key to sign any transaction. The command returns the address, public key and a seed phrase which you can use it to recover your account if you forget your password later. Keep the seed phrase in a safe place in case you have to use them. The address showing here is your account address. Let\u2019s call this . It stores your assets.","title":"Prepare stake address"},{"location":"cyberd/run_validator/#send-create-validator-transaction","text":"Validators are actors on the network committing new blocks by submitting their votes. It refers to the node itself, not a single person or a single account. Therefore, the public key here is referring to the node public key, not the public key of the address you have just created. To get the node public key, run the following command. docker exec cyberd cyberd tendermint show_validator It will return a bech32 public key. Let\u2019s call it . The next step you have to declare a validator candidate. The validator candidate is the account which stake the coins. So the validator candidate is an account this time. To declare a validator candidate, run the following command adjusting stake amount and other fields. docker exec cyberd cyberdcli tx stake create-validator \\ --amount = 100CBD \\ --pubkey = <your_node_pubkey> \\ --moniker = <your_node_nickname> \\ --trust-node \\ --from = <your_key_name> \\ --commission-rate = \"0.10\" \\ --commission-max-rate = \"0.20\" \\ --commission-max-change-rate = \"0.01\"","title":"Send create validator transaction"},{"location":"cybernode/cybernode/","text":"Client for p2p node system","title":"About cybernode"},{"location":"cybernode/dev-setup/","text":"","title":"Dev-setup"},{"location":"cybernode/k8s-cheat-sheet/","text":"K8s cheat sheet \u00b6 k8s dashboard \u00b6 Local dashboard proxy: kubectl proxy Get cluster access token: kubectl config view | grep -A10 \"name: $( kubectl config current-context ) \" | awk '$1==\"access-token:\"{print $2}' Get logs of previously running container (if it failed and then restarts): kubectl logs mypod --previous Reset GKE Node \u00b6 gcloud compute instances reset gke-kubia-default-pool-b46381f1-zwko Forward pod port on localhost \u00b6 kubectl port-forward -n monitoring prometheus-kube-prometheus-0 9090 Forward port of first found pod with specific namespace and label: kubectl port-forward -n chains $( kubectl get pod -n chains -l chain = bitcoind-btc -o jsonpath = \"{.items[0].metadata.name}\" ) 8332 Elassandra commands \u00b6 Get nodes status: kubectl exec -n search elassandra-0 -- nodetool status Open cqlsh CLI tool kubectl exec -it -n search elassandra-0 -- cqlsh Dive into elassandra docker container shell(index stats, delete index commands examples) kubectl exec -n search -it elassandra-0 bash curl -XGET 'localhost:9200/_cat/indices?v&pretty' curl -XDELETE 'localhost:9200/twitter?pretty'","title":"Cheat-sheet"},{"location":"cybernode/k8s-cheat-sheet/#k8s-cheat-sheet","text":"","title":"K8s cheat sheet"},{"location":"cybernode/k8s-cheat-sheet/#k8s-dashboard","text":"Local dashboard proxy: kubectl proxy Get cluster access token: kubectl config view | grep -A10 \"name: $( kubectl config current-context ) \" | awk '$1==\"access-token:\"{print $2}' Get logs of previously running container (if it failed and then restarts): kubectl logs mypod --previous","title":"k8s dashboard"},{"location":"cybernode/k8s-cheat-sheet/#reset-gke-node","text":"gcloud compute instances reset gke-kubia-default-pool-b46381f1-zwko","title":"Reset GKE Node"},{"location":"cybernode/k8s-cheat-sheet/#forward-pod-port-on-localhost","text":"kubectl port-forward -n monitoring prometheus-kube-prometheus-0 9090 Forward port of first found pod with specific namespace and label: kubectl port-forward -n chains $( kubectl get pod -n chains -l chain = bitcoind-btc -o jsonpath = \"{.items[0].metadata.name}\" ) 8332","title":"Forward pod port on localhost"},{"location":"cybernode/k8s-cheat-sheet/#elassandra-commands","text":"Get nodes status: kubectl exec -n search elassandra-0 -- nodetool status Open cqlsh CLI tool kubectl exec -it -n search elassandra-0 -- cqlsh Dive into elassandra docker container shell(index stats, delete index commands examples) kubectl exec -n search -it elassandra-0 bash curl -XGET 'localhost:9200/_cat/indices?v&pretty' curl -XDELETE 'localhost:9200/twitter?pretty'","title":"Elassandra commands"},{"location":"cybernode/requirements/","text":"Requirements \u00b6 For near future(1.0 release) cybernode will act as a helping tool around docker images to instantiate p2p systems and app around them. For the first release, we consider to do next: 1 ) cybernode settings cybernode help 2 ) cybernode status 3 ) cybernode chains ethereum | ethereum_kovan | ethereum_classic start ( --light --autostartup ) cybernode chains ethereum stop cybernode chains bitcoin help cybernode chains bitcoin start ( --autostartup ) cybernode chains bitcoin stop cybernode chains status cybernide chains help 4 ) cybernode p2p ipfs start ( --autostartup ) cybernode p2p ipfs stop 5 ) cybernode apps | platforms cyb start cybernode apps | platforms cyb stop First run initialization \u00b6 For first run of cybernode, user should specify where to store data for applications: cybernode.data.location = ~/.cybernode/data By default, all cybernode configs|state will be stored in ~/.cybernode path. Nonchangeble. Research: - [] Path autocompletion? cybernode chains subcomands \u00b6 User shoud be able to run chains with specified options. Research: - [] UI for chains - [] Light clients for chains cybernode p2p subcomands \u00b6 User shoud be able to run p2p systems with specified options. Research: - [] Key pair managment if chain reuqired apps|platforms subcomands \u00b6 User should be able to run DApps. Each DApp also have dependencies on p2p or chains entities. This subcommand should start only missing entities. cybernode status \u00b6 Should display to user cybernode running|stopped entities. Also should be available for all subcommands. cybernode help \u00b6 Should display help message to the user. Also should be available for all subcommands. cybernode settings \u00b6 Should print current settings. Research: - [] Possible solutions for autocomplete for MacOS, Linux systems - [] Language and framework for CLI - [] Package distribution for various platforms(brew, npm, deb etc) - [] Initial Message??","title":"Requirements"},{"location":"cybernode/requirements/#requirements","text":"For near future(1.0 release) cybernode will act as a helping tool around docker images to instantiate p2p systems and app around them. For the first release, we consider to do next: 1 ) cybernode settings cybernode help 2 ) cybernode status 3 ) cybernode chains ethereum | ethereum_kovan | ethereum_classic start ( --light --autostartup ) cybernode chains ethereum stop cybernode chains bitcoin help cybernode chains bitcoin start ( --autostartup ) cybernode chains bitcoin stop cybernode chains status cybernide chains help 4 ) cybernode p2p ipfs start ( --autostartup ) cybernode p2p ipfs stop 5 ) cybernode apps | platforms cyb start cybernode apps | platforms cyb stop","title":"Requirements"},{"location":"cybernode/requirements/#first-run-initialization","text":"For first run of cybernode, user should specify where to store data for applications: cybernode.data.location = ~/.cybernode/data By default, all cybernode configs|state will be stored in ~/.cybernode path. Nonchangeble. Research: - [] Path autocompletion?","title":"First run initialization"},{"location":"cybernode/requirements/#cybernode-chains-subcomands","text":"User shoud be able to run chains with specified options. Research: - [] UI for chains - [] Light clients for chains","title":"cybernode chains subcomands"},{"location":"cybernode/requirements/#cybernode-p2p-subcomands","text":"User shoud be able to run p2p systems with specified options. Research: - [] Key pair managment if chain reuqired","title":"cybernode p2p subcomands"},{"location":"cybernode/requirements/#appsplatforms-subcomands","text":"User should be able to run DApps. Each DApp also have dependencies on p2p or chains entities. This subcommand should start only missing entities.","title":"apps|platforms subcomands"},{"location":"cybernode/requirements/#cybernode-status","text":"Should display to user cybernode running|stopped entities. Also should be available for all subcommands.","title":"cybernode status"},{"location":"cybernode/requirements/#cybernode-help","text":"Should display help message to the user. Also should be available for all subcommands.","title":"cybernode help"},{"location":"cybernode/requirements/#cybernode-settings","text":"Should print current settings. Research: - [] Possible solutions for autocomplete for MacOS, Linux systems - [] Language and framework for CLI - [] Package distribution for various platforms(brew, npm, deb etc) - [] Initial Message??","title":"cybernode settings"},{"location":"cybernode/chains/bitcoin/","text":"","title":"Bitcoin"},{"location":"cybernode/chains/ethereum/","text":"Known issues \u00b6 For 2806264 block there is a transaction with size more than 1mb, general rule, size of item can be >1mb.","title":"Ethereum"},{"location":"cybernode/chains/ethereum/#known-issues","text":"For 2806264 block there is a transaction with size more than 1mb, general rule, size of item can be >1mb.","title":"Known issues"},{"location":"cybernode/components/chain-nodes-components/","text":"Chain Nodes Components \u00b6 Component Cluster Address Parity 1.9.6 eth parity-eth.chains.svc:8545 Parity 1.9.6 etc parity-et\u0441.chains.svc:8545 Bitcoind 0.16.0 bitcoind-btc.chains.svc:8332","title":"Chain-nodes"},{"location":"cybernode/components/chain-nodes-components/#chain-nodes-components","text":"Component Cluster Address Parity 1.9.6 eth parity-eth.chains.svc:8545 Parity 1.9.6 etc parity-et\u0441.chains.svc:8545 Bitcoind 0.16.0 bitcoind-btc.chains.svc:8332","title":"Chain Nodes Components"},{"location":"cybernode/components/components-requirments/","text":"Components requirements \u00b6 Resources requirements to run components in gcloud cluster Component Min CPU Max CPU Min RAM Max RAM Kafka Broker 1.5 2 10GB 10GB Kafka Exporter 0.1 0.2 1GB 2GB Kafka Manager 0.1 0.2 1GB 2GB Zoo Keeper 0.1 0.2 1.25GB 1.25GB Elassandra 3.5 3.5 55GB 55GB Grafana 0.1 0.2 100MB 200MB Prometheus 0.25 1 3GB 3GB Prometheus Operator 0.1 0.2 100MB 200MB Parity ETH 3 3 15GB 20GB Parity ETC 1 1.5 10GB 15GB ETH Pump 1.5 3 5GB 6GB ETH Cassandra Dump 1.5 3 3.75GB 3.75GB ETH Contract Summary 1.5 3 3.75GB 3.75GB ETC Pump 1.5 3 3.75GB 3.75GB ETC Cassandra Dump 1.5 3 3.75GB 3.75GB ETC Contract Summary 1.5 3 3.75GB 3.75GB Bitcoind 2 3 20GB 30GB BTC Pump* 2.5 2.5 22GB 25GB BTC Dump 3 3 3.75GB 3.75GB BTC Contract Summary 1.5 3 3.75GB 3.75GB Search Api 0.5 1 3.75GB 3.75GB Search Api Docs 0.1 0.2 100MB 200MB * - With cache size = 10GB","title":"Requirements"},{"location":"cybernode/components/components-requirments/#components-requirements","text":"Resources requirements to run components in gcloud cluster Component Min CPU Max CPU Min RAM Max RAM Kafka Broker 1.5 2 10GB 10GB Kafka Exporter 0.1 0.2 1GB 2GB Kafka Manager 0.1 0.2 1GB 2GB Zoo Keeper 0.1 0.2 1.25GB 1.25GB Elassandra 3.5 3.5 55GB 55GB Grafana 0.1 0.2 100MB 200MB Prometheus 0.25 1 3GB 3GB Prometheus Operator 0.1 0.2 100MB 200MB Parity ETH 3 3 15GB 20GB Parity ETC 1 1.5 10GB 15GB ETH Pump 1.5 3 5GB 6GB ETH Cassandra Dump 1.5 3 3.75GB 3.75GB ETH Contract Summary 1.5 3 3.75GB 3.75GB ETC Pump 1.5 3 3.75GB 3.75GB ETC Cassandra Dump 1.5 3 3.75GB 3.75GB ETC Contract Summary 1.5 3 3.75GB 3.75GB Bitcoind 2 3 20GB 30GB BTC Pump* 2.5 2.5 22GB 25GB BTC Dump 3 3 3.75GB 3.75GB BTC Contract Summary 1.5 3 3.75GB 3.75GB Search Api 0.5 1 3.75GB 3.75GB Search Api Docs 0.1 0.2 100MB 200MB * - With cache size = 10GB","title":"Components requirements"},{"location":"cybernode/components/monitoring-components/","text":"Monitoring Components \u00b6 During lifetime cybernode collects various metrics using following components: Component Description Cluster Address External Prometheus Operator(PO) Manages Prometheus Configuration Prometheus Metrics Storage prometheus.monitoring.svc:9090 Default Service Monitor Default Service Monitor for PO Grafana Metrics Alerts and Web UI grafana.monitoring.svc:3000 y Prometheus \u00b6 Prometheus is used as a main metrics storage. Components collect metrics and expose them via HTTP endpoint, that Prometheus pulls at configured interval(our is 15s). Prometheus Operator \u00b6 To deploy Prometheus atop Kubernetes, we use Prometheus Operator . It consists of Prometheus Operator itself, that introduced to cluster CRDs : Prometheus, Service Monitor. PO automatically(dynamically) generate monitoring target configurations based on familiar Kubernetes label queries; no need to learn a Prometheus specific configuration language. Default Service Monitor \u00b6 To enable dynamic mapping for new pods to prometheus, service monitor(SM) is used. Each SM defines set of kubernetes label filters. Grafana \u00b6 To visualize metrics collected by Prometheus we use Grafana . In order to configure Grafana, three config maps are used: grafana-datasources . Define Prometheus datasource. grafana-dashboards . Gather predefined dashboards into single folder. grafana-dashboards-providers . Configuration for importing dashboards. Grafana Alerts \u00b6 Also Grafana supports checking metrics satisfied defined criteria. If criteria is violated, than notification is raised to setup alert channels(in our case Telegram Group). To view list of available alerts check \"{grafana_host}/alerting/list\" page.","title":"Monitoring"},{"location":"cybernode/components/monitoring-components/#monitoring-components","text":"During lifetime cybernode collects various metrics using following components: Component Description Cluster Address External Prometheus Operator(PO) Manages Prometheus Configuration Prometheus Metrics Storage prometheus.monitoring.svc:9090 Default Service Monitor Default Service Monitor for PO Grafana Metrics Alerts and Web UI grafana.monitoring.svc:3000 y","title":"Monitoring Components"},{"location":"cybernode/components/monitoring-components/#prometheus","text":"Prometheus is used as a main metrics storage. Components collect metrics and expose them via HTTP endpoint, that Prometheus pulls at configured interval(our is 15s).","title":"Prometheus"},{"location":"cybernode/components/monitoring-components/#prometheus-operator","text":"To deploy Prometheus atop Kubernetes, we use Prometheus Operator . It consists of Prometheus Operator itself, that introduced to cluster CRDs : Prometheus, Service Monitor. PO automatically(dynamically) generate monitoring target configurations based on familiar Kubernetes label queries; no need to learn a Prometheus specific configuration language.","title":"Prometheus Operator"},{"location":"cybernode/components/monitoring-components/#default-service-monitor","text":"To enable dynamic mapping for new pods to prometheus, service monitor(SM) is used. Each SM defines set of kubernetes label filters.","title":"Default Service Monitor"},{"location":"cybernode/components/monitoring-components/#grafana","text":"To visualize metrics collected by Prometheus we use Grafana . In order to configure Grafana, three config maps are used: grafana-datasources . Define Prometheus datasource. grafana-dashboards . Gather predefined dashboards into single folder. grafana-dashboards-providers . Configuration for importing dashboards.","title":"Grafana"},{"location":"cybernode/components/monitoring-components/#grafana-alerts","text":"Also Grafana supports checking metrics satisfied defined criteria. If criteria is violated, than notification is raised to setup alert channels(in our case Telegram Group). To view list of available alerts check \"{grafana_host}/alerting/list\" page.","title":"Grafana Alerts"},{"location":"cybernode/staging/chains/","text":"Current chains nodes tables \u00b6 App data path port current size parity /cyberdata/parity 34545 174 gb parity-kovan /cyberdata/parity-kovan 34645 30 gb Local forwarding port for chains \u00b6 ssh -L 8545:localhost:34545 -L 8546:localhost:34546 earth@earth.cybernode.ai -p 33324 Commands used to run chain and live probe \u00b6 Parity \u00b6 Run: docker run -d -p 34546:8546 -p 34545:8545 -v /cyberdata/parity:/cyberdata \\ -v /home/earth/.local/share/io.parity.ethereum/jsonrpc.ipc:/home/parity/.local/share/io.parity.ethereum/jsonrpc.ipc \\ --name parity --restart always --ipc=host parity/parity:stable \\ --db-compaction ssd --unsafe-expose --db-path /cyberdata --tracing=on --fat-db=on Probe: curl -X POST -H \"Content-Type: application/json\" \\ --data '{\"jsonrpc\":\"2.0\",\"method\":\"eth_getBlockByNumber\",\"params\":[\"0x1\", true],\"id\":1}'\\ http://127.0.0.1:34545 Parity --chain kovan \u00b6 Run: docker run -d -p 34646:8546 -p 34645:8545 -v /cyberdata/parity:/cyberdata \\ -v /home/earth/.local/share/io.parity.ethereum/jsonrpc.ipc:/home/parity/.local/share/io.parity.ethereum/jsonrpc.ipc \\ --name parity-kovan --restart always --ipc=host parity/parity:stable \\ --db-compaction ssd --unsafe-expose --db-path /cyberdata --tracing=on --fat-db=on --chain kovan Probe: curl -X POST -H \"Content-Type: application/json\" \\ --data '{\"jsonrpc\":\"2.0\",\"method\":\"eth_getBlockByNumber\",\"params\":[\"0x1\", true],\"id\":1}' \\ http://127.0.0.1:34645","title":"Chains"},{"location":"cybernode/staging/chains/#current-chains-nodes-tables","text":"App data path port current size parity /cyberdata/parity 34545 174 gb parity-kovan /cyberdata/parity-kovan 34645 30 gb","title":"Current chains nodes tables"},{"location":"cybernode/staging/chains/#local-forwarding-port-for-chains","text":"ssh -L 8545:localhost:34545 -L 8546:localhost:34546 earth@earth.cybernode.ai -p 33324","title":"Local forwarding port for chains"},{"location":"cybernode/staging/chains/#commands-used-to-run-chain-and-live-probe","text":"","title":"Commands used to run chain and live probe"},{"location":"cybernode/staging/chains/#parity","text":"Run: docker run -d -p 34546:8546 -p 34545:8545 -v /cyberdata/parity:/cyberdata \\ -v /home/earth/.local/share/io.parity.ethereum/jsonrpc.ipc:/home/parity/.local/share/io.parity.ethereum/jsonrpc.ipc \\ --name parity --restart always --ipc=host parity/parity:stable \\ --db-compaction ssd --unsafe-expose --db-path /cyberdata --tracing=on --fat-db=on Probe: curl -X POST -H \"Content-Type: application/json\" \\ --data '{\"jsonrpc\":\"2.0\",\"method\":\"eth_getBlockByNumber\",\"params\":[\"0x1\", true],\"id\":1}'\\ http://127.0.0.1:34545","title":"Parity"},{"location":"cybernode/staging/chains/#parity-chain-kovan","text":"Run: docker run -d -p 34646:8546 -p 34645:8545 -v /cyberdata/parity:/cyberdata \\ -v /home/earth/.local/share/io.parity.ethereum/jsonrpc.ipc:/home/parity/.local/share/io.parity.ethereum/jsonrpc.ipc \\ --name parity-kovan --restart always --ipc=host parity/parity:stable \\ --db-compaction ssd --unsafe-expose --db-path /cyberdata --tracing=on --fat-db=on --chain kovan Probe: curl -X POST -H \"Content-Type: application/json\" \\ --data '{\"jsonrpc\":\"2.0\",\"method\":\"eth_getBlockByNumber\",\"params\":[\"0x1\", true],\"id\":1}' \\ http://127.0.0.1:34645","title":"Parity --chain kovan"},{"location":"cybernode/staging/continuous-delivery/","text":"Circle Ci job example \u00b6 To enable auto staging-redeploy for your application you should: Enable staging docker images build. For example, you can use dockerhub tag \"STAGING\". Add to the repo you image running script. Add a CircleCi job, depended on docker build&push job. It's all. Example: build and deploy for all commits in master \u00b6 aliases: - &staging_filter filters: tags: only: /.*/ branches: only: master jobs: build_and_push_image_staging_image: working_directory: ~/build docker: - image: docker:17.05.0-ce-git steps: - checkout - setup_remote_docker: version: 17.05.0-ce - run: name: Build and upload staging images command: | docker build -t build/cs-search-api -f ./devops/search-api/search-api ./ docker login -u $DOCKER_USER -p $DOCKER_PASS docker tag build/cs-search-api cybernode/cs-search-api:staging docker push cybernode/cs-search-api:staging deploy_stagin_image: working_directory: ~/build docker: - image: docker:17.05.0-ce-git steps: - run: name: Rerun image on staging command: >- ssh mars@staging.cyber.fund -p 33322 -o \"StrictHostKeyChecking no\" 'cd /cyberdata/cybernode && git pull && sh /cyberdata/cybernode/up.search.sh' workflows: version: 2 staging_cd: jobs: - build_and_push_image_staging_image: <<: *staging_filter - deploy_stagin_image: <<: *staging_filter requires: - build_and_push_image_staging_image","title":"CD"},{"location":"cybernode/staging/continuous-delivery/#circle-ci-job-example","text":"To enable auto staging-redeploy for your application you should: Enable staging docker images build. For example, you can use dockerhub tag \"STAGING\". Add to the repo you image running script. Add a CircleCi job, depended on docker build&push job. It's all.","title":"Circle Ci job example"},{"location":"cybernode/staging/continuous-delivery/#example-build-and-deploy-for-all-commits-in-master","text":"aliases: - &staging_filter filters: tags: only: /.*/ branches: only: master jobs: build_and_push_image_staging_image: working_directory: ~/build docker: - image: docker:17.05.0-ce-git steps: - checkout - setup_remote_docker: version: 17.05.0-ce - run: name: Build and upload staging images command: | docker build -t build/cs-search-api -f ./devops/search-api/search-api ./ docker login -u $DOCKER_USER -p $DOCKER_PASS docker tag build/cs-search-api cybernode/cs-search-api:staging docker push cybernode/cs-search-api:staging deploy_stagin_image: working_directory: ~/build docker: - image: docker:17.05.0-ce-git steps: - run: name: Rerun image on staging command: >- ssh mars@staging.cyber.fund -p 33322 -o \"StrictHostKeyChecking no\" 'cd /cyberdata/cybernode && git pull && sh /cyberdata/cybernode/up.search.sh' workflows: version: 2 staging_cd: jobs: - build_and_push_image_staging_image: <<: *staging_filter - deploy_stagin_image: <<: *staging_filter requires: - build_and_push_image_staging_image","title":"Example: build and deploy for all commits in master"},{"location":"cybernode/staging/kubernetes/","text":"Prerequisites \u00b6 Install kubectl for interacting with Kubernetes. Install minikube if you want to test setup locally. Make sure to install driver for your OS or else slow VirtualBox will be used. Running minikube for local testing \u00b6 $ minikube start --vm-driver kvm2 Starting local Kubernetes v1.9.0 cluster... Starting VM... Getting VM IP address... Moving files into cluster... Setting up certs... Connecting to cluster... Setting up kubeconfig... Starting cluster components... Kubectl is now configured to use the cluster. Loading cached images from config file. Check status. $ minikube status minikube: Running cluster: Running kubectl: Correctly Configured: pointing to minikube-vm at 192.168.39.6 Run dashboard. $ minikube dashboard Opening kubernetes dashboard in default browser... Run single container in a cluster \u00b6 $ kubectl run miniecho --image=k8s.gcr.io/echoserver:1.9 --port=8080 This adds deployment called miniecho to cluster, runs Docker echoserver image in it and tells docker to open port 8080. The port is not accessible until we create service . $ kubectl expose deployment miniecho --type=NodePort service \"miniecho\" exposed This makes miniecho service accessible from your machine on random port. $ minikube service list |-------------|----------------------|---------------------------| | NAMESPACE | NAME | URL | |-------------|----------------------|---------------------------| | default | kubernetes | No node port | | default | miniecho | http://192.168.39.6:30426 | | kube-system | kube-dns | No node port | | kube-system | kubernetes-dashboard | http://192.168.39.6:30000 | |-------------|----------------------|---------------------------|","title":"Kubernetes"},{"location":"cybernode/staging/kubernetes/#prerequisites","text":"Install kubectl for interacting with Kubernetes. Install minikube if you want to test setup locally. Make sure to install driver for your OS or else slow VirtualBox will be used.","title":"Prerequisites"},{"location":"cybernode/staging/kubernetes/#running-minikube-for-local-testing","text":"$ minikube start --vm-driver kvm2 Starting local Kubernetes v1.9.0 cluster... Starting VM... Getting VM IP address... Moving files into cluster... Setting up certs... Connecting to cluster... Setting up kubeconfig... Starting cluster components... Kubectl is now configured to use the cluster. Loading cached images from config file. Check status. $ minikube status minikube: Running cluster: Running kubectl: Correctly Configured: pointing to minikube-vm at 192.168.39.6 Run dashboard. $ minikube dashboard Opening kubernetes dashboard in default browser...","title":"Running minikube for local testing"},{"location":"cybernode/staging/kubernetes/#run-single-container-in-a-cluster","text":"$ kubectl run miniecho --image=k8s.gcr.io/echoserver:1.9 --port=8080 This adds deployment called miniecho to cluster, runs Docker echoserver image in it and tells docker to open port 8080. The port is not accessible until we create service . $ kubectl expose deployment miniecho --type=NodePort service \"miniecho\" exposed This makes miniecho service accessible from your machine on random port. $ minikube service list |-------------|----------------------|---------------------------| | NAMESPACE | NAME | URL | |-------------|----------------------|---------------------------| | default | kubernetes | No node port | | default | miniecho | http://192.168.39.6:30426 | | kube-system | kube-dns | No node port | | kube-system | kubernetes-dashboard | http://192.168.39.6:30000 | |-------------|----------------------|---------------------------|","title":"Run single container in a cluster"},{"location":"cybernode/staging/run/","text":"Port mapping \u00b6 Ports that are not accessible from internet. Use ssh forwarding. 8332 - chain btc 18332 - chain bth 8545 - chain eth 18545 - chain etc 2181 - zookeper 9092 - kafka 9042 - elassandra search 9043 - elassandra markets 9200 - elastic rest search 9201 - elastic rest markets 9300 - elastic transport search 9301 - elastic transport markets Ports open for internet. 32001 - portainer 32002 - grafana (monitoring) 32500 - browser-ui 32600 - chaingear 1.0 api 32800 - markets rest api 32801 - markets stream api 32901 - search api Docker data \u00b6 Run chains \u00b6 Chains should be run manually once and ideally should not require addition work. To run chains, execute: sudo bash /cyberdata/cybernode/chains/up.sh Run portainer.io \u00b6 Run once docker run -d -p 32001:9000 --name portainer --restart always \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /cyberdata/portainer:/data portainer/portainer Run components \u00b6 sudo bash /cyberdata/cybernode/up.browser.sh sudo bash /cyberdata/cybernode/up.chaingear.sh sudo bash /cyberdata/cybernode/up.ethereum.index.sh sudo bash /cyberdata/cybernode/up.search.api.sh Then go into each kafka , elassandra and run docker-compose up -d To access monitoring dashboard, run docker-compose up -d in monitoring as well.","title":"Run"},{"location":"cybernode/staging/run/#port-mapping","text":"Ports that are not accessible from internet. Use ssh forwarding. 8332 - chain btc 18332 - chain bth 8545 - chain eth 18545 - chain etc 2181 - zookeper 9092 - kafka 9042 - elassandra search 9043 - elassandra markets 9200 - elastic rest search 9201 - elastic rest markets 9300 - elastic transport search 9301 - elastic transport markets Ports open for internet. 32001 - portainer 32002 - grafana (monitoring) 32500 - browser-ui 32600 - chaingear 1.0 api 32800 - markets rest api 32801 - markets stream api 32901 - search api","title":"Port mapping"},{"location":"cybernode/staging/run/#docker-data","text":"","title":"Docker data"},{"location":"cybernode/staging/run/#run-chains","text":"Chains should be run manually once and ideally should not require addition work. To run chains, execute: sudo bash /cyberdata/cybernode/chains/up.sh","title":"Run chains"},{"location":"cybernode/staging/run/#run-portainerio","text":"Run once docker run -d -p 32001:9000 --name portainer --restart always \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /cyberdata/portainer:/data portainer/portainer","title":"Run portainer.io"},{"location":"cybernode/staging/run/#run-components","text":"sudo bash /cyberdata/cybernode/up.browser.sh sudo bash /cyberdata/cybernode/up.chaingear.sh sudo bash /cyberdata/cybernode/up.ethereum.index.sh sudo bash /cyberdata/cybernode/up.search.api.sh Then go into each kafka , elassandra and run docker-compose up -d To access monitoring dashboard, run docker-compose up -d in monitoring as well.","title":"Run components"},{"location":"cybernode/staging/setup/","text":"Mars is our staging server. You may reuse its config for your own. Current Mars storage setup \u00b6 Name Type Size LABEL Mapping nvme0n1p1 ssd 1 tb SDD1 /cyberdata/ssd1tb nvme2n1p1 ssd 500 gb SDD2 /cyberdata/ssd05tb sdc1 hdd 3.5 tb HDD3 /cyberdata/elassandra-markets sdb1 hdd 3.5 tb HDD2 /cyberdata/elassandra-search sda1 hdd 3.5 tb HDD1 /backupsData sdd1 hdd 3.5 tb cyberdata /cyberdata /cyberdata contents \u00b6 /cyberdata/portainer/ - portainer data directory /cyberdata/cybernode/ - clone of repo https://github.com/cyberFund/cybernode Useful commands \u00b6 Format partition: sudo mkfs -t ext4 -L SSD1 /dev/nvme0n1p1 sudo mkfs -t ext4 -L SSD2 /dev/nvme2n1p1 /etc/fstab addition lines: LABEL=cyberdata /cyberdata ext4 defaults 0 0 LABEL=SSD2 /cyberdata/ssd05tb ext4 nofail 0 0 LABEL=SSD1 /cyberdata/ssd1tb ext4 nofail 0 0 LABEL=HDD1 /backupsData ext4 defaults 0 0 LABEL=HDD2 /cyberdata/elassandra-search ext4 defaults 0 0 LABEL=HDD3 /cyberdata/elassandra-markets ext4 defaults 0 0 Copy data: sudo cp -R --verbose /backupsData/chain/eth/eth /cyberdata/ssd05tb/eth/ sudo cp -R --verbose /cyberdata/bitcoind /cyberdata/ssd05tb/bitcoind/","title":"Setup"},{"location":"cybernode/staging/setup/#current-mars-storage-setup","text":"Name Type Size LABEL Mapping nvme0n1p1 ssd 1 tb SDD1 /cyberdata/ssd1tb nvme2n1p1 ssd 500 gb SDD2 /cyberdata/ssd05tb sdc1 hdd 3.5 tb HDD3 /cyberdata/elassandra-markets sdb1 hdd 3.5 tb HDD2 /cyberdata/elassandra-search sda1 hdd 3.5 tb HDD1 /backupsData sdd1 hdd 3.5 tb cyberdata /cyberdata","title":"Current Mars storage setup"},{"location":"cybernode/staging/setup/#cyberdata-contents","text":"/cyberdata/portainer/ - portainer data directory /cyberdata/cybernode/ - clone of repo https://github.com/cyberFund/cybernode","title":"/cyberdata contents"},{"location":"cybernode/staging/setup/#useful-commands","text":"Format partition: sudo mkfs -t ext4 -L SSD1 /dev/nvme0n1p1 sudo mkfs -t ext4 -L SSD2 /dev/nvme2n1p1 /etc/fstab addition lines: LABEL=cyberdata /cyberdata ext4 defaults 0 0 LABEL=SSD2 /cyberdata/ssd05tb ext4 nofail 0 0 LABEL=SSD1 /cyberdata/ssd1tb ext4 nofail 0 0 LABEL=HDD1 /backupsData ext4 defaults 0 0 LABEL=HDD2 /cyberdata/elassandra-search ext4 defaults 0 0 LABEL=HDD3 /cyberdata/elassandra-markets ext4 defaults 0 0 Copy data: sudo cp -R --verbose /backupsData/chain/eth/eth /cyberdata/ssd05tb/eth/ sudo cp -R --verbose /cyberdata/bitcoind /cyberdata/ssd05tb/bitcoind/","title":"Useful commands"},{"location":"\u0441haingear/Changelog/","text":"Changelog \u00b6 All major changes of chaingear will be documented in this file. Sprint 9: \u00b6 1. Started frontend migration to new architecture (spring 9 - initialial relealse contracts's epic) 2. Changed architecture: - Splitting Registry creation in two stages - creation (in Chaingear) and post-initialization with EntryCore's bytecode (in Registry via admin/token holder) - Splitting Entry creation in two states - creation/deletion in Registry (with token minting/burning) and updating/reading in EntryCore 3. Improved policies: - Improved policies for Chaingear - Improved policies for Registry 3. Added advanced tests: - Improved and added more tests to Chaingear (Registry Creation, Chaingear Settings, policies) - Improved and added more tests to Registry (EntryCore initialization, CRUD, policies, Registry settings) 4. Initial gas estimation: - First meaningfull estimates of gas consumption for Chaingear creation, Registry creation, Registry initialization (with user generated EntryCore bytecode), Entry adding 5. Refactoring: - Policies refactoring 5. Improved documentation: - Improved NatSpec 6. Iceboxing after Research and Refactoring: - Funding of Chaingear registries - Funding of Registry entries Sprint 7-8: \u00b6 1. Initial frontend with Web3 and Metamask/Truffle integration 2. Created initial version of contract of Chaingear/Registry 3. Initial research/integration of ERC721 to tokenize registries/entries in Chaingear/Registry 4. Initial realization of policies for Chaingear/Registry 5. Initial research/realization of cross-linking Registries tokens in Chaingear with side Registries smart-contracts","title":"Changelog"},{"location":"\u0441haingear/Changelog/#changelog","text":"All major changes of chaingear will be documented in this file.","title":"Changelog"},{"location":"\u0441haingear/Changelog/#sprint-9","text":"1. Started frontend migration to new architecture (spring 9 - initialial relealse contracts's epic) 2. Changed architecture: - Splitting Registry creation in two stages - creation (in Chaingear) and post-initialization with EntryCore's bytecode (in Registry via admin/token holder) - Splitting Entry creation in two states - creation/deletion in Registry (with token minting/burning) and updating/reading in EntryCore 3. Improved policies: - Improved policies for Chaingear - Improved policies for Registry 3. Added advanced tests: - Improved and added more tests to Chaingear (Registry Creation, Chaingear Settings, policies) - Improved and added more tests to Registry (EntryCore initialization, CRUD, policies, Registry settings) 4. Initial gas estimation: - First meaningfull estimates of gas consumption for Chaingear creation, Registry creation, Registry initialization (with user generated EntryCore bytecode), Entry adding 5. Refactoring: - Policies refactoring 5. Improved documentation: - Improved NatSpec 6. Iceboxing after Research and Refactoring: - Funding of Chaingear registries - Funding of Registry entries","title":"Sprint 9:"},{"location":"\u0441haingear/Changelog/#sprint-7-8","text":"1. Initial frontend with Web3 and Metamask/Truffle integration 2. Created initial version of contract of Chaingear/Registry 3. Initial research/integration of ERC721 to tokenize registries/entries in Chaingear/Registry 4. Initial realization of policies for Chaingear/Registry 5. Initial research/realization of cross-linking Registries tokens in Chaingear with side Registries smart-contracts","title":"Sprint 7-8:"},{"location":"\u0441haingear/contracts/","text":"Contracts Overview \u00b6 Design rationale \u00b6 Main design principle goes from ERC721 NFT tokenization of Registries (in Chaingear/Metaregistry) and Entries (in custom Registry). In reason of Registries tokenization in Chaingear which allows token holder acts as administrator of their Registry, Chaingear acts to Registry as owner, which sets holder as administrator on creation phase, changes administrator when holder transfers token to another user, and transfers ownership when user unregister Registry in Chaingear, giving them full control to contract. Registry deep-linked to Chaingear registry token. In other words, token ownership means control of Registry. Also, Chaingear supports multiple Registry Creators (fabrics of registries), and allows Chaingear owners provide different kind and versioning of Registries. In reason of providing user functionality to describe their custom registry data structures and CRUD operations, the user creates their custom smart-contract, which implements the EntryInterface interface. This contract acts as inner storage, defines schema, and Registry acts them on token operations (creating and deleting). A user may deploy erroneous or vulnerable EntryCore contract, but this should not crash Chaingear-Registry NFT-token logic and Registry inner entry NFT-logic too. Even if this happens Registry crashing should not affect Chaingear/metaregistry contract. We proceed from the premise that the creator of the registry (administrator) is positive and it does not make sense for them to break his registry by initializing it with an incorrect contract. This brings us to tokenized ( C RU D ) operations (and inner private in EntryCore) in Registry-EntryCore and public ( C R UD ) actions/direct EntroCore tokenized ( CR U D ) operations. Chaingear inheritance \u00b6 Registry inheritance \u00b6 PS: OZ stands to Open Zeppelin contracts \u00b6 /chaingear \u00b6 Chaingear allows any user to create his own registry. Building fee is collecting by new registry creation. All builded registries are tokenized with ERC721 NFT token standard and saved in Chaingear metaregistry with registry metainformation. Creator of registry may transfer tokenized ownership of registry and destroy registry with token burning. Tokenized registries Entries may collect funds by users. Chaingear supports multiple benefitiaries witch have access to collected fees. ChaingearCore holds general logic of Chaingear. Allows change chaingear's metainformation, amount of registration fee, adding multiple registry creator contracts based on versioning or/and functionality. RegistryBase holds struct of data with describes registry metainformation which associated with token, views function for registry metainformation. RegistryCreator contains the code of specified version of Registry. This code used by Chaingear for Registry creation process. Registry Creator should be added with specified version and description to Chaingear Registry Creators inner registry. Chaingear contract should be added as builder to Registry Creator with reason to allow creation calls only by Chaingear contact. /common \u00b6 RegistySafe allows creator contract transfer ETHs to them and claim from, accounting logic holded by owner contract. SplitPaymentChangeable allows add beneficiaries to contract (addresses and shares amount) and change payee address. Beneficiaries can claim ETHs from contract proportionally to their shares. /registry \u00b6 Chaingeareable holds basic logic of Registry as registry basic information, balance and fees amount. Contains getters and setters for registry name, description, tags, entry base address. EntryInterface interface for EntryCore . Holds entry metainformation and interfaces of functions ( C RU D ) which should be implemented in EntryCore . Uses for interaction between Registry and EntryCore. EntryCore partially code-generated contract where registry creator setup their custom entry structure and setters/getters. EntryCore then initializes in Registry by their creator (as admin) and completes Registry setup process. Provides public ( C RU D ) actions for users and inner ( C RU D ) tokenized actions for Registry. Registry goes as owner of contract (and acts as proxy) with entries creating, token-based transferring and deleting. Registry contract witch tokenize entries as NFT tokens via ERC721 standard. Users can create tokenized empty entries according to entry access policy setted in Registry. Registry provides tokenized ( C RU D ) actions, after creation of token and empty registry object, user should initialize them in EntryCore. Also users can fund entries with ETHs which send to RegistrySafe where owner of entry token can claim funds. RegistryAccessControl holds logic of controlling registry and accessing to entries creation. Policy options to entries creation are OnlyAdministrator, AllUsers. Chaingear acts as owner of Registry and creator of registry acts of administrator with separated policies to Registry functions. EntryInterface interface (should be implemented in user EntryCore contract) \u00b6 contract EntryInterface { function entriesAmount () external view returns ( uint256 ); function createEntry () external returns ( uint256 ); function deleteEntry ( uint256 ) external ; } Example EntryCore (with example custom structure and required functions) \u00b6 pragma solidity 0.4.24 ; import \"../common/EntryInterface.sol\" ; import \"openzeppelin-solidity/contracts/ownership/Ownable.sol\" ; //This is Example of EntryCore contract EntryCore is EntryInterface , Ownable { struct Entry { address expensiveAddress ; uint256 expensiveUint ; int128 expensiveInt ; string expensiveString ; } mapping ( string => bool ) internal entryExpensiveStringIndex ; Entry [] internal entries ; function () external {} function createEntry () external onlyOwner returns ( uint256 ) { Entry memory m = ( Entry ( { expensiveAddress : address ( 0 ), expensiveUint : uint256 ( 0 ), expensiveInt : int128 ( 0 ), expensiveString : \"\" })); uint256 newEntryID = entries . push ( m ) - 1 ; return newEntryID ; } function updateEntry ( uint256 _entryID , address _newAddress , uint256 _newUint , int128 _newInt , string _newString ) external { require ( owner . call ( bytes4 ( keccak256 ( \"checkAuth(uint256, address)\" )), _entryID , msg . sender )); // for uniq check example require ( entryExpensiveStringIndex [ _newString ] == false ); Entry memory m = ( Entry ({ expensiveAddress : _newAddress , expensiveUint : _newUint , expensiveInt : _newInt , expensiveString : _newString })); entries [ _entryID ] = m ; // for uniq check example entryExpensiveStringIndex [ _newString ] = true ; require ( owner . call ( bytes4 ( keccak256 ( \"updateEntryTimestamp(uint256)\" )), _entryID )); } function deleteEntry ( uint256 _entryIndex ) external onlyOwner { uint256 lastEntryIndex = entries . length - 1 ; Entry storage lastEntry = entries [ lastEntryIndex ]; entries [ _entryIndex ] = lastEntry ; delete entries [ lastEntryIndex ]; entries . length -- ; } function entriesAmount () external view returns ( uint256 entryID ) { return entries . length ; } function entryInfo ( uint256 _entryID ) external view returns ( address , uint256 , int128 , string ) { return ( entries [ _entryID ]. expensiveAddress , entries [ _entryID ]. expensiveUint , entries [ _entryID ]. expensiveInt , entries [ _entryID ]. expensiveString ); } }","title":"Contracts"},{"location":"\u0441haingear/contracts/#contracts-overview","text":"","title":"Contracts Overview"},{"location":"\u0441haingear/contracts/#design-rationale","text":"Main design principle goes from ERC721 NFT tokenization of Registries (in Chaingear/Metaregistry) and Entries (in custom Registry). In reason of Registries tokenization in Chaingear which allows token holder acts as administrator of their Registry, Chaingear acts to Registry as owner, which sets holder as administrator on creation phase, changes administrator when holder transfers token to another user, and transfers ownership when user unregister Registry in Chaingear, giving them full control to contract. Registry deep-linked to Chaingear registry token. In other words, token ownership means control of Registry. Also, Chaingear supports multiple Registry Creators (fabrics of registries), and allows Chaingear owners provide different kind and versioning of Registries. In reason of providing user functionality to describe their custom registry data structures and CRUD operations, the user creates their custom smart-contract, which implements the EntryInterface interface. This contract acts as inner storage, defines schema, and Registry acts them on token operations (creating and deleting). A user may deploy erroneous or vulnerable EntryCore contract, but this should not crash Chaingear-Registry NFT-token logic and Registry inner entry NFT-logic too. Even if this happens Registry crashing should not affect Chaingear/metaregistry contract. We proceed from the premise that the creator of the registry (administrator) is positive and it does not make sense for them to break his registry by initializing it with an incorrect contract. This brings us to tokenized ( C RU D ) operations (and inner private in EntryCore) in Registry-EntryCore and public ( C R UD ) actions/direct EntroCore tokenized ( CR U D ) operations.","title":"Design rationale"},{"location":"\u0441haingear/contracts/#chaingear-inheritance","text":"","title":"Chaingear inheritance"},{"location":"\u0441haingear/contracts/#registry-inheritance","text":"","title":"Registry inheritance"},{"location":"\u0441haingear/contracts/#ps-oz-stands-to-open-zeppelin-contracts","text":"","title":"PS: OZ stands to Open Zeppelin contracts"},{"location":"\u0441haingear/contracts/#chaingear","text":"Chaingear allows any user to create his own registry. Building fee is collecting by new registry creation. All builded registries are tokenized with ERC721 NFT token standard and saved in Chaingear metaregistry with registry metainformation. Creator of registry may transfer tokenized ownership of registry and destroy registry with token burning. Tokenized registries Entries may collect funds by users. Chaingear supports multiple benefitiaries witch have access to collected fees. ChaingearCore holds general logic of Chaingear. Allows change chaingear's metainformation, amount of registration fee, adding multiple registry creator contracts based on versioning or/and functionality. RegistryBase holds struct of data with describes registry metainformation which associated with token, views function for registry metainformation. RegistryCreator contains the code of specified version of Registry. This code used by Chaingear for Registry creation process. Registry Creator should be added with specified version and description to Chaingear Registry Creators inner registry. Chaingear contract should be added as builder to Registry Creator with reason to allow creation calls only by Chaingear contact.","title":"/chaingear"},{"location":"\u0441haingear/contracts/#common","text":"RegistySafe allows creator contract transfer ETHs to them and claim from, accounting logic holded by owner contract. SplitPaymentChangeable allows add beneficiaries to contract (addresses and shares amount) and change payee address. Beneficiaries can claim ETHs from contract proportionally to their shares.","title":"/common"},{"location":"\u0441haingear/contracts/#registry","text":"Chaingeareable holds basic logic of Registry as registry basic information, balance and fees amount. Contains getters and setters for registry name, description, tags, entry base address. EntryInterface interface for EntryCore . Holds entry metainformation and interfaces of functions ( C RU D ) which should be implemented in EntryCore . Uses for interaction between Registry and EntryCore. EntryCore partially code-generated contract where registry creator setup their custom entry structure and setters/getters. EntryCore then initializes in Registry by their creator (as admin) and completes Registry setup process. Provides public ( C RU D ) actions for users and inner ( C RU D ) tokenized actions for Registry. Registry goes as owner of contract (and acts as proxy) with entries creating, token-based transferring and deleting. Registry contract witch tokenize entries as NFT tokens via ERC721 standard. Users can create tokenized empty entries according to entry access policy setted in Registry. Registry provides tokenized ( C RU D ) actions, after creation of token and empty registry object, user should initialize them in EntryCore. Also users can fund entries with ETHs which send to RegistrySafe where owner of entry token can claim funds. RegistryAccessControl holds logic of controlling registry and accessing to entries creation. Policy options to entries creation are OnlyAdministrator, AllUsers. Chaingear acts as owner of Registry and creator of registry acts of administrator with separated policies to Registry functions.","title":"/registry"},{"location":"\u0441haingear/contracts/#entryinterface-interface-should-be-implemented-in-user-entrycore-contract","text":"contract EntryInterface { function entriesAmount () external view returns ( uint256 ); function createEntry () external returns ( uint256 ); function deleteEntry ( uint256 ) external ; }","title":"EntryInterface interface (should be implemented in user EntryCore contract)"},{"location":"\u0441haingear/contracts/#example-entrycore-with-example-custom-structure-and-required-functions","text":"pragma solidity 0.4.24 ; import \"../common/EntryInterface.sol\" ; import \"openzeppelin-solidity/contracts/ownership/Ownable.sol\" ; //This is Example of EntryCore contract EntryCore is EntryInterface , Ownable { struct Entry { address expensiveAddress ; uint256 expensiveUint ; int128 expensiveInt ; string expensiveString ; } mapping ( string => bool ) internal entryExpensiveStringIndex ; Entry [] internal entries ; function () external {} function createEntry () external onlyOwner returns ( uint256 ) { Entry memory m = ( Entry ( { expensiveAddress : address ( 0 ), expensiveUint : uint256 ( 0 ), expensiveInt : int128 ( 0 ), expensiveString : \"\" })); uint256 newEntryID = entries . push ( m ) - 1 ; return newEntryID ; } function updateEntry ( uint256 _entryID , address _newAddress , uint256 _newUint , int128 _newInt , string _newString ) external { require ( owner . call ( bytes4 ( keccak256 ( \"checkAuth(uint256, address)\" )), _entryID , msg . sender )); // for uniq check example require ( entryExpensiveStringIndex [ _newString ] == false ); Entry memory m = ( Entry ({ expensiveAddress : _newAddress , expensiveUint : _newUint , expensiveInt : _newInt , expensiveString : _newString })); entries [ _entryID ] = m ; // for uniq check example entryExpensiveStringIndex [ _newString ] = true ; require ( owner . call ( bytes4 ( keccak256 ( \"updateEntryTimestamp(uint256)\" )), _entryID )); } function deleteEntry ( uint256 _entryIndex ) external onlyOwner { uint256 lastEntryIndex = entries . length - 1 ; Entry storage lastEntry = entries [ lastEntryIndex ]; entries [ _entryIndex ] = lastEntry ; delete entries [ lastEntryIndex ]; entries . length -- ; } function entriesAmount () external view returns ( uint256 entryID ) { return entries . length ; } function entryInfo ( uint256 _entryID ) external view returns ( address , uint256 , int128 , string ) { return ( entries [ _entryID ]. expensiveAddress , entries [ _entryID ]. expensiveUint , entries [ _entryID ]. expensiveInt , entries [ _entryID ]. expensiveString ); } }","title":"Example EntryCore (with example custom structure and required functions)"},{"location":"\u0441haingear/development/","text":"Configuring, development and deploying \u00b6 Development environment \u00b6 Recommending to use Remix Ethereum Online IDE or desktop electron-based Remix IDE PS: to import to IDE open-zeppelin contacts change imports this way: import \"github.com/openZeppelin/zeppelin-solidity/contracts/ownership/Ownable.sol\" ; Truffle + Ganache workflow \u00b6 Install Ganache (with UI) from latest release or npm package => npm install -g ganache-cli Configure development config in truffle.js and launch Ganache (configure them too if needed) and: ganache-cli -p 7545 ( in first tab/or run UI-client ) truffle migrate --network development --reset ( in second tab ) truffle console --network development ( in second tab ) Creating new registry/CRU entries (truffle console way/no Remix IDE) \u00b6 var chaingear = Chaingear.at ( Chaingear.address ) var beneficiaries = [] var shares = [] var buildingFee = 100000 var gas = 10000000 chaingear.registerRegistry ( \"V1\" , beneficiaries, shares, \"BlockchainRegistry\" , \"BLR\" , { value: 100000 , gas: 10000000 }) var registryAddress = chaingear.registryInfo.call ( 0 ) var registry = Registry.at ( 'insert_registry_address_here' ) registry.initializeRegistry ( \"IPFS_HASH\" , EntryCore.bytecode ) registry.createEntry () var entryCoreAddress = registry.getEntriesStorage () var entryCore = EntryCore.at ( 'insert_entry_core_address_here' ) entryCore.updateEntry ( 0 , '0xa2f0dde51cb715f9cc7c12763fef90270bd50f70' , 256 , -127, \"helloworld\" ) entryCore.entryInfo ( 0 ) // --->>> [ '0xa2f0dde51cb715f9cc7c12763fef90270bd50f70' , BigNumber { s: 1 , e: 2 , c: [ 256 ] } , BigNumber { s: -1, e: 2 , c: [ 127 ] } , 'helloworld' ] Linting: \u00b6 npm install -g solium solium -d contracts Testing: \u00b6 truffle test PS: script will run separate tests flow for each file, also produce gas report. Temp solution, for while we don't fix problem with tests falls when running for all files at one time what in truffle by default. Deploying (for example kovan): \u00b6 parity ui --chain = kovan truffle migrate --network = kovan PS: approve transaction in parity ui (http://127.0.0.1:8180/) (Optional) Build contract in file: \u00b6 truffle-flattener contracts/registry/Registry.sol > Registry_full.sol","title":"Development"},{"location":"\u0441haingear/development/#configuring-development-and-deploying","text":"","title":"Configuring, development and deploying"},{"location":"\u0441haingear/development/#development-environment","text":"Recommending to use Remix Ethereum Online IDE or desktop electron-based Remix IDE PS: to import to IDE open-zeppelin contacts change imports this way: import \"github.com/openZeppelin/zeppelin-solidity/contracts/ownership/Ownable.sol\" ;","title":"Development environment"},{"location":"\u0441haingear/development/#truffle-ganache-workflow","text":"Install Ganache (with UI) from latest release or npm package => npm install -g ganache-cli Configure development config in truffle.js and launch Ganache (configure them too if needed) and: ganache-cli -p 7545 ( in first tab/or run UI-client ) truffle migrate --network development --reset ( in second tab ) truffle console --network development ( in second tab )","title":"Truffle + Ganache workflow"},{"location":"\u0441haingear/development/#creating-new-registrycru-entries-truffle-console-wayno-remix-ide","text":"var chaingear = Chaingear.at ( Chaingear.address ) var beneficiaries = [] var shares = [] var buildingFee = 100000 var gas = 10000000 chaingear.registerRegistry ( \"V1\" , beneficiaries, shares, \"BlockchainRegistry\" , \"BLR\" , { value: 100000 , gas: 10000000 }) var registryAddress = chaingear.registryInfo.call ( 0 ) var registry = Registry.at ( 'insert_registry_address_here' ) registry.initializeRegistry ( \"IPFS_HASH\" , EntryCore.bytecode ) registry.createEntry () var entryCoreAddress = registry.getEntriesStorage () var entryCore = EntryCore.at ( 'insert_entry_core_address_here' ) entryCore.updateEntry ( 0 , '0xa2f0dde51cb715f9cc7c12763fef90270bd50f70' , 256 , -127, \"helloworld\" ) entryCore.entryInfo ( 0 ) // --->>> [ '0xa2f0dde51cb715f9cc7c12763fef90270bd50f70' , BigNumber { s: 1 , e: 2 , c: [ 256 ] } , BigNumber { s: -1, e: 2 , c: [ 127 ] } , 'helloworld' ]","title":"Creating new registry/CRU entries (truffle console way/no Remix IDE)"},{"location":"\u0441haingear/development/#linting","text":"npm install -g solium solium -d contracts","title":"Linting:"},{"location":"\u0441haingear/development/#testing","text":"truffle test PS: script will run separate tests flow for each file, also produce gas report. Temp solution, for while we don't fix problem with tests falls when running for all files at one time what in truffle by default.","title":"Testing:"},{"location":"\u0441haingear/development/#deploying-for-example-kovan","text":"parity ui --chain = kovan truffle migrate --network = kovan PS: approve transaction in parity ui (http://127.0.0.1:8180/)","title":"Deploying (for example kovan):"},{"location":"\u0441haingear/development/#optional-build-contract-in-file","text":"truffle-flattener contracts/registry/Registry.sol > Registry_full.sol","title":"(Optional) Build contract in file:"},{"location":"\u0441haingear/overview/","text":"Overview \u00b6 This project allows you to create your own Registry of general purpose entries on Ethereum blockchain. Entry type is defined during creation, so you can put into entry any custom logic you want (validation, entry-level permission control). Entries are tokenized as NFT. Your creating your registry in Chaingear - metaregistry, which are one point of access to all registries. Registries on chaingear level are tokenized as NFT. Chaingear is most expensive registry, so you should pay for your registry creation. Features \u00b6 Chaingear \u00b6 Metaregistry with Registries entries, where each entry are ERC721 token Fee-based Registry creation Creating Registries with different functionality Token-based ownership/administration for Registry Funding in ETH for Registries Custom registry \u00b6 Custom data structure for Registry (EntryCore) Each Entry is ERC721 token Fee-based Entry creation Token-based ownership Entry management Entry creation policies (Administrator, Whitelist, AllUsers) Chaingear UI (browser/stand-alone web3 DApp) \u00b6 Web3/Metamask/Truffle/IPFS based Full Chaingear control interface Full custom Registry control interface Simple smart-contract EntryCore code generation in client Registries ABI and metainformation savings in IPFS","title":"Overview"},{"location":"\u0441haingear/overview/#overview","text":"This project allows you to create your own Registry of general purpose entries on Ethereum blockchain. Entry type is defined during creation, so you can put into entry any custom logic you want (validation, entry-level permission control). Entries are tokenized as NFT. Your creating your registry in Chaingear - metaregistry, which are one point of access to all registries. Registries on chaingear level are tokenized as NFT. Chaingear is most expensive registry, so you should pay for your registry creation.","title":"Overview"},{"location":"\u0441haingear/overview/#features","text":"","title":"Features"},{"location":"\u0441haingear/overview/#chaingear","text":"Metaregistry with Registries entries, where each entry are ERC721 token Fee-based Registry creation Creating Registries with different functionality Token-based ownership/administration for Registry Funding in ETH for Registries","title":"Chaingear"},{"location":"\u0441haingear/overview/#custom-registry","text":"Custom data structure for Registry (EntryCore) Each Entry is ERC721 token Fee-based Entry creation Token-based ownership Entry management Entry creation policies (Administrator, Whitelist, AllUsers)","title":"Custom registry"},{"location":"\u0441haingear/overview/#chaingear-ui-browserstand-alone-web3-dapp","text":"Web3/Metamask/Truffle/IPFS based Full Chaingear control interface Full custom Registry control interface Simple smart-contract EntryCore code generation in client Registries ABI and metainformation savings in IPFS","title":"Chaingear UI (browser/stand-alone web3 DApp)"},{"location":"\u0441haingear/pipelines/","text":"General Chaingear/Registry pipeline \u00b6 Registry CRUD/tokenized Entry/Funds pipeline \u00b6 Chaingear tokenized Registries pipeline \u00b6","title":"Pipelines"},{"location":"\u0441haingear/pipelines/#general-chaingearregistry-pipeline","text":"","title":"General Chaingear/Registry pipeline"},{"location":"\u0441haingear/pipelines/#registry-crudtokenized-entryfunds-pipeline","text":"","title":"Registry CRUD/tokenized Entry/Funds pipeline"},{"location":"\u0441haingear/pipelines/#chaingear-tokenized-registries-pipeline","text":"","title":"Chaingear tokenized Registries pipeline"}]}